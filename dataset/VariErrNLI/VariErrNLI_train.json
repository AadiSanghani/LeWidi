{
    "23751": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Part of the reason for the difference in pieces per possible delivery may be due to the fact that five percent of possible residential deliveries are businesses, and it is thought, but not known, that a lesser percentage of possible deliveries on rural routes are businesses.",
            "statement": "It is thought, but not known, that a lesser percentage of possible deliveries on rural routes are businesses, and part of the reason for the difference in pieces per possible delivery, may be due to the fact that five percent of possible residential deliveries are businesses."
        },
        "number of annotators": 2,
        "annotators": "Ann1,Ann3",
        "number of annotations": 2,
        "annotations": {
            "Ann1": "contradiction",
            "Ann3": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The reason for the diffenrence in pieces per possible delivery mentioned in the context is that the difference percentage of businesses deliveries on residential and rural routes. But the reason in the statement only include the percentage of residential deliveries, not the diffenrence of deliveries.",
                "Statement just changed the order of two hypothesis in the context."
            ]
        }
    },
    "61429": {
        "annotation task": "natural language inference",
        "text": {
            "context": "In this enclosed but airy building, you'll find ladies with large machetes expertly chopping off hunks of kingfish, tuna, or shark for eager buyers.",
            "statement": "You'll find small lepers chopping of chunks of tuna, its the only place they can work."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention whether the ladies are small lepers and whether its the only place they can work.",
                "The context does not say anything about lepers or where they could work.",
                "\"Small lepers\" don't have to be \"ladies\"; we don't know whether \"small lepers\" can find other jobs.",
                "Lepers and the only place to work at are not mentioned"
            ]
        }
    },
    "54811": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The park on the hill of Monte makes a good playground, while the ride down in a wicker toboggan is straight out of an Old World theme park (though surely tame for older kids).",
            "statement": "the park on the Hill of Monte is only for children."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann2,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "contradiction,neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention whether the park is only for children.",
                "The phrase \"makes a good playground\" suggests that the park is not a designated playground and thus is open for all age groups. Also, even if the park would be a designated playground, these can typically be also used by adults (at least the parents).",
                "Only because the park makes a good playground it doesn't necessarily have to be only for kids.",
                "No proof that the park can not be for adults.",
                "It's not mentioned that it's only for children"
            ]
        }
    },
    "12601": {
        "annotation task": "natural language inference",
        "text": {
            "context": "I touched my palm to his mutilated cheek, and tried to stem my instinctive revulsion.",
            "statement": "You could see where the bear had scratched across his cheek."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context contains no information about him being scratched on the cheek by a bear.",
                "The context does not say anything about how his cheek was mutilated, i.e. that it was a bear is not entailed.",
                "We don't know who/what makes his cheek mutilated.",
                "We don't know if it's a bear that scratched him"
            ]
        }
    },
    "38477": {
        "annotation task": "natural language inference",
        "text": {
            "context": "She wears either revealing clothes or professional clothes (or perhaps both).",
            "statement": "She only wears short skirts."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann3,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "contradiction",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.0,
                "1": 1.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "She doesn't only wear short skirts. She wears revealing clothes or professional clothes,  the former don't have to be just short skirts, the latter are most likely not short skirts.",
                "Usually, \"professinal clothes\" are not \"short skirts\".",
                "She could also wear professional clothes"
            ]
        }
    },
    "1735": {
        "annotation task": "natural language inference",
        "text": {
            "context": "that doesn't seem fair does it",
            "statement": "That might possibly be fair."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann2,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "contradiction,neutral",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.0,
                "1": 1.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context suggests that this may not be fair, while the statement suggests that it may be fair.",
                "The context can be interpreted as saying that it's really not fair, so it cannot be possibly fair.",
                "The context can be interpreted as saying that it seems unfair but that this maybe wouldn't hold under closer inspection. In that case it doesn't really say something about the actual fairness.",
                "The context suggests a higher possibility of unfairness in this matter, which the statement does not reflect.",
                "It's a rhetorical question. The speaker means it's not fair"
            ]
        }
    },
    "7449": {
        "annotation task": "natural language inference",
        "text": {
            "context": "In 1982, Wallace won his last race for governor with a quarter of the black votes cast in the Democratic primary, a fact alluded to in a written epilogue at the end of the film.",
            "statement": "Wallace was reelected as governor."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only mentioned that Wallance won his last race for gevernor, we don't know if this was his first win or if he won again.",
                "The context does not say anything about whether Wallace was governer before his 1982 win.",
                "Wallace was elected as governor, but we don't know whether he was \"reelected\".",
                "We don't know if he was governor before"
            ]
        }
    },
    "24385": {
        "annotation task": "natural language inference",
        "text": {
            "context": "farmworkers conducted by the U.S.",
            "statement": "Some farm laborers were sampled."
        },
        "number of annotators": 2,
        "annotators": "Ann1,Ann3",
        "number of annotations": 2,
        "annotations": {
            "Ann1": "neutral",
            "Ann3": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context contains no information about whether the conduction sampled the farmworkers.",
                "We don't know whether it is a census or a sampling survey."
            ]
        }
    },
    "48454": {
        "annotation task": "natural language inference",
        "text": {
            "context": "These revelations were embarrassing to Clinton's opponents, wrote the Washington Post . The Sun-Times quoted Rahm Emanuel, Stephanopoulos' successor, on the  From Day One I always thought this was politically motivated and had politics written all over it; after five years, it is nice to have the truth catch up with the president's political opponents.",
            "statement": "Clinton's supporters were pleased with how the hearings went."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction,neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only mentions clinton's opponents,  not clinton's supporters.",
                "The context is about a single supporter [probably?] of Clinton so we cannot deduce that all or most of the supporters were pleased with the hearing.",
                "These revelations were embarrassing to Clinton's opponents, but Clinton's supporters can also be unsatisfied with process for some reasons.",
                "It was embarrassing to them",
                "We don't know there are hearings"
            ]
        }
    },
    "73260": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The disputes among nobles were not the first concern of ordinary French citizens.",
            "statement": "Ordinary French citizens were not concerned with the disputes among nobles."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann2,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment,neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "\"not the first concern\" doesn't mean not the concern. The statement can be true or false.",
                "In the context, \"The first concern\" can be read as a pars pro toto which would mean that it was really no concern at all.",
                "It might not be the most important concern to the French citizens, but maybe an important concern after all.",
                "The disputes among nobles could be second concern of ordinary French citizens.",
                "They could be concerned. But it is not their first concern"
            ]
        }
    },
    "76219": {
        "annotation task": "natural language inference",
        "text": {
            "context": "and i and i may have been the only one that did both because the mentality in Dallas was that you couldn't like both you had to like one and hate the other",
            "statement": "I did not follow the mentality in Dallas, of liking only one team."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment,neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "In the context is stated that the author likes both, but the mentality in Dallas was that you couldn't like both. So the statement is true.",
                "The context clearly states that the mentality in Dallas was to like only one team and \"may have been the only one\" strongly suggests that the author did not adhere to this mentality.",
                "I did both instead of only liking one.",
                "We don't know what does \"did both\" mean. It could refer to liking both teams, but also could be others, for example  likes both McDonalds and Burger King.",
                "True, because I like both"
            ]
        }
    },
    "10229": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The governing statute provides that a committee consisting of the Comptroller General, the Speaker of the House and President Pro Tempore of the Senate, the Majority and Minority leaders, and the Chairmen and Ranking Minority Members of the Senate Governmental Affairs and House Government Reform Committees recommend an individual to the President for appointment.",
            "statement": "The process is long and will be reformed in the coming years."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context makes no mention about how long the process takes and whether it will be reformed.",
                "While the context could suggest that the process takes long because so many individuals are involved, it does not say anything about reform.",
                "\"Reform\" is not mentioned in the context.",
                "The time duration of the process is not mentioned"
            ]
        }
    },
    "99791": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Even analysts who had argued for loosening the old standards, by which the market was clearly overvalued, now think it has maxed out for a while.",
            "statement": "Some analysts wanted to make the old standards less restrictive for investors."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "entailment",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only mentions that there are analysts who had wanted to make the old standards less restrictive, but doesn't mention for whom should old standards be loosened.",
                "It is not clear from the context whether the standards should have been loosened specifically for investors.",
                "\"... analysts ... had argued for loosening the old standards\"",
                "It's not known if the loosening was thought for investors"
            ]
        }
    },
    "13964": {
        "annotation task": "natural language inference",
        "text": {
            "context": "uh plastic is just too easy i mean that's the that's the whole problem with it um have",
            "statement": "I find plastic to be too easy to use."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann1,Ann2,Ann3",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment,neutral",
            "Ann2": "entailment",
            "Ann3": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.33299999999999996,
                "1": 0.667
            },
            "neutral": {
                "0": 0.33299999999999996,
                "1": 0.667
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context refers to the problem of plastic is that it it too easy. So the statement could be true, because too tasy to use is indeed a problem of plastic.",
                "The statement could be undetermined because in the context it doesn't explicitly state to what is plastic too easy, could be too easy to use or maybe to produce.",
                "The context clearly states that the speaker finds plastic too easy.",
                "Plastics can be just too easy to \"use\", to \"produce\", to \"dump\"..."
            ]
        }
    },
    "66185": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The political cleansing that did not happen through the impeachment process leaves Clinton with a great and serious burden.",
            "statement": "There was no such instance of political cleansing."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The political cleansing did exist, as mentioned in the context. It just didn't happen throught the impeachment process.",
                "The context states that a specific instance of political cleansing did not happen and we can assume that the statement refers to that instance.",
                "“The political cleansing that did not happen through the impeachment process”, but it could happen anywhere else.",
                "True, because the political cleansing did not happen"
            ]
        }
    },
    "65066": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Larger ski resorts are 90 minutes away.",
            "statement": "The largest resort is actually 100 minutes away."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only mentions the larger ski resorts not the largest resort.",
                "The largest resort can be among the larger resorts 90 minutes away or it could be even further away.",
                "Larger one and the largest one can be different.",
                "No info about the largest ski resort"
            ]
        }
    },
    "79141": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Isn't a woman's body her most personal property?",
            "statement": "Women's bodies belong to themselves, they should decide what to do with it."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann2,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment,neutral",
            "Ann3": "entailment",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context states that women's bodies are her their personal property. Personal property of women does belong to themselves. So the statement is true.",
                "The context can be interpreted as a rhetorical question. In that case, it reasonably entails the statement.",
                "The context can be interpreted as a honest question. In that case, it's not affirmative and thus neutral to the statement.",
                "If a woman's body is her personal property, then the body belongs to her and she has right to dominate it.",
                "context and statement do not contradicting each other, or one entail the other"
            ]
        }
    },
    "97926": {
        "annotation task": "natural language inference",
        "text": {
            "context": "General Motors, for instance, lost $460 million to strikes in 1997, but investors treated the costs as a kind of extraordinary charge and valued the company as if the losses had never happened.",
            "statement": "GM lost a lot almost a million dollars in labor disputes."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "contradiction",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.0,
                "1": 1.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "GM lost more than a million dollars.",
                "GM lost over 460 million which is much more than one million.",
                "No, GM lost $460 million in labor disputes, far more than a million.",
                "They lost 460 million"
            ]
        }
    },
    "45957": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Bauerstein had been at Styles on the fatal night, and added: \"He said twice: 'That alters everything.' And I've been thinking.",
            "statement": "The fact that Styles was at Bauerstein changes everything."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "contradiction",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.25,
                "1": 0.75
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention what changed everything.",
                "The statement indicates that Bauerstein is a person and Styles is a location. The statement has it the other way around, so it is a likely contradiction.",
                "It is Bauerstein at Styles, not Styles at Bauerstein.",
                "It should be Bauerstein had been at Styles"
            ]
        }
    },
    "24126": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The door did not budge.",
            "statement": "The door was stuck, so it did not move."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only mentions the state of the door, has no information about what caused the state.",
                "There could have been other reasons that the door didn't moove. For example, that it was locked.",
                "We don't know the reason why the door did not budge, maybe it worked well.",
                "budge entails that the door wouldn't move"
            ]
        }
    },
    "135251": {
        "annotation task": "natural language inference",
        "text": {
            "context": "it's like but the time we went to Florida and needed to rent a car you know he believed in it",
            "statement": "We rented a car while we were in Florida."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context is unclear because it only mentions that they needed to rent a car, doesn't explain whether they did or not.",
                "The context says that they needed to rent a car, so it's likely that they also did rent a car.",
                "Maybe back then they are so poor that they “Needed to rent”, but could not afford it.",
                "It's not known if they rented the car in the end. It's only known that they needed to rent a car"
            ]
        }
    },
    "48223": {
        "annotation task": "natural language inference",
        "text": {
            "context": "yeah although i do worry that how easy this one was might be a bad lesson uh to the to the younger people um you know than there is the other generation",
            "statement": "I do worry that it might be a bad lesson for the kids."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "contradiction,entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context refers to the worry about giving a bad lesson to the younger people, which matches the statement.",
                "The speaker in the context explicitly says that they worry about it being a bad lesson for younger people which includes kids.",
                "younger people can be kids, adolescents or young adults",
                "younger people are not kids",
                "true, if the younger people are considered as kids"
            ]
        }
    },
    "73518": {
        "annotation task": "natural language inference",
        "text": {
            "context": "no North Carolina State",
            "statement": "North Carolina is a county"
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "contradiction",
            "Ann3": "contradiction,neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only doesn't state that whether North Carolina is a county or not.",
                "The context says that North Carolina is a state. So it's not a county.",
                "It is a state but not North Carolina, so North Carolina is a state instead of a country.",
                "There is no North Carolina State, North Carolina could be a city or an area.",
                "context and statement are irrelevant"
            ]
        }
    },
    "16996": {
        "annotation task": "natural language inference",
        "text": {
            "context": "In the short term, U.S. consumers will benefit from cheap imports (as will U.S. multinationals that use parts made in East Asian factories).",
            "statement": "U.S. consumers and factories in East Asia benefit from imports."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only mentions that the U.S. consumers and multinationals willl benefit from cheap imports, has no information about the benifits to factories in East Asia.",
                "The context does not say anything whether the factories in East Asia also benefit from the imports",
                "We don't know whether \"factories in East Asia benefit from imports\".",
                "It should be U.S. cunsumers in U.S."
            ]
        }
    },
    "30282": {
        "annotation task": "natural language inference",
        "text": {
            "context": "wow who can afford that  my God i can't afford to miss a day let alone six",
            "statement": "It's amazing that some people can afford to miss days from work, whereas I can't even afford to miss one."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only mentions that the author can't afford to miss one day, doesn't contain any information that others can afford to miss days.",
                "The \"wow\" in the context indicates amazement at the fact that some people can afford to miss six days from work.",
                "It is cleat that \"I can't afford to miss a day\", but we don't know whether some people can afford to miss days from work.",
                "It's not known if some people can afford it or not."
            ]
        }
    },
    "142430": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Flying at a discount should be more dangerous.",
            "statement": "It's totally safe to take advantage of discounted flying."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "neutral",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.25,
                "1": 0.75
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context states that it's dangerous to fly at a discount, whereas the statement states that dictount flying is safe.",
                "The stance given in the context that discounted flying should be more dangerous, can mean that it should be even more dangerous. Thus, it does not really say something about the safety of discounted flying.",
                "It is not totally safe, it is more dangerous to choose discounted flying.",
                "context says more danguerous flying at discount, whereas statement says total safe."
            ]
        }
    },
    "127290": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The logic of analysis in case studies is the same",
            "statement": "The logic for the case studies is the same thing as in the data collection."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The same logic of analysis in case studies doesn't mean the same logic of case studies and data collection. The context doesn't mention the data collection.",
                "The context does not specify where else the logic is the same. No mention about data collection.",
                "In context, the subject is the logic of analysis in different case studies; in statement, the subject is the logic in case studies and the logic in data collection.",
                "It's not known if the logic for case studies is same as in data collection from context. It could be same as in other fields."
            ]
        }
    },
    "100792": {
        "annotation task": "natural language inference",
        "text": {
            "context": "yeah but uh do you have small kids",
            "statement": "It matters not if children are involved."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Based on the context, we don't know if it matters to have kids.",
                "The speaker in the context asks about small kids, so at least it matters to them whether kids are involved.",
                "The question indicates that kids matter, otherwise there should be no \"but uh\".",
                "Context entails the importance of kids, which contradicts the statement."
            ]
        }
    },
    "75572": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Marriage is an important institution.",
            "statement": "Marriage is crucial to society."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Based on the context, we only knowthe attributes of marriage as an important institution,not whether it is inportant to society.",
                "That something is an important institution can be interpreted as being important for society.",
                "marriage could be crucial to other objects, like \"longevity\", \"personal health\", etc.",
                "Insitution is a part of society. Marriage being an import institution entails marriage being important to the society"
            ]
        }
    },
    "22235": {
        "annotation task": "natural language inference",
        "text": {
            "context": "yeah really no kidding",
            "statement": "It's crazy!"
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann3,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "entailment",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.33299999999999996,
                "1": 0.667
            },
            "neutral": {
                "0": 0.667,
                "1": 0.333
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "People might say \"no kidding\" when they realize what they're talking about is crazy.",
                "it can be not crazy but serious.",
                "Something sounds so crazy that people would think it is a joke. \"No kidding\" means it's as crazy as it sounds."
            ]
        }
    },
    "40486": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The Women's Haven, which provides shelter and outreach to domestic-violence victims, already has a full-time attorney.",
            "statement": "The Haven is a useful resource in the community."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context contains no information onthe importance of the Haven.",
                "Providing shelter and outreach to domestic-violence victims sounds like being useful for the community.",
                "The Women's Haven is not the Haven.",
                "The Haven provides shelter to victims, making it a useful resource in the society"
            ]
        }
    },
    "13133": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The newspaper publishes just one letter a week from a reader, always with an editorial riposte at the bottom.",
            "statement": "There are many letters submitted each week, but only one is chosen."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The newspaper publishes only one letter a week, either because they receive only one per week or because they receive many but choose only one. We don't know which is true.",
                "The context does not say anything about how many letters are submitted.",
                "We don't know whether there are many letters submitted.",
                "It's not known if many letters are submitted, but only known that only one is published."
            ]
        }
    },
    "145495": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The students' reaction was swift and contentious, as if their feelings had been hurt.",
            "statement": "The students reacted with horror."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention what kind of reaction the students had.",
                "The context mentions that the students' feelings might have been hurt but it doesn't specify whether they also experienced horror.",
                "The students could reacted with anger or disappointment.",
                "It's not known what emotion was reacted with, could also be sadness or anger"
            ]
        }
    },
    "93357": {
        "annotation task": "natural language inference",
        "text": {
            "context": "So is the salt, drying in the huge, square pans at Las Salinas in the south.",
            "statement": "Pepper is made wet in Las Salinas."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only mentions the salt, not the pepper.",
                "The context talks about salt. Pepper is not mentioned at all.",
                "Pepper is not mentioned in context.",
                "No info about where the pepper is made wet"
            ]
        }
    },
    "42388": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Daniel took it upon himself to explain a few things.",
            "statement": "Daniel explained what was happening."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 6,
        "annotations": {
            "Ann1": "entailment,neutral",
            "Ann2": "neutral",
            "Ann3": "entailment,neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "This statement could be true, because Daniel explained a few things, which could include what was happening.",
                "The statement could be undetermined because the context does not make it clear what things Daniel explained.",
                "The context does not specify what Daniel explained. It could also be other things than what was happening.",
                "Daniel explained actively.",
                "Daniel felt responsible to explain, but maybe fail to explain in the end.",
                "True"
            ]
        }
    },
    "22587": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Classic Castilian restaurant.",
            "statement": "The restaurant is based off a classic Castilian style."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "\"classic castilian restaurant\" means that the style of the restaurant is classic Castilian, so the statement is true.",
                "Both context and statement clearly mention that the restaurant is classic Castilian.",
                "Usually it is true.",
                "It is a castilian restaurant, so it has castilian style."
            ]
        }
    },
    "131261": {
        "annotation task": "natural language inference",
        "text": {
            "context": "But I'll take up my stand somewhere near, and when he comes out of the building I'll drop a handkerchief or something, and off you go!\"",
            "statement": "I want you to follow him, so watch for the signal that I give."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The speaker wants the other one watch for the signal, i.e., the handkerchief dropping, so this part is correct, but we don't know if the intention is for the other person to follow the person he/she is observing.",
                "It's not clear from the context, whether the speaker really wants the person they're talking to to follow someone. It could also be that they want the person to leave.",
                "Maybe “off you go” means something else, like detonating the rubbish bin etc.",
                "It is not known if \"I\" want \"you\" to follow him from the context"
            ]
        }
    },
    "51353": {
        "annotation task": "natural language inference",
        "text": {
            "context": "It is not a surprise, either, that Al Pacino chews the scenery in Devil's Advocate . And the idea that if the devil showed up on Earth he'd be running a New York corporate-law firm is also, to say the least, pre-chewed.",
            "statement": "The fact that the devil would work in law is extremely cliche."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann2,Ann3",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.33299999999999996,
                "1": 0.667
            },
            "neutral": {
                "0": 0.667,
                "1": 0.333
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The word \"pre-chewed\" in the context indicates that it is cliche.",
                "\"chewing the scenery\" means artificial acting, so \"pre-chewed\" likely means cliche",
                "The devil would work in law, which is an idea, not the fact, and the fact of devil's job is not given in the context."
            ]
        }
    },
    "10547": {
        "annotation task": "natural language inference",
        "text": {
            "context": "He jumped up, planting one hand on the charging horse, and came at the brute with the axe.",
            "statement": "He swung at the brute with his sword."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "contradiction",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.0,
                "1": 1.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "He came at the brute with the axe, not the sword.",
                "The statement says that the person has one hand on the horse, so the other hand has to be holding the axe. Thus, he does not have any hand free to use a sword.",
                "Not with \"sword\", but with \"axe\".",
                "No, with his axe"
            ]
        }
    },
    "136360": {
        "annotation task": "natural language inference",
        "text": {
            "context": "I can FEEL him.\"",
            "statement": "I can sense his presence."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "I think \"feel him\" is another way of saying \"sense his presence\".",
                "If a person can feel someone, then they most likely also sense their presence.",
                "\"Feel\" is synonym of sense.",
                "true, I can feel him, meaning I can sense his presence either physical or spiritually"
            ]
        }
    },
    "88188": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The air is warm.",
            "statement": "The arid air permeates the surrounding land."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Warm air is not necessarily associated with arid air.",
                "Air can be arid and cold at the same time.",
                "The air can be warm but mosit.",
                "irrelavant"
            ]
        }
    },
    "138966": {
        "annotation task": "natural language inference",
        "text": {
            "context": "It's thought he used the same architect who worked on the Taj Mahal.",
            "statement": "In reality, he did not use the Taj Mahal's architect."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context suggests that he might used the same architect, while the statement states that he did not.",
                "People can be wrong about him using the same architect.",
                "We don't know in reality whether he used the Taj Mahal's architect.",
                "The reality is not mentioned in the context"
            ]
        }
    },
    "48222": {
        "annotation task": "natural language inference",
        "text": {
            "context": "News berates computer users for picking obvious, easily cracked passwords and chastises system administrators for ignoring basic security precautions.",
            "statement": "Users and system administrators both do not prioritize security."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only mentions users' behaviour, we don't know if the system administrators prioritize security.",
                "The news can be wrong about the prevalence of this problem.",
                "Users pick obvious, easily cracked passwords; administrators ignore basic security precautions.",
                "true, users pick easy passwords and administrators irgnore basic security precautions, showing the low priority of security for them"
            ]
        }
    },
    "18428": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Companies that were foreign had to accept Indian financial participation and management.",
            "statement": "Foreign companies had to take Indian money in order to operate their businesses."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "entailment,neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Running businesses are the purpose of opening a company. So based on the context, companies have to satisfiy the prerequisites for accepting indian financial intervention in order to run businesses.",
                "That the companies had to accept \"Indian financial participation\" means that they had to take Indian money.",
                "Financial participation and management can have other forms, e.g. audit and supervision",
                "True, if indian financial participation means investment from india",
                "financial participation could mean other possibilites other than investing money"
            ]
        }
    },
    "116059": {
        "annotation task": "natural language inference",
        "text": {
            "context": "These days, newspaper writers are no longer allowed the kind of license he took.",
            "statement": "Newspaper writers need to be more factual and careful these days."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't specify what kind of license he took. So we don't know whether it is associated with factual and carefull, as suggested in the statement.",
                "It is not clear which freedoms the writer in the context took. It might not be about factuality and carefulness.",
                "\"the kind of license\" can mean a facual and careful style, then the opposite should be not rigorous.",
                "irrelevant"
            ]
        }
    },
    "67610": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Sorry but that's how it is.",
            "statement": "This is how things are and there are no apologies about it."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann2,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment,neutral",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "We don't know the \"sorry\" in the context is meant to be an apology or just to comfort someone, so the need for an apology is unknown.",
                "The context can be interpreted as being unapologetic.",
                "The context can also be interpreted as being matter of factly but not unapologetic.",
                "In context, it reads \"sorry\", and usually it means apology.",
                "There are apologies in the context"
            ]
        }
    },
    "95186": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The cane plantations, increasingly in the hands of American tycoons, found a ready market in the US.",
            "statement": "The US market was ready for the cane plantations, according to the economists."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context mentions that there is a ready market in the US, but it's not clear who indicates this.",
                "The context does not say anything about whether economists believed that the market was ready.",
                "We don't know the resource of this ovservation that \"The cane plantations ... found a ready market in the US.\"",
                "It is not mentioned about the economists"
            ]
        }
    },
    "77875": {
        "annotation task": "natural language inference",
        "text": {
            "context": "As legal scholar Randall Kennedy wrote in his book Race, Crime, and the Law , Even if race is only one of several factors behind a decision, tolerating it at all means tolerating it as potentially the decisive factor.",
            "statement": "Race is one of several factors in some judicial decisions"
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only means decisions, not judicial decisions.",
                "The conditional in the context does not assert whether race is one of several reasons or the only one in all judicial decisions.",
                "We don't know whether in fact race is one of several factors in some judicial decisions.",
                "It is not mentioned in what process is the race a factor of"
            ]
        }
    },
    "65130": {
        "annotation task": "natural language inference",
        "text": {
            "context": "In Mumbai, both Juhu and Chowpatty beaches are, for instance, definitely a bad idea, and though the Marina beaches in Chennai are cleaner, there may be sharks.",
            "statement": "The beaches are very dirty in Mumbai."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "neutral",
            "Ann3": "entailment,neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context mentions that two beaches in Mumbai are cleaner than others, but the statement suggests that all beaches in Mumbai are dirty.",
                "There could be other beaches than Juhu and Chowpatty in Mumbai which could be cleaner.",
                "Mumbai's beaches are dirtier than beaches in Chennai, and described as \"definitely a bad idea.\"",
                "Mumbai's beaches are dirtier than beaches in Chennai, but it is not promised that they are very dirty objectively.",
                "Marian beaches are compared to Juhu and Chowpatty cleaner, meaning the beaches in Mumbai beaches are dirty."
            ]
        }
    },
    "19578": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Moreover, Las Vegas has recently started to show signs of maturity in its cultural status as well.",
            "statement": "The culture of Las Vegas has a lot of room for improvement."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "contradiction,neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context states that the culture of Las Vegas is improving, but the is no information about the room for improvement.",
                "That there were signs of maturity indicates improvement. However, it does not say anything about whether things can improve further.",
                "The culture started to show signs of maturity, then they must have reached some standard and doesnot have much to improve.",
                "We don't know whether the culture has a lot of room to improve.",
                "The context is talking about the cultural status, whereas the statement the culture"
            ]
        }
    },
    "83657": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Think of it this  When consumer confidence declines, it is as if, for some reason, the typical member of the co-op had become less willing to go out, more anxious to accumulate coupons for a rainy day.",
            "statement": "Coupon collecting is no longer allowed in most US stores."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context mentions that some people tend to collect coupon, which implies that coupon collection is actually allowed. However, it doesn't include location information, so we don't know if it refers to the United States.",
                "The context does not say anything about whether coupon collection is legal or not in most US stores.",
                "We don't know whether the context is based on the U.S. society.",
                "Not to collect coupon is a choice of the consumers. It is not forbidden."
            ]
        }
    },
    "102817": {
        "annotation task": "natural language inference",
        "text": {
            "context": "yes they would they just wouldn't be able to own the kind of automobiles that they think they deserve to own or the kind of homes that we think we deserve to own we might have to you know just be able to i think if we a generation went without debt then the next generation like if if our our generation my husband and i we're twenty eight if we lived our lives and didn't become you know indebted like you know our generation before us that um the budget would balance and that we became accustomed to living with what we could afford which we wouldn't be destitute i mean we wouldn't be living on the street by any means but just compared to how spoiled we are we would be in our own minds but i feel like the generation after us would oh man it it would be so good it would be so much better it wouldn't be perfect but then they could learn to live with what what they could afford to save to buy and if you want a nicer car than that well you save a little longer",
            "statement": "I am glad our generation has no debt."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context implies that our generation does have debt, so the statement is wrong.",
                "The speaker in the context would be glad if their generation had no debt, but does not assert this.",
                "We don't know whether \"our generation\" has no debt.",
                "We are indebted and therefore living on the street"
            ]
        }
    },
    "124590": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The great attraction of the church is the splendid exterior, which is crowned by golden onion-shaped cupolas.",
            "statement": "The outside of the church isn't much to look at, but the inside is intricately decorated."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "contradiction",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.0,
                "1": 1.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context states that the exteroir of the church is great acctraction, but the statement implies the opposite.",
                "The context clearly states that the exterior of the church is splendid.",
                "The outside of the church is attactive.",
                "Is is the exterior that is a great attraction."
            ]
        }
    },
    "96583": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Mack Lee, Body Servant of General Robert E. Lee Through the Civil War , published in 1918.",
            "statement": "The book was first drafted in early 1915."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only mention the year the book published, no when it was drafted.",
                "The context mentions only the publication date, not the date of the first draft.",
                "We just know the publishment of the book, but don't know about the draft.",
                "No info about the first draft only the first publication"
            ]
        }
    },
    "132525": {
        "annotation task": "natural language inference",
        "text": {
            "context": "She had the pathetic aggression of a wife or mother--to Bunt there was no difference.",
            "statement": "Bunt was raised motherless in an orphanage."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It is unclear where Bunt was raised based on the context.",
                "The context does not mention anything about Bunt having or not having a mother or where they were raised.",
                "We don't know where and how Hunt grew up.",
                "We don't know what the relation \"she\" is to Bunt. She could also be his wife"
            ]
        }
    },
    "11362": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The volumes are available again but won't be returned to the stacks until the damp library itself gets renovated.",
            "statement": "The volumes will be available to the public after renovation."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "neutral",
            "Ann3": "contradiction,neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.25,
                "1": 0.75
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context mentions that the library needs to be renovated, not the volumes.",
                "It is not clear whether the library is a public or a private library.",
                "The volumes are available before the renovation.",
                "We don't know whether after renovation the volumes will be available to the public or not after renovation..",
                "It is already availabe before the renovation of stacks are done."
            ]
        }
    },
    "55888": {
        "annotation task": "natural language inference",
        "text": {
            "context": "You've got the keys still, haven't you, Poirot? I asked, as we reached the door of the locked room.",
            "statement": "I had the keys in my pocket."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "neutral",
            "Ann3": "contradiction,neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.25,
                "1": 0.75
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "If the speaker had the keys, then she/he won't ask whether the other one had keys.",
                "It is not clear from the premise where the keys are.",
                "If I have the keys, usually I will open the door, instead of asking others.",
                "We can't make sure whether I have the key or not.",
                "\"I\" think Poirot has the keys"
            ]
        }
    },
    "138285": {
        "annotation task": "natural language inference",
        "text": {
            "context": "i cried when the horse got killed and when the wolf got killed",
            "statement": "Animal killings make me want to cry."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 6,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment,neutral",
            "Ann3": "entailment,neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both horse and wolf are animals, the speaker cried when they got killed. So the statement is correct.",
                "If killings of two different animals make the narrator cry, then they probably generally care about animal killings.",
                "It could be that the narrator had a personal relationship to the killed animals and does not care about animal killings in general.",
                "Some animal like horse and wolf killings make me cry.",
                "We don't know whether all kinds of animal killings will make me want to cry, maybe I don't want to cry for a rat killing.",
                "Overgeneralization.  I could be sad maybe only because I know the horse and wolf"
            ]
        }
    },
    "57454": {
        "annotation task": "natural language inference",
        "text": {
            "context": "what does um is Robby Robin Williams does he have a funny part in the movie or is",
            "statement": "Is Robin Williams in the movie?"
        },
        "number of annotators": 2,
        "annotators": "Ann2,Ann4",
        "number of annotations": 2,
        "annotations": {
            "Ann2": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The speaker seems to know that Robin Williams is in the movie, but not which part he has.",
                "He is in the movie, the context asked about if he had a funny part in the movie"
            ]
        }
    },
    "88605": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The remaining parts of the north, although enticing, are difficult to explore.",
            "statement": "The rest of the north presents a steep challenge."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both context and statement are saying that  the rest of the north is hard to explore.",
                "If the remaining parts of the north are difficult to explore, then they also present a steep challenge.",
                "\"Difficult to explore\" means \"a steep challenge\".",
                "\"difficult to explore\" entails a chanllenge"
            ]
        }
    },
    "72721": {
        "annotation task": "natural language inference",
        "text": {
            "context": "no i i i don't i it completely beyond me i went to my under graduate uh education",
            "statement": "I can't remember, I did my undergraduate education."
        },
        "number of annotators": 2,
        "annotators": "Ann1,Ann3",
        "number of annotations": 2,
        "annotations": {
            "Ann1": "contradiction",
            "Ann3": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.0,
                "1": 1.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context mentions that the speaker did undergraduate education, the statement is false if the speaker of both sentences is the same person.",
                "In context, it read \"I went to my under graduate uh education\", so \"I\" do remember it."
            ]
        }
    },
    "49611": {
        "annotation task": "natural language inference",
        "text": {
            "context": "How did you get it?\" A chair was overturned.",
            "statement": "\"How did you get your hands on this object?\""
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann3,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "neutral",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.33299999999999996,
                "1": 0.667
            },
            "neutral": {
                "0": 0.667,
                "1": 0.333
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "I am asking my self whether the question belongs to a kind of hypothesis/statement. I can't make a conclusion based on the two questions in the provided context and statement.",
                "“get your hands on this object” can be understood as get something",
                "paraphrases"
            ]
        }
    },
    "16989": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Auditors from another country engaged to conduct audits in their country should meet the professional qualifications to practice under that country's laws and regulations or other acceptable standards, such as those issued by the International Organization of Supreme Audit Institutions.",
            "statement": "All auditors report to a globally managed governing body."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context implies that there are several possibilities for the standards that the auditors are expected meet, so the standard mentioned in the statement is one of the standards, not a mandatory one.",
                "The context does not say anything about to whom the auditors report.",
                "We don't know whether auditors dealing with domestic companies need to report to a globally managed governing body.",
                "In the context, it is not mentioned that they report to the globally managed governing body"
            ]
        }
    },
    "112349": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The idea that Clinton's approval represents something new and immoral in the country is historically shortsighted.",
            "statement": "It's accurate to conclude that Clinton's approvals signify the start of a new form of immorality in the country."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "contradiction",
            "Ann3": "contradiction",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.25,
                "1": 0.75
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The contexts suggests that the conclusion about Clinton's approval is shortsighted, while the statement refers to the accuration of this conclusion, so it is incorrect.",
                "If an idea is historically shortsighted it is not accurate.",
                "To the same idea, the context remarked it as \"historically shortsighted\", but the statement took it as \"accurate\".",
                "the approval only signified that the somehting immoral is historically shortsighted but does not introduce it."
            ]
        }
    },
    "43440": {
        "annotation task": "natural language inference",
        "text": {
            "context": "And you are wrong in condemning it.",
            "statement": "Everybody does it; it's normal."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "contradiction,neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context refers to the wrongfulness of the condemnation,it is unclear what the normal situation is to which the statement refers.",
                "You cannot infer that something is normal because it is wrong to condemn it.",
                "In reality, if everybody does a thing, the thing will become a costum, and will not be condemned.",
                "We can not say a thing is correct, juest because everybody does it.",
                "The reason of wrongfulness in condemning is not known"
            ]
        }
    },
    "91709": {
        "annotation task": "natural language inference",
        "text": {
            "context": "San'doro didn't make it sound hypothetical, thought Jon.",
            "statement": "San'doro's words were hollow, and Jon knew the truth of that immediately."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "neutral",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.25,
                "1": 0.75
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "\"didn't sound hypothetical\" implies that San'doro's words were actually not hollow, so the statement is  false.",
                "Jon only thought about how San'doro made it sound. This doesn't tell us anything about whether he believed the words.",
                "Jon did not regard San'doro's words as hollow, instead, he regarded it as not hypothetical.",
                "san'doro's words sound factual."
            ]
        }
    },
    "47798": {
        "annotation task": "natural language inference",
        "text": {
            "context": "On the west side of the square is Old King's House (built in 1762), which was the official residence of the British governor; it was here that the proclamation of emancipation was issued in 1838.",
            "statement": "The Old King's House had an incident where the King was murdered inside of it."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "There is no information in the context about the murder inside of the Old King's House, so the statement could be true or false.",
                "The context doesn't say anything about whether someone was killed in Old King's House.",
                "We don't know whether a King was murdered inside of the Old King's House.",
                "No info about the murder"
            ]
        }
    },
    "103364": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Several of its beaches are officially designated for nudism (known locally as naturisme) the most popular being Pointe Tarare and a functionary who is a Chevalier de la L??gion d'Honneur has been appointed to supervise all aspects of sunning in the buff.",
            "statement": "They do not mind having nude people."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "contradiction,entailment",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The beaches mentioned in the context accept people sunning naked, so we can conclude that they don't mind having nude people.",
                "Beaches officially designated for nudism do not mind having nude people.",
                "The nude people are supervised. If they don't mind, there should be no supervisors especially for such issues.",
                "Some beaches are offically designated for nudism, which means nude people are allowed to be there.",
                "Overgeneralization: they do not mind only in several of its beaches"
            ]
        }
    },
    "49462": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The village is Sainte-Marie, named by the explorer when he landed on 4 November 1493, attracted by the waterfalls and river he could see flowing down the green inland mountains.",
            "statement": "The village is not named after the settling explorer."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "entailment",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "There is no information in the context about how the village is named, so the statement could be true or false.",
                "The village was named by the explorer, it is not clear whether he named it after himself.",
                "It is named by the settling explorer, but named after.",
                "It is named after him"
            ]
        }
    },
    "97011": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Expectations that the ANC would oversee land reform--returning land seized during apartheid's forced migrations--and wealth redistribution have not been met.",
            "statement": "The ANC would not be in charge of land reform."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The expectations have not been met, which means the the ANC didn't voersee land reform, so the statement is true.",
                "If the ANC does oversee the land reform then it isn't in charge of it.",
                "The expectations have not been met.",
                "True, the expections that ANC oversees this is not met"
            ]
        }
    },
    "56743": {
        "annotation task": "natural language inference",
        "text": {
            "context": "I found her leaning against the bannisters, deadly pale.",
            "statement": "She couldn't stand on her own so she leaned against the bannisters until I found her."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The reason why she leaned against the bannisters in unclear,  it may or may not have been because she couldn't stand on her own.",
                "She could have leaned on the bannisters for other reasons than not being able to stand on her own.",
                "Maybe she could stand on her own, but she would not.",
                "No info: no info about if she standed there till I found her, or if she really couln't stand on her own"
            ]
        }
    },
    "48300": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The activities included in the Unified Agenda are, in general, those expected to have a regulatory action within the next 12 months, although agencies may include activities with an even longer time frame.",
            "statement": "Some actions were implemented for being shorter than 12 months."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context mentions that some activities with an longer time frame than 12 months will be included, so it could be true that there are some activities are shorter that 12 months.",
                "The context states that the actions should generally see action within 12 months, so at least some are implemented for being shorter than 12 months.",
                "Some activities are maybe longer than 12 months.",
                "Miss interpretation: some actions were to be implemented in less than 12 months"
            ]
        }
    },
    "136752": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The questions may need to be tailored to",
            "statement": "A majority of the questions referenced will need to be tailored to."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't specify how many questions need to be tailored.",
                "The statement is a paraphrase of the context.",
                "Maybe all the questions referenced will need to be tailored to.",
                "No info about whether the majority of the questions or all of them"
            ]
        }
    },
    "14388": {
        "annotation task": "natural language inference",
        "text": {
            "context": "life in prison then he's available for parole if it's if it's life and a day then he's not eligible for parole so what you know let's quit BSing with the system",
            "statement": "The system is corrupt because he won't be able to get parole if it's life and a day."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Complaints were made about the system, as implied both in the context and statement.",
                "The speaker thinks the system is very bad but doesn't say anything about corrupt.",
                "The system can be corrupt, but also can be ridiculous.",
                "No info: there is not comment about whether or not the rules of this system are counted as corrupted."
            ]
        }
    },
    "103559": {
        "annotation task": "natural language inference",
        "text": {
            "context": "A martini should be gin and vermouth and a twist.",
            "statement": "A martini must be composed by gin and vermouth."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The statement doesn't mention twist, but it is mentioned in the context.",
                "The context clearly states that a martini should contain gin and vermouth.",
                "Gin and vermouth are necessary for a martini.",
                "true. The ingredients of martini are gin and vermouth"
            ]
        }
    },
    "132019": {
        "annotation task": "natural language inference",
        "text": {
            "context": "and uh really they're about it they've got a guy named Herb Williams that that i guess sort of was supposed to take the place of uh Tarpley but he uh he just doesn't have the offensive skills",
            "statement": "Tarpley is a better offensive player that Herb Williams."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context mentions that Williams doesn't have the offensive skills, even though he will replace Tarpley. So we can conclude from the context that Tarpley's offensive skills is better than Williams'.",
                "Herb Williams couldn't replace Tarpley because of his lack of offensive skills. Thus, it is reasonable to assume that Tarpley is the better offensive player.",
                "Herb Williams doesn't have the offensive skills and falls to take the place of Tarpley. So Tarpley should be a better offensive player.",
                "Herb williams does not have the offensive skills like Tarpley do"
            ]
        }
    },
    "118999": {
        "annotation task": "natural language inference",
        "text": {
            "context": "that's true i didn't think about that",
            "statement": "You've changed my mind with a new perspective."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The only thing mention in the context is the new perspective, we can't conclude whether the speaker changed mind.",
                "The context is a paraphrase of the statement.",
                "\"I\" can also keep my mind, although you offer a new perspective.",
                "No Info: no info about whether or not I have changed my mind"
            ]
        }
    },
    "47404": {
        "annotation task": "natural language inference",
        "text": {
            "context": "do you really romance",
            "statement": "Do you really have an affair?"
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both are questions and are asking about the same thing, which is about having an affair.",
                "You can have other types of romance than an affair.",
                "A romance can happen between two unmarried single person",
                "romance does not directly relate to affair."
            ]
        }
    },
    "30171": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Until all members of our society are afforded that access, this promise of our government will continue to be unfulfilled.",
            "statement": "The government is flawed and unfulfilled."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann3,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "contradiction",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.667,
                "1": 0.333
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.33299999999999996,
                "1": 0.667
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The government will not always be unfulfilled. If all menbers are afforded that access, the government could fulfill the promise mentioned in the context.",
                "We don't know whether all members of the society are afforded that access.",
                "The promise of the government is unfulfilled. Can not conclude that the government is therefore flawed and unfulfilled."
            ]
        }
    },
    "134514": {
        "annotation task": "natural language inference",
        "text": {
            "context": "However, co-requesters cannot approve additional co-requesters or restrict the timing of the release of the product after it is issued.",
            "statement": "They will restrict timing of the release of the product."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "neutral",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.25,
                "1": 0.75
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "They can't restrict the release time, as mentioned in the context.",
                "Whether they can restrict the timing of the product at all is not clear beacuse we don't know whether it was already issued.",
                "Co-requesters cannot restrict the timing of the release of the product.",
                "They can NOT restrict the timing of the release"
            ]
        }
    },
    "135021": {
        "annotation task": "natural language inference",
        "text": {
            "context": "you know we keep a couple hundred dollars um if that much charged on those which isn't too bad it's just your normal",
            "statement": "We have money on there, which isn't great"
        },
        "number of annotators": 2,
        "annotators": "Ann1,Ann3",
        "number of annotations": 2,
        "annotations": {
            "Ann1": "neutral",
            "Ann3": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The \"not too bad\" comment in the context is about charging much, while the statement is about having money on there.",
                "Spending money on that is not great, because it is normal."
            ]
        }
    },
    "98710": {
        "annotation task": "natural language inference",
        "text": {
            "context": "well Jerry do you have a favorite team",
            "statement": "Jerry, do you follow any sports?"
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "neutral",
            "Ann3": "entailment,neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both context and statement are questions about sports. If someone has a favorite team, then he/she must follow this sport.",
                "Jerry can follow sports without having a favorite team.",
                "If Jerry has a favorite team, he/she should follow this sport.",
                "Jerry can be a fake fan, for example he/she support a local team, but even don't know the rule of the sport.",
                "irrelevant"
            ]
        }
    },
    "30894": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Earlier this week, the Pakistani paper Dawn ran an editorial about reports that Pakistani poppy growers are planning to recultivate opium on a bigger scale because they haven't received promised compensation for switching to other crops.",
            "statement": "Pakistani poppy growers are mad at the government."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "neutral",
            "Ann3": "entailment,neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Pakistani poppy growers haven't received compensation, which could be a reason why they might be mad at the government. Planning to recultivate opium on a bigger scale could be a sideways indication that they're mad.",
                "It is not clear whether the poppy growers are really mad. Maybe they did not care much about the promised compensations.",
                "Yes, because Pakistani poppy growers haven't received promised compensation.",
                "Maybe Pakistani poppy are mad at the government, maybe they are not mad.",
                "Because the growers haven't received promised compensation from the government, so they can be mad at government because of it"
            ]
        }
    },
    "81356": {
        "annotation task": "natural language inference",
        "text": {
            "context": "In keeping with other early Buddhist tenets, there is no figurative representation of Buddha here, However, there is a large gilded statue from a later period inside, and behind the temple are the spreading branches and trunks of the sacred Bodhi Tree, which is said to have grown from a sapling of the first one that stood here 2,500 years ago.",
            "statement": "There is no statue of Buddha located there."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "contradiction,neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "No figurative representation of Buddha indicates that there is no satues of Buddha.",
                "If there is no figurative representation of Buddha then there can be no statue of Buddha.",
                "\"However\" means although the early Buddhist tenet forbade the figurative representation of Buddha, later it was somehow disobeyed or changed, and there is a large gilded statue here.",
                "We don't know whether the \"large gilded statue\" is a statue of Buddha or something else.",
                "There is a large gilded statue from a later period"
            ]
        }
    },
    "132516": {
        "annotation task": "natural language inference",
        "text": {
            "context": "right right they left a woman and a child or the cat the sheep yeah",
            "statement": "They were merciful in this regard, only taking the men as slaves."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "entailment,neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The statement could be true because only the men are not mentioned in the context. They may have been merciful if they only took the men but left the women and children.",
                "The context doesn't mention men.",
                "They left a woman or a child. They could still have taken other women or children.",
                "What did they take is not sure, maybe they didn't take anybody, just took some fortune away.",
                "No info about taking the men as slaves"
            ]
        }
    },
    "34176": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The rustic Bras-David picnic area, for example, is set alongside a burbling stream.",
            "statement": "The stream is always burbling."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "neutral",
            "Ann3": "entailment,neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both context and statement mention the burbling stream.",
                "The stream could also be burbling only sometimes or most of the time.",
                "\"Burbling\" is how the stream is described in the context.",
                "It can't be promised that it is \"always\" burbling, maybe sometimes the rainfall will influence the volume of the stream.",
                "The area is set alongside a burbling stream. No info about if all stream is burbling"
            ]
        }
    },
    "77116": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The third row of Exhibit 17 shows the Krewski, et al.",
            "statement": "Exhibit 17 has many rows."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "neutral",
            "Ann3": "entailment,neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context suggests that Exhibit 17 has at least three rows. So there are indeed many rows.",
                "We only know that it has at least three rows, which is not many.",
                "As there is \"the third row\", we can suppose that there are more than one row.",
                "\"Many\" can be quite a large number, but it is not refered in the context.",
                "There is the third row of Exhibit 17, meaning it has at least first and second row"
            ]
        }
    },
    "101245": {
        "annotation task": "natural language inference",
        "text": {
            "context": "we were lucky in that in one respect in that after she had her stroke she wasn't really you know really much aware of what was going on",
            "statement": "She had a very serious stroke."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention the seriousness of the stroke.",
                "The stroke left her unaware of her surroundings, so it has to have been serious.",
                "It can be a very serious stroke, but also can be a mild stroke.",
                "No info about the seriousness of her stroke; She could a serious or not so serious stroke"
            ]
        }
    },
    "8545": {
        "annotation task": "natural language inference",
        "text": {
            "context": "He hadn't seen even pictures of such things since the few silent movies run in some of the little art theaters.",
            "statement": "He had recently seen pictures depicting those things."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "neutral",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.25,
                "1": 0.75
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It is clear from the context that he hadn't seen pictures of such things, while the statement suggests that he had recently seen them.",
                "He could have visited the little art theaters recently or not. It is not clear.",
                "No, \"he hadn't seen pictures of such things\" since the silent movies run in some art theaters.",
                "He hadn't seen pictures of those things"
            ]
        }
    },
    "8219": {
        "annotation task": "natural language inference",
        "text": {
            "context": "it depends a lot of uh a lot of things were thought that uh as you know the farmers thought okay we got chemicals we're putting chemicals on the field well the ground will naturally filter out the",
            "statement": "The farming chemicals are filtered by the ground."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann2,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.667,
                "1": 0.333
            },
            "neutral": {
                "0": 0.33299999999999996,
                "1": 0.667
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only mentions the thought of the farmers. It is not clear whether this thought is the truth.",
                "The speaker talks about the farmers thinking that the ground will filter out the chemicals. But they don't say that they themselves believe it.",
                "The farmers think the groud will naturally filter out the chemicals"
            ]
        }
    },
    "115247": {
        "annotation task": "natural language inference",
        "text": {
            "context": "oh really yeah so he he's uh he's probably going to be going to jail and and the problem with him is he's on a guaranteed salary like for three years so whether he plays or not they've got to pay him ten million dollars so if they",
            "statement": "He is so hardworking and has helped the team achieve so much, I don't see anything wrong with paying him a million dollar salary."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "contradiction",
            "Ann3": "contradiction,neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "We can't infer from the context whether he is hardworking or not.",
                "The speaker clearly says that they believe it's a problem that the person has a guaranteed million dollar salary.",
                "He will probably go to jail, and \"I\" worry about the ten million dollars, but although he can not work in jail, the money still need to be paid.",
                "His personality and characters are not mentioned in the context, maybe he is not hardworking.",
                "No info about where he is hardworking or not or about my subject feelings on him got paid a million dollar salary"
            ]
        }
    },
    "99708": {
        "annotation task": "natural language inference",
        "text": {
            "context": "It was made up to look as much like an old-fashioned steam train as possible.",
            "statement": "It was built in the modern era to look like something built in the past."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context mentions the building of an old-fashioned train, the word old-fashioned would only be used in the modern era. So the statement is true.",
                "It was made to look like a steam train which is something from the past.",
                "It was made up to look as an \"old-fashioned\" steam train, if it is made in the past, it should be described as \"fashion\" instead of \"old-fashioned\". So it was built in the modern era but to look like something old.",
                "No info about when it was build. It could be built in 20th century to look like something built in 19th century"
            ]
        }
    },
    "82415": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Then he sobered.",
            "statement": "He was drunk."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "entailment,neutral",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "If he was not sober due to alcohol, then the statement is true.",
                "He could also be not sober because of the drugs.",
                "For a person to sober, they have to be drunk before.",
                "He can be faint because of hunger or desease.",
                "sobered means becoming not drunk"
            ]
        }
    },
    "82700": {
        "annotation task": "natural language inference",
        "text": {
            "context": "During his disastrous campaign in Russia, he found time in Moscow to draw up a new statute for the Com??die-Francaise (the national theater), which had been dissolved during the Revolution.",
            "statement": "Russia has been successfully invaded hundreds of times."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention how many times Russia has been invaded.",
                "There is no information about how many times Russia was invaded.",
                "Nothing shows how many times has Russia been invaded.",
                "No info about how many times Russia has been invaded"
            ]
        }
    },
    "1073": {
        "annotation task": "natural language inference",
        "text": {
            "context": "News ' cover says the proliferation of small computer devices and the ascendance of Web-based applications are eroding Microsoft's dominance.",
            "statement": "Microsoft is a more profitable company than Apple."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention the profits of Microsoft and Apple.",
                "The context does not say anything about Apple.",
                "Apple is not mentioned in the context, so we can not compare which company is more profitable.",
                "No info about the profit-comparision between Microsoft and Apple"
            ]
        }
    },
    "92062": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Krugman's column will henceforth be known as The Dismal Science, a phrase too famous to be ownable by anyone, except possibly British essayist Thomas Carlyle (1795-1881), who coined it.",
            "statement": "Krugman writes novels."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Krugman is a columnist, he doesn't write novels.",
                "It's clear that Krugman writes a column, but not clear whether they write novels.",
                "Krugman has a column known as \"The Dismal Science\", it can be a novel column, but also can be others like essay column.",
                "No, in the context is his column, which appears often in newspaper"
            ]
        }
    },
    "131235": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Even if the entire unified surplus were saved, GDP per capita would fall somewhat short of the U.S. historical average of doubling every 35 years.",
            "statement": "Even if the entire unified surplus were lost, GDP per capita would fall somewhat short of the U.S. historical average of doubling every 35 years."
        },
        "number of annotators": 2,
        "annotators": "Ann2,Ann4",
        "number of annotations": 2,
        "annotations": {
            "Ann2": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It is not clear what would happen to the GDP if the surplus would be lost instead of saved.",
                "Context state if the surplus were saved"
            ]
        }
    },
    "46820": {
        "annotation task": "natural language inference",
        "text": {
            "context": "and my and my part-time work you know it's not our the restaurant our favorite restaurant in the town of Salisbury where actually we live you know where my where i'll return to my job or whatever we can normally eat out for um under fourteen dollars",
            "statement": "My first part time job was in a restaurant in Salisbury where you could eat out for under $14."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann3,Ann4,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction,neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.667,
                "1": 0.333
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It is not mentioned in the context whether the speaker's first part time job was in a restaurant in Salisbury.",
                "1. The part-time work in Salisbury can not be the first job; 2. The part-time job can not in a restaurant.",
                "My part time job is not in a restaurant",
                "No info about if it was my first part time job"
            ]
        }
    },
    "77590": {
        "annotation task": "natural language inference",
        "text": {
            "context": "do you think most states have that or",
            "statement": "I think most states have that."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann2,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The statement is an answer to the question in the context. It may be true or false.",
                "The speaker poses a question and doesn't assert that most states have that.",
                "No info about what I think in the context"
            ]
        }
    },
    "128542": {
        "annotation task": "natural language inference",
        "text": {
            "context": "There should be someone here who knew more of what was going on in this world than he did now.",
            "statement": "He knew things, but hoped someone else knew more."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only suggests that someone else knew more than he did, it is not clear whether he hoped so.",
                "It's not clear whether he hoped or only epected that there was someone who knew more.",
                "He supposed that someone else knew more, but it can not reflect whether he hoped so or not, maybe he hoped that he knew the most in the world.",
                "someone is hoped to be here to know more of what was going on than he did"
            ]
        }
    },
    "74768": {
        "annotation task": "natural language inference",
        "text": {
            "context": "She admits to Dorcas, 'I don't know what to do; scandal between husband and wife is a dreadful thing.' At 4 o'clock she has been angry, but completely mistress of herself.",
            "statement": "She had remained in control despite her anger."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann2,Ann3",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It is clear from the context that she got control of herself.",
                "\"Mistress of herself\" means that the she was in control of herself.",
                "\"She has been angry, but completely mistress of herself. \""
            ]
        }
    },
    "122928": {
        "annotation task": "natural language inference",
        "text": {
            "context": "A small page-boy was waiting outside her own door when she returned to it.",
            "statement": "When she came back to her door she found something waiting."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "contradiction,neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context mentions that someone was waiting, we don't know if something was waiting as well.",
                "The someone waiting was the small page-boy.",
                "It is not correct to say \"a small page-boy\" as \"something\".",
                "Maybe beside the page-boy, there are something else waiting.",
                "it was rather someone waiting"
            ]
        }
    },
    "139409": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Then, all the time, it was in the spill vase in Mrs. Inglethorp's bedroom, under our very noses? I cried.",
            "statement": "You mean we were so near it constantly?"
        },
        "number of annotators": 2,
        "annotators": "Ann2,Ann4",
        "number of annotations": 2,
        "annotations": {
            "Ann2": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "\"Under our very noses\" means that it was very near to us.",
                "\"under our very noses\" means in our very nearby surrendings"
            ]
        }
    },
    "80517": {
        "annotation task": "natural language inference",
        "text": {
            "context": "This doesn't look good.",
            "statement": "This looks really bad."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "entailment,neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context suggests that it doesn't look good, which may be normal or bad.",
                "Not good is not equivalent to really bad.",
                "This doesn't look good, but it can look just a little bit bad, or really bad.",
                "\"not look good\" implies \"bad\"",
                "possible exaggeration: \"not looking good\" might be not as serious as \"really bad\""
            ]
        }
    },
    "93236": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The word itself, tapa, is translated as  lid  and derives from the old custom of offering a bite of food along with a drink, the food being served on a saucer sitting on top of the glass like a lid.",
            "statement": "Tapas are large portions and are a very filling meal."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention the protion size of tapas.",
                "The meaning of the word tapa could have radically changed and now signify large portions.",
                "Tapas only offer \"a bite of food\", so it is not \"very filling\".",
                "Tapas are small portions"
            ]
        }
    },
    "142643": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The standard technology assumptions of scenario A were used by EIA in the development of the AEO2001 reference case projections.",
            "statement": "EIA used the standard technology assumptions to eliminate the AEO2001 reference case projections."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "contradiction",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.0,
                "1": 1.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context talks about developing AEO2001 reference case projections by using the assumptions, while the statement talks about elimination.",
                "The assumptions were used to develop the projections, not eliminate them.",
                "EIA used the standard technology assumptions to develop the AEO2001 reference case projections, not to \"eliminate\" them.",
                "It was used during the development of the AEO2021"
            ]
        }
    },
    "31113": {
        "annotation task": "natural language inference",
        "text": {
            "context": "One wag, J., wrote in to ask, Is there a difference between pests and airlines?",
            "statement": "J. thinks there is no difference between pests and airlines."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann1,Ann2,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment,neutral",
            "Ann2": "entailment",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.33299999999999996,
                "1": 0.667
            },
            "neutral": {
                "0": 0.33299999999999996,
                "1": 0.667
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The question from J. could be a rhetorical question to which the speaker already has a standard answer,  which is that there is no difference.",
                "The question from J. could be a simple question which needs to be answered.",
                "Literally taken the question is so absurd that it is most likely a rhetorical question implying that there really is no difference.",
                "In the context J only asked a questions. It is unknown about his opinion"
            ]
        }
    },
    "34776": {
        "annotation task": "natural language inference",
        "text": {
            "context": "We did not study the reasons for these deviations specifically, but they likely result from the context in which federal CIOs operate.",
            "statement": "The Context in which federal CIOs operate is no different from other CIOs."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context refers only to a hypothetical, we don't know whether it is the truth that the context in which federal CIOs operate is different.",
                "Other CIOs are not mentioned in the context.",
                "Federal CIOs can be semilar with other CIOs, also can be different from others.",
                "The contexts of different CIOs potentially lead to these deviations"
            ]
        }
    },
    "121422": {
        "annotation task": "natural language inference",
        "text": {
            "context": "it it like strange that it you're right in the middle of the mountains and it's so brown and dry but boy you just didn't feel",
            "statement": "you are in the right part of the mountains."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "contradiction",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention whether someone is in the right part or not.",
                "It's not clear whether right in the middle of the mountains is also the right part of the mountains.",
                "\"right\" in context means exactly, not the side of the mounstains, and your location is in the middle of the mountains.",
                "Not clear context"
            ]
        }
    },
    "104805": {
        "annotation task": "natural language inference",
        "text": {
            "context": "California is high",
            "statement": "California is hyped up!"
        },
        "number of annotators": 3,
        "annotators": "Ann2,Ann3,Ann4,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann2": "neutral",
            "Ann3": "entailment",
            "Ann4": "entailment,neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.33299999999999996,
                "1": 0.667
            },
            "neutral": {
                "0": 0.33299999999999996,
                "1": 0.667
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "High is not the same as hyped up but also does not exclude the possibility.",
                "\"high\" is one state  of \"hyped up\".",
                "If the \"high\" means in the context of spirit, that Californa is a hyped city",
                "If the \"high\" in the context mean that Californa is geographically high. Then it does not entail the statement"
            ]
        }
    },
    "115821": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The Chinese calendar was used to calculate the year of Japan's foundation by counting back the 1,260 years of the Chinese cosmological cycle.",
            "statement": "The calculation of Japan's year of foundation was very exact."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context suggests how the Japan' year was calculated, but doesn't mention whether this calculation was exact or not.",
                "It's not clear whether this calculation is exact.",
                "In context it just states how Japan's year of foundation is calculated, but it can not prove the method is exact or not.",
                "yes, because it was counted back 1,260 years of the Chinese cosmological cycle"
            ]
        }
    },
    "105179": {
        "annotation task": "natural language inference",
        "text": {
            "context": "I was to watch for an advertisement in the Times.",
            "statement": "I looked for an ad in my mailbox."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The  speaker indeed looked for an ad, but the context doesn't mention where the speaker looked for.",
                "The context talks about an obligation, but it is not clear whether the speaker then acts accordingly.",
                "I searched \"the Times\" not \"my mailbox\" for an ad.",
                "The Times is a newpaper"
            ]
        }
    },
    "109876": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Text Box 2.1: Gross Domestic Product and Gross National Product 48Text Box 4.1: How do the NIPA and federal unified budget concepts of",
            "statement": "This text displays how GDP and GNP is calculated."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't show the calculation of GDP and DNP.",
                "It's not clear what the text displays.",
                "In context it only offer a theme of GDP and GNP, but it can be about every aspect of the concepts, like the growth or the depression, and the calculation methods.",
                "No it doesn't. It only shows the what GDP and GNP stand for"
            ]
        }
    },
    "65353": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Don't take it to heart, lad, he said kindly.",
            "statement": "He was trying to console the lad."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context is indeed about the consolation.",
                "\"Don't take it to heart\" is an attempt of consolation.",
                "The words and the attitude of him can express his attempt to console the lad.",
                "\"don't take it to heart\" means not to overthink something.. So the lad should not overthink something that might bother him"
            ]
        }
    },
    "16494": {
        "annotation task": "natural language inference",
        "text": {
            "context": "It cannot be outlawed.",
            "statement": "It has to be made illegal."
        },
        "number of annotators": 2,
        "annotators": "Ann1,Ann4",
        "number of annotations": 2,
        "annotations": {
            "Ann1": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.0,
                "1": 1.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It cannot be outlawed means that it is legal.",
                "\"cannot be outlawed\" means \"cannot be made illegal\". So it has to stay legal"
            ]
        }
    },
    "60732": {
        "annotation task": "natural language inference",
        "text": {
            "context": "It started with The Wild Bunch : We sexualized violence, we made it beautiful.",
            "statement": "Violence is now look at in the positive due to The Wild Bunch."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "neutral",
            "Ann3": "entailment",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Making violence beautiful is is a positive way of looking at violence.",
                "It's not clear whether people have a positive opinion about violence, only because The Wild Bunch made it look beautiful.",
                "People made violence beautiful is a way to look at it positively.",
                "The current state of how violence is looked at is unknown; We only know it started to be looked at in a positive view"
            ]
        }
    },
    "88646": {
        "annotation task": "natural language inference",
        "text": {
            "context": "You see, he said sadly, \"you have no instincts.\"",
            "statement": "He said that I had no willpower."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "contradiction",
            "Ann3": "contradiction",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention the comments on willpower.",
                "He said \"no instincts\", not \"no willpower\".",
                "\"Instincts\" are not the same as \"willpower\".",
                "Instincts do not totally relate with willpower."
            ]
        }
    },
    "53074": {
        "annotation task": "natural language inference",
        "text": {
            "context": "ooh it's kind of tough to think of some of the others although i do watch some of some of those frivolous things uh like on Thursday nights at nine o'clock when i get home from aerobics i will watch uh Knots Landing",
            "statement": "I only watch frivolous things on Thursday nights."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only mentions that the speaker watch frivolous things on Thursdays, but doesn't mention if he/she watch them on other days as well.",
                "They could also watch frivolous things at other times.",
                "I do watch frivolous things on Thursday nights, but maybe I also watch at other time.",
                "Thursday is mentioned as example. There is no info about what he does on other weekday nights"
            ]
        }
    },
    "61818": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Kutchins and Kirk cite a particularly amusing example of such  Robert Spitzer, the man in charge of DSM-III , was sitting down with a committee that included his wife, in the process of composing a criteria-set for Masochistic Personality Disorder--a disease that was suggested for, but never made it into, the DSM-III-R (a revised edition).",
            "statement": "DSM-III-R is a book of personality disorders."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "entailment",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only mentions that Masochistic Personality Disorder is not in the book, it doesn't mention the content of the book.",
                "The book could contain also other things than only personality disorders.",
                "Masochistic Personality Disorder was suggested fot DSM-III-R, so the later should be about personality disorders.",
                "We only know that DSM-III is a personality disorders. But DSM-III-R could just be a medical book for all kinds of disorders"
            ]
        }
    },
    "98739": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The living is not equal to the Ritz, he observed with a sigh.",
            "statement": "The living is nothing compared to the glamour of the Ritz, he said sadly."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "neutral",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "We can infer from the sigh that the speaker did think that living here is not as good as in Ritz.",
                "It's not clear whether he sighed from disappointment or relief.",
                "The sign and the words proved that he didn't think the living is good as the Ritz.",
                "paraphrases"
            ]
        }
    },
    "108624": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Exhibit 3 presents total national emissions of NOx and SO2 from all sectors, including power.",
            "statement": "In Exhibit 3 there are the total regional emissions od NOx and SO2 from all sectors."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "contradiction",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.0,
                "1": 1.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Exhibit 3 shows the  total national emmissions, not regional emissions.",
                "The emissions are national not regional.",
                "Exhibit 3 is about \"national\" not \"regional\" emissions.",
                "It should be national emissions"
            ]
        }
    },
    "106013": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Ca'daan heard the Kal grunt and felt the horse lift.",
            "statement": "The Kal heard Ca'daan grunt."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "contradiction",
            "Ann3": "contradiction",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.25,
                "1": 0.75
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The grunt was from Kal, not Ca'daan.",
                "Ca'daan grunted not the Kal.",
                "It is Ca'daan that heard the Kal, not reverse.",
                "true, statement is a part of the context"
            ]
        }
    },
    "37407": {
        "annotation task": "natural language inference",
        "text": {
            "context": "5 are highly correlated during summer months in some areas.",
            "statement": "Six are correlated to winter in certain areas."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only suggests the correlation during summer months, not winter.",
                "It could be that six are correlated to winter, the context only speaks about summer.",
                "Winter months are not discussed in the context.",
                "No info about 6"
            ]
        }
    },
    "70590": {
        "annotation task": "natural language inference",
        "text": {
            "context": "China's civil war sent distressing echoes to Hong Kong.",
            "statement": "Japan fought a civil war."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction,neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "There is no mention of the Japan civil war in the context.",
                "It could be that Japan also fought a civil war, but it is not clear from the context.",
                "Japanese civil war is not mentioned in the context.",
                "China fought a civil war",
                "no info about japan; irrelavant"
            ]
        }
    },
    "23901": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Then Shuman claims that Linux provides no graphical user interface.",
            "statement": "They made accusations about the platform."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "We can infer from the claim in the context that the made accusations.",
                "That an operating system doesn't contain a graphical user interface can be called an accusation.",
                "\"Linux provides no graphical user interface\" can be a common description or an accusation, the attitude needs more background to prove.",
                "No info about the claim being an accusation"
            ]
        }
    },
    "23280": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Sphinxes were guardian deitiesinEgyptianmythologyandthis was monumentalprotection,standing73 m (240 ft)longand20 m (66 feet) high.",
            "statement": "Sphinxes guarded people."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "neutral",
            "Ann3": "entailment,neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context clearly suggests that Sphinxes were guardian deities.",
                "It is not clear from the context what exactly sphinxes guard.",
                "True, Sphinxes were guardian deities.",
                "In Egyptian mythology, Sphinxes were guardian deities, but in reality, maybe they guarded people, maybe not.",
                "No info about what Sphinxes guarded, it could be people or a temple."
            ]
        }
    },
    "135247": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The original wax models of the river gods are on display in the Civic Museum.",
            "statement": "They have models made out of clay."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention about the clay models.",
                "They could also have models made out of clay next to those made out of wax.",
                "In the context it just refered to a wax model, but provided no information about clay model.",
                "It is out of wax"
            ]
        }
    },
    "144753": {
        "annotation task": "natural language inference",
        "text": {
            "context": "When he's ready for a major strike, how many innocents do you suppose are going to suffer? To quote one of your contemporaries; 'The needs of the many outweigh the needs of the few.' '",
            "statement": "If he does a big strike, many people will suffer."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann1,Ann2,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment,neutral",
            "Ann2": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 0.667,
                "1": 0.333
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The question in the context might be a rhetorical question which suggests exactly that many innocents will suffer.",
                "If the question in the context is just a ordinary question, then we don't know if many people will suffer.",
                "\"how many innocents do you suppose are going to suffer\" implies that \"many people will suffer\".",
                "The question in the context implies that a major strike leads to suffering of many innocent people"
            ]
        }
    },
    "15100": {
        "annotation task": "natural language inference",
        "text": {
            "context": "but uh that has been the major change that we have noticed in gardening and that's about the extent of what we've done just a little bit on the patio and uh and waiting for the the rain to subside so we can  mow we after about a month we finally got to mow this weekend",
            "statement": "We have not done much gardening yet because of the rain."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context mentions that them have only done a little gardening and needed to wait for the rain to die down, which suggests that the reason was raining.",
                "The speakers says that they did only a little gardening because of the rain.",
                "We have done just a little bit, but the reason can be the rain or something else like temperature.",
                "We plan to mow after the rain subside"
            ]
        }
    },
    "28507": {
        "annotation task": "natural language inference",
        "text": {
            "context": "It is, as you see, highly magnified.",
            "statement": "It is plain for you to see that it is amplified."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both the context and the statement suggest that it is magnified.",
                "The statement is a paraphrase of the context.",
                "\"highly magnified\" can be interpreted \"amplified\".",
                "It can be seen, and it is magnified"
            ]
        }
    },
    "123748": {
        "annotation task": "natural language inference",
        "text": {
            "context": "There are many such at the present time.",
            "statement": "There are over two currently."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The word \"many\" in the context indicates more than one, but we don't know if there are more than two.",
                "If there are many then there are certainly over two.",
                "\"Many\" ususally is over two.",
                "Many means more than two"
            ]
        }
    },
    "111338": {
        "annotation task": "natural language inference",
        "text": {
            "context": "He threw one of them and shot the other.",
            "statement": "He kept his gun holstered."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "neutral",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.25,
                "1": 0.75
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The statement indicates that he didn't use the gun.",
                "It is not clear whether he had other shooting weapons than his gun.",
                "He \"shot\" one, so he need to pull his gun out of the holster.",
                "He shot one of them"
            ]
        }
    },
    "52171": {
        "annotation task": "natural language inference",
        "text": {
            "context": "For such a governmentwide review, an entrance conference is generally held with applicable central agencies, such as the Office of Management and Budget (OMB) or the Office of Personnel Management.",
            "statement": "An entrance conference is held with specialized agencies."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment,neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Applicable central agencies mentioned in the context are indeed specialized agencies.",
                "The statement is a paraphrase of the context with less detail.",
                "The  Office of Management and Budget (OMB) or the Office of Personnel Management are specialized agencies.",
                "For a governmentwide review the statement is true, but for a normal entrance conference, it is could be held with specialized agencies or any temporary agencies.",
                "the conference is held with applicable central agencies. They can then be considered as specialized"
            ]
        }
    },
    "90548": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Splendid!",
            "statement": "The speaker is excited by the situation."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The comment \"splendid\" and the exclamation mark indicate that the speaker is excitied.",
                "The speaker calls out \"splendid!\" so they are probably excited about the situation.",
                "\"Splendid\" means very good, and the exclamation mark also conveys the speaker's excitement.",
                "Splendid entails excitement"
            ]
        }
    },
    "81842": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Answer? said Julius.",
            "statement": "Julius already knew the answer."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann1,Ann2,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction,entailment",
            "Ann2": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.667,
                "1": 0.333
            },
            "entailment": {
                "0": 0.667,
                "1": 0.333
            },
            "neutral": {
                "0": 0.33299999999999996,
                "1": 0.667
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Julius is asking for answer, the question might indicate that he didn't know the answer.",
                "Julius is asking someone for answer, he might already know the right answer and wants to know if others do.",
                "It is not clear whether Julius is really asking for the answer.",
                "It was rather a question from Julius, we do not know if he knows the answer"
            ]
        }
    },
    "113644": {
        "annotation task": "natural language inference",
        "text": {
            "context": "so do you have do you have the long i guess not not if there's see i was raised in New York but i guess up there you all don't have too long of a growing season do you",
            "statement": "I am looking for a written guide to growing plants in different places in the country."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann2,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Althought the speaker asks about the growing season in the context, the reason is not mentioned.",
                "It is not clear whether the speaker is looking for a guide or simply asking a question.",
                "not clear context; potential irrelevance"
            ]
        }
    },
    "47260": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The good news, however, can be found in reports like this one.",
            "statement": "The good news is that the puppy's life was able to be saved."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention what is the good news.",
                "It is not clear what the good news is.",
                "The topic of good news can be puppy's life or cat's life or parrot's life or anything.",
                "No info about a puppy in the context"
            ]
        }
    },
    "129081": {
        "annotation task": "natural language inference",
        "text": {
            "context": "right oh they've really done uh good job of keeping everybody informed of what's going on sometimes i've wondered if it wasn't almost more than we needed to know",
            "statement": "After sharing all information with everyone, I think I may have shared too much."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "neutral",
            "Ann3": "contradiction,neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.25,
                "1": 0.75
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It's not the speaker who shares the information with everybody, it's them.",
                "The speaker talks about \"they\" sharing the information not about themselves.",
                "It is not I but they who shared all information.",
                "It is not given in the context whether I have shared all information, or do I think I have shared too much.",
                "\"I\" did not share the information, \"they\" did"
            ]
        }
    },
    "18189": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The important thing is to realize that it's way past time to move it.",
            "statement": "It has not been moved yet in the past."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context suggests that it's no longer the time to move, which indicates that it hasn't been moved yet.",
                "It might have been moved in the past and now is needed to be moved again.",
                "The context states it is not time to move it now, but the history of movement is not refered to.",
                "Because it is too late to move it now, so in the past it was not moved"
            ]
        }
    },
    "91797": {
        "annotation task": "natural language inference",
        "text": {
            "context": "We know they will have to come from the south but that gives them a space as wide as the town in which to launch their attack.",
            "statement": "The south is totally protected against an attack."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "contradiction",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.25,
                "1": 0.75
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context suggests that the south is the right place to launch the attack, which indicates that the south cannot totally defend itself against an attack.",
                "The space in the south makes it not totally protected.",
                "In the context, it is analysed that they will attack from the south, but the protection of the south is not measurable.",
                "There is a space in the south as wide as the town to launch their attack"
            ]
        }
    },
    "917": {
        "annotation task": "natural language inference",
        "text": {
            "context": "eligible individuals and the rules that apply if a state does not substantially enforce the statutory requirements.",
            "statement": "It does not matter whether or not a state enforces the statutory requirements."
        },
        "number of annotators": 3,
        "annotators": "Ann2,Ann3,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann2": "contradiction",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.33299999999999996,
                "1": 0.667
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.667,
                "1": 0.333
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It matters whether the state enforces the statuory requirements because then other rules apply.",
                "We can only make sure, if a state does not enforces the statutory requirements, the rules apply; otherwise, we don't know whether the rules take effect or not.",
                "There are rules that apply if the state does not enforces the statutory requirements, meaning it does matter that the state do enforces these requirements. Because if not, then there is no need for those rules to exist"
            ]
        }
    },
    "89995": {
        "annotation task": "natural language inference",
        "text": {
            "context": "yeah then you don't have you don't have that mess to clean up when you use an oil oil base painting and boy i'll tell you oh",
            "statement": "Typically oil based paints are easy to work with and clean up."
        },
        "number of annotators": 2,
        "annotators": "Ann2,Ann4",
        "number of annotations": 2,
        "annotations": {
            "Ann2": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It is not clear whether oil based paints are also easy to work with.",
                "Because in the context, \"you\" didn't use oil based paints, so \"you\" have a mess to clean up"
            ]
        }
    },
    "33822": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Why shouldn't he be?",
            "statement": "He doesn't actually want to be that way."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann2,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "His thought is not mentioned in the context.",
                "It's not clear whether he wants to be like that.",
                "No info about his intention"
            ]
        }
    },
    "26143": {
        "annotation task": "natural language inference",
        "text": {
            "context": "However, the associated cost is primarily some of the costs of assessing and collecting duties on imported merchandise, such as the salaries of import specialists (who classify merchandise) and the costs of processing paperwork.",
            "statement": "the associated cost is how much people spend relative to this amount"
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann3,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "contradiction",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.0,
                "1": 1.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The associated cost is not in the general sense of how much people spend, but is specifically defined in the context.",
                "In context, \"the associated cost is primarily some of the costs of assessing and collecting duties on imported merchandise\", it is about the goods, so the description \"relative to this amount\" is different from the definition given before.",
                "It's the cost of assessing and collecting duties"
            ]
        }
    },
    "100768": {
        "annotation task": "natural language inference",
        "text": {
            "context": "well in a way you can travel light",
            "statement": "You won't need to pack much."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann2,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment,neutral",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Traveling light means exactly no need to pack much.",
                "If you can travel light, then you don't need to pack much.",
                "\"In a way\" suggests that it could be not the usual meaning of travelling light.",
                "\"Travel light\" means exactly not packing much.",
                "Traveling light entails traveling with a small package"
            ]
        }
    },
    "82510": {
        "annotation task": "natural language inference",
        "text": {
            "context": "although the uh it's uh it we almost one day we painted the house to uh we painted we painted the whole inside and it had all this dark trim we thought uh you know we did the one wall but the other trim i'm trying to think i think i think we left most of it because it gets to be uh they don't do that in the newer houses now we don't the uh mold everything is white in a new house everything is white",
            "statement": "It took over a day to paint the house"
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "neutral",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.25,
                "1": 0.75
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "They took almost one day to paint the house, which is less than a day.",
                "If it took \"almost a day\" then it took less not more than one day.",
                "No, it took almost one day to paint the house.",
                "It took almost one day, so less than a day"
            ]
        }
    },
    "102563": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The judge gave vent to a faint murmur of disapprobation, and the prisoner in the dock leant forward angrily.",
            "statement": "The judge ordered the court to be silent."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The judge only made a murmur and didn't give a direct order.",
                "The context does not say anything about slience.",
                "The judge \"gave vent to a faint murmur of disapprobation\" can not be taken as an order for quiet.",
                "He let the crowd murmur"
            ]
        }
    },
    "48553": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Keep your eyes open for Renaissance details, grand doorways, and views into lovely courtyards.",
            "statement": "All of the doorways and courtyards have been completely remodeled since the Renaissance."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "contradiction",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention if the doorways and courtyards were remodeled since the Renaissance.",
                "If the doorways and courtyards would have been remodeled since the Renaissance, then it wouldn't make sense to look for the Reneaissance details.",
                "It is not garanteed that all of the objects mentioned in the context have been completely remodeled, maybe part of them kept the same as before the Renaissance.",
                "No info about whether they have been remodeled or not. They could also be built in the Renaissance time"
            ]
        }
    },
    "54458": {
        "annotation task": "natural language inference",
        "text": {
            "context": "This one ended up being surprisingly easy!",
            "statement": "This question was very easy to answer."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both context and statement suggest the easiness of what they talk about.",
                "The statement is a paraphrase of the context.",
                "It is true because the question \"ended up being surprisingly easy\".",
                "A question ended up easy, meaning it's easy to answer"
            ]
        }
    },
    "17576": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The percent of total cost for each function included in the model and cost elasticity (with respect to volume) are shown in Table 1.",
            "statement": "Table 1 also shows a picture diagram for each function."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "contradiction",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention if Table 1 presents the picture diagram.",
                "It is not clear in which way the values are presented in Table 1.",
                "No, Table 1 shows the \"the percent of total cost for each function\" and \"cost elasticity\", not picture diagram.",
                "And this diagram shows the cost for each function"
            ]
        }
    },
    "103431": {
        "annotation task": "natural language inference",
        "text": {
            "context": "In addition, the senior executives at these organizations demonstrated their sustained commitment to financerelated improvement initiatives by using key business/line managers to drive improvement efforts, attending key meetings, ensuring that the necessary resources are made available, and creating a system of rewards and incentives to recognize those who support improvement initiatives.",
            "statement": "This system of rewards and incentives will hopefully improve company performance."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "entailment,neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The rewards and incentives will be given to those who support improvement initiatives, so the company is more likely to improve their proferance under these rewards incentives.",
                "The system of rewards and incentives",
                "It is not clear whether the speaker hopes that the rewards and incentives improve company performance.",
                "The measures mentioned in the context can not prove their effect \"will hopefully improve company performace\" or not.",
                "The measurement taken is thought for finance-related improvenemt."
            ]
        }
    },
    "11297": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Transforming Control of Public Health Programs Raises Concerns (",
            "statement": "Everyone is content with the change of public health programs."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "contradiction",
            "Ann3": "contradiction",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention whether people are satisfied with the programs.",
                "The program raises concerns, so not everyone is content with it.",
                "No, because the change of control of Public Health Programs \"raises concerns\", there must be someone not content with it.",
                "exaggeration: \"Raising concerns\" doesn't mean that everyone is concerned. Maybe someone is not concerned and the context will still hold"
            ]
        }
    },
    "32889": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Extremely limited exceptions to the authority are established in 31 U.S.C.",
            "statement": "They were trying to eliminate all exceptions."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only mentions that there are extremely limited exceptions, but not whether someone wanted to eliminate the exceptions.",
                "It is not clear whether they tried to eliminate the exceptions.",
                "The limitation of exceptions will not definitey lead to eliminating all exceptions.",
                "The motive of them is unknown. Maybe they didn't try to eliminate all, but just eliminate the unnecessary ones."
            ]
        }
    },
    "19": {
        "annotation task": "natural language inference",
        "text": {
            "context": "On the northern slopes of this rocky outcropping is the site of the ancient capital of the island, also called Thira, which dates from the third century b.c. (when the Aegean was under Ptolemaic rule).",
            "statement": "Is the site of the ancient asteroid impact, also called Thira."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It is not clear if the site of the ancient capital mentioned in the context is also the site of the ancient asteroid impact.",
                "It is not clear whether there was an asteroid impact on Thira.",
                "\"The ancient asteroid impact\" is not mentioned in the context.",
                "It is the ancient capital of the island, not an asteroid impact"
            ]
        }
    },
    "32754": {
        "annotation task": "natural language inference",
        "text": {
            "context": "After shuttering the DOE, Clinton could depict himself as a crusader against waste and bureaucracy who succeeded where even Reagan failed.",
            "statement": "Clinton shuttered the DOE to move against waste."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only suggests that Clinton can describe the reason as being against waste, but it is not clear if this reason is actually the truth.",
                "If shuttering the DOE allowed Clinton to appear as a crusader against waste then it probably was done to do something against waste.",
                "\"Clinton could depict himself as a crusader against waste and bureaucracy\" doesn't mean it is his true intention is \"to move against waste.\" Perhaps he really is just trying to save energy, but maybe he has other intentions.",
                "He shuttered the DOE, and were depicted as crusader against waste"
            ]
        }
    },
    "12815": {
        "annotation task": "natural language inference",
        "text": {
            "context": "yeah well that's my uh i mean every time i've tried to go you know it's always there's there's always a league bowling",
            "statement": "Every time I try to go bowling there are leagues only and I can't bowl."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both context and statement suggests that the speaker can't go bowling because of the leagues.",
                "The statement is a paraphrase of the context.",
                "Usually if the place is occupied by a league, then other people can not play there.",
                "Maybe the place is big enough, so even the there is a league bowling, I can still bowl"
            ]
        }
    },
    "135898": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The end is near!  Then a shout went up, and Hanson jerked his eyes from the gears to focus on a group of rocs that were landing at the far end of the camp.",
            "statement": "It's all over, Hanson whispered as he stared at the gears."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann2,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.667,
                "1": 0.333
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.33299999999999996,
                "1": 0.667
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It is not clear from the context if \"It's all over\" was Hanson's whisper.",
                "It's not clear whether or what Hanson whispered",
                "The end is near, meaning it's soon to be over but not all over yet"
            ]
        }
    },
    "82830": {
        "annotation task": "natural language inference",
        "text": {
            "context": "In the 19th century, when Kashmir was the most exotic hill-station of them all, the maharaja forbade the British to buy land there, so they then hit on the brilliant alternative of building luxuriously appointed houseboats moored on the lakes near Srinagar.",
            "statement": "The maharaja allowed the British to build houseboats on the lakes."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only mentions what the maharaja forbade, not what he allowed.",
                "The British did build houseboats on the lakes, so the maharaja must at least have tolerated it.",
                "The British are allowed to moor houseboats on the lakes, but whether it is allowed for them to \"build\" houseboats is not mentioned.",
                "The British was forbidden to buy land in Kashmir, so they buit houseboats on the lakes near Srinagar. Because they built it, so they must have been allowed"
            ]
        }
    },
    "74509": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Under the budget deal, by 2002, national defense will consume about $273 billion a year compared with $267 billion now.",
            "statement": "The United States national defense budget will increase by 6 billion dollars."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The budget increases from 267 to 273 billion, an increase of $6 billion.",
                "$273 billion - $267 billion = $6 billion",
                "It is true, because 273-267 = 6.",
                "right now the budget is 267 Million, in 2002 it will be 273 Million, making the incease by 6 million"
            ]
        }
    },
    "117576": {
        "annotation task": "natural language inference",
        "text": {
            "context": "but i don't know you know  maybe you could do that for a certain period of time but i mean how long does that kind of a thing take you know to to um say to question the person or to get into their head",
            "statement": "It might take a long time to do that because getting inside a person's head takes time."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The statement seems to be a continuation of the context, the reason why it meight take a long time is not mentioned in the context.",
                "It's not clear whether it will take to get into a person's head.",
                "It is true, because the possibility exists, that it takes a long time to get into their head.",
                "Context states it is not known how long it would take, so it might take a long time"
            ]
        }
    },
    "50484": {
        "annotation task": "natural language inference",
        "text": {
            "context": "All of our many earnest experiments produced results in line with random chance, they conclude.",
            "statement": "The experiments proved it was a much better predictor."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "contradiction",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.0,
                "1": 1.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The results are in line with random chance, which indicates that it was not a better predictor, it was just random.",
                "If results were in line with random chance, it was not a good predictor.",
                "No, if it was a much better predictor, then the earnest experiments should produce results much better than random chance.",
                "Because the results are random, so the predictor is not good"
            ]
        }
    },
    "98445": {
        "annotation task": "natural language inference",
        "text": {
            "context": "It seeks genuine direct elections after a period that is sufficient to organize alternative parties and prepare a campaign based on freedom of speech and other civil rights, the right to have free trade unions, the release of more than 200 political prisoners, debt relief, stronger penalties for corruption and pollution, no amnesty for Suharto and his fellow thieves, and a respite for the poor from the hardest edges of economic reform.",
            "statement": "The only thing that can our society is more power to the presidential electors."
        },
        "number of annotators": 2,
        "annotators": "Ann1,Ann4",
        "number of annotations": 2,
        "annotations": {
            "Ann1": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't contain any information about the power of the presidential electors.",
                "There is stronger penalties for corruption, so more restrictions on the presidential electors."
            ]
        }
    },
    "7856": {
        "annotation task": "natural language inference",
        "text": {
            "context": "but how do you know  the good from the bad",
            "statement": "Why care if it's good or bad?"
        },
        "number of annotators": 1,
        "annotators": "Ann4",
        "number of annotations": 1,
        "annotations": {
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Two different questions"
            ]
        }
    },
    "8257": {
        "annotation task": "natural language inference",
        "text": {
            "context": "'But if White has any designs at all on living, he'll be as far from Little as he can possibly get by now.'",
            "statement": "White should be afraid to come back to Little."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context suggests that White will die if he is with Little, we can infer that he should be afriaid of being with Litte.",
                "It sounds like Little is after White's life.",
                "In context, White should be far away from Little if he wants to live, so coming back to Little should be terrible, and he should be afraid of that.",
                "If White has any planning on life, then he should not come back to Little. It means that Little is not a good place for living, so he should be afraid to come back to this place"
            ]
        }
    },
    "141110": {
        "annotation task": "natural language inference",
        "text": {
            "context": "yes well yeah i am um actually actually i think that i at the higher level education i don't think there's so much of a problem there it's pretty much funded well there are small colleges that i'm sure are struggling",
            "statement": "Small colleges usually have trouble with funding and resources."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both the context and the statement suggest that small colleges have funding issues.",
                "The speaker doesn't say anything about 'usually'. They only assert that there are some small colleges that are struggling.",
                "Some small colleges are struggling, but it can be a common phenomenon, or may be quite rare.",
                "The speaker mentioned his college is well funded and then said some small colleges are struggling. It can not be concluded that all small colleges usually have trouble with funding"
            ]
        }
    },
    "84055": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Even if auditors do not follow such other standards and methodologies, they may still serve as a useful source of guidance to auditors in planning their work under GAGAS.",
            "statement": "GAGAS requires strict compliance for auditors to follow."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context suggests that auditors need to plan their work under GAGAS.",
                "It is not clear how strict GAGAS is.",
                "The context is not about GAGAS's requirements, we only know under GAGAS auditors can use other standards and methodologies as reference.",
                "GAGAS can also only serve as guidance to the auditors"
            ]
        }
    },
    "143789": {
        "annotation task": "natural language inference",
        "text": {
            "context": "What a brilliantly innocuous metaphor, devised by a master manipulator to obscure his manipulations.",
            "statement": "The metaphor was created by the manipulator to convince people of something."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "contradiction",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Manipulation does aim to make people believe certain things.",
                "The metaphor was created to hide the manipulations, not for manipulating directly.",
                "The purpose of the metaphor in the context is described as \"to obscure his manipulations\", whether he wanted to \"convince people of something\" is not given.",
                "It was created to hide his true intention to manipulate"
            ]
        }
    },
    "49396": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The road along the coastline to the south travels through busy agricultural towns and fishing villages untouched by tourism.",
            "statement": "There are no tourists on the road through the agricultural towns and fishing villages."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "\"untouched\" implies that these places are not visited by tourists.",
                "The towns and villages are untouched by tourism so probably the route going through them is also relatively free of tourists.",
                "We can only know that the fishing villages are not the destination of tourists, but we can't promise there are no tourists on the road through it.",
                "No info about the road being touched by the tourists or not. The towns and villages are not touched by the tourists"
            ]
        }
    },
    "73444": {
        "annotation task": "natural language inference",
        "text": {
            "context": "well they're so close to an undefeated undefeated season they can taste it and they wanna make history so i don't think they're gonna lack for motivation",
            "statement": "Unless they suffer any losses, they'll remain motivated."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention what would happen if they suffer any losses, they may or may not lose motivation.",
                "They are motivated by being undefeated. This will persist unless they are defeated.",
                "After suffering any losses they maybe will lose motivation, but maybe still remain motivated.",
                "They are motivated to get a full win season. So if they suffer loss, then they can no longer make history, so their movitation may lack"
            ]
        }
    },
    "79106": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The woman rolled and drew two spears before the horse had rolled and broken the rest.",
            "statement": "They were in rotation on the ground grabbing their weapons."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "contradiction",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The woman and the horse were grabbing the spears. They rolled indicates that they were on the ground.",
                "The horse most likely wasn't grabbing a weapon.",
                "The context only described a woman, we don't know who are \"they\" in the statement.",
                "The women and horse both rolled. And the woman grabbed two spears"
            ]
        }
    },
    "82230": {
        "annotation task": "natural language inference",
        "text": {
            "context": "However, the other young lady was most kind.",
            "statement": "I received a warm welcome from the other young lady who was present."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention what the young lady did to the speaker.",
                "Only because the woman was kind, she did not have to extend a warm welcome.",
                "The other young lay was most kind, but she could do anything to show her kindness, maybe to give warm welcome, maybe others.",
                "The young lady was kind, but we don't know what she did"
            ]
        }
    },
    "14280": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The author began with a set of hunches or hypotheses about what can go wrong in agency management, and what would be evidence supporting-or contradicting-these hypotheses.",
            "statement": "The hunches provided by the author weren't realistic as it pertains to agency management."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It is not clear whether the hunches provided by the author were realistic or not.",
                "It is not clear how realistic the hypotheses were.",
                "The judgement of the hunches provided by the author is not given in the context.",
                "The hunches could be realistic, as the author provide potential evidence supporting these hypotheses"
            ]
        }
    },
    "19668": {
        "annotation task": "natural language inference",
        "text": {
            "context": "okay and and i think we just hang up i don't think we have to do anything else",
            "statement": "We need to wait until they tell us what to do."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "contradiction",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.25,
                "1": 0.75
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't talk about why they don't have to do anything.",
                "They explicitly say that they have to only hang up and not do anything else.",
                "\"I don't think we have to do anything else\" means we don't need to nothing, so we don't need to wait.",
                "We don't need to do anything, meaning also not waiting for them to tell us what to do"
            ]
        }
    },
    "12562": {
        "annotation task": "natural language inference",
        "text": {
            "context": "David Cope, a professor of music at the University of California at Santa Cruz, claims to have created a 42 nd Mozart symphony.",
            "statement": "Music Professor David Cope who specializes in Mozart's music claims to have created Mozart's 42nd symphony."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "They context doesn't mention the speciality of Professor David Cope.",
                "The statement is a paraphrase of the context.",
                "Whether David Cope specialized in Mozart's music is not given in the context.",
                "No info about the specialization of the Music Professor"
            ]
        }
    },
    "111693": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The conspiracy-minded allege that the chains also leverage their influence to persuade the big publishers to produce more blockbusters at the expense of moderate-selling books.",
            "statement": "Big publishers want to produce more high budget films, even if that means badly selling books."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "According to the claim of conspiracy-minded, it's the chains that want to produce more blockbusters, not the big publishers. And we don't know if the claim is true.",
                "The statement is a paraphrase of the context.",
                "Producing film is not mentioned in the context, so we don't know the attitude of big publishers to it.",
                "It is at the expense of moderate-selling books, not badly selling books"
            ]
        }
    },
    "129601": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Took forever.",
            "statement": "Lasted too long"
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann2,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "\"Forvever\" is indeed too long.",
                "The statement is a paraphrase of the context.",
                "took forever\" is an expression to say that it is taking too long"
            ]
        }
    },
    "26372": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Just like we have hairpins and powder-puffs.\" Tommy handed over a rather shabby green notebook, and Tuppence began writing busily.",
            "statement": "Tommy handed Tuppence an empty shabby green notebook."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "We don't know if the notebook is empty.",
                "The statement is a paraphrase of a part of the context.",
                "The shabby green notebook can be empty or not.",
                "No info about whether the notebook is empty"
            ]
        }
    },
    "74534": {
        "annotation task": "natural language inference",
        "text": {
            "context": "And far, far away- lying still on the tracks- was the back of the train.",
            "statement": "The train wasn't moving but then it started up."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "contradiction",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It is not clear whether the train moved or not.",
                "The tarin was lying still, so it didn't start up.",
                "The movement of the train is not given in the context.",
                "No info about whether the train started up"
            ]
        }
    },
    "63469": {
        "annotation task": "natural language inference",
        "text": {
            "context": "It lacked intelligence, introspection, and humor--it was crass, worthy of Cosmopolitan or Star . I do have a sense of humor, but can only appreciate a joke when it starts with a grain of truth.",
            "statement": "The article won a Pulitzer Prize."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "contradiction",
            "Ann3": "contradiction",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention the Pulitzer Prize.",
                "Apparantely, the article was very bad, so it most likely did not win a Pulitzer.",
                "The article was lacked intelligence, introspection, and humor, and that is not the taste of Pulitzer Prize.",
                "No info about the article winnning a prize"
            ]
        }
    },
    "141321": {
        "annotation task": "natural language inference",
        "text": {
            "context": "It will be held in the Maryland woods, and the telecast will consist of jittery footage of the contestants' slow descent into madness as they are systematically stalked and disappeared/disqualified by Bob Barker.",
            "statement": "The show will be set in the woods north of Boston."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "contradiction",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.0,
                "1": 1.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The show will be held in the Maryland woods, not the woods north of Boston.",
                "Boston is not near Maryland.",
                "No, the show \"will be held in the Maryland woods\".",
                "Maryland is to the south of Boston"
            ]
        }
    },
    "68946": {
        "annotation task": "natural language inference",
        "text": {
            "context": "It has served as a fortress for the Gallo-Romans, the Visigoths, Franks, and medieval French (you can see the layers of their masonry in the ramparts).",
            "statement": "The fortress was built by the medieval French in 1173."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "contradiction",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention when and by whom the fortress was built.",
                "The Gallo-Romans were much earlier than 1173, so the fortress was built earlier.",
                "The building year of the fortress is not given in the context.",
                "No info about who built the fortress. The medieval French only had used it, so did Gallo-Romans."
            ]
        }
    },
    "41975": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Tommy realized perfectly that in his own wits lay the only chance of escape, and behind his casual manner he was racking his brains furiously.",
            "statement": "He'd been stuck for hours, starting to feel doubt crawl into his mind."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "There is no mention in the context of how long he was stuck.",
                "It is not clear whether Tommy experienced doubt.",
                "We don't know from which situation need Tommy escape , and the doubt didn't show up in the context.",
                "No info about how long he had been stuck."
            ]
        }
    },
    "100349": {
        "annotation task": "natural language inference",
        "text": {
            "context": "He touched it and felt his skin swelling and growing hot.",
            "statement": "His skin was burning."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The reason that hes skon swelling and growing hot is not clear. It may or may not be because his skin was burning.",
                "The skin grew hot, so it was burning.",
                "His could be burning or maybe he was taken a bug's bite",
                "his skin was swelling and growning hot. They are signs of burning"
            ]
        }
    },
    "58954": {
        "annotation task": "natural language inference",
        "text": {
            "context": "For an authentic feel of old Portugal, slip into the cool entrance hall of theimpressive Leal Senado ( Loyal Senate building), a fine example of colonial architecture.",
            "statement": "All that remains of  Leal Senado is old ruins."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "contradiction",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The current status of Leal Senado is not mentioned.",
                "Leal Senado is impressive and has an entrance hall, so it can hardly be only ruins.",
                "The condition of Leal Senado is not mentioned in the context.",
                "There is entrance hall of Leal Senado, so it can not be ruins"
            ]
        }
    },
    "74377": {
        "annotation task": "natural language inference",
        "text": {
            "context": "no chemicals and plus then you can use it as a fertilizer and not have to worry about spreading those chemicals like on your lawn or your bushes or whatever",
            "statement": "We don't want to use chemicals on our lawn"
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann2,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "We don't need to worry about using them on the lawn because they are not chemicals, which suggests that we don't want to use chemicals on the lawn.",
                "The speaker says that they might worry about spreading the chemicals on the lawn, so they don't want that.",
                "when using chemicals, we need to worry about spreading the chemicals on the lawn"
            ]
        }
    },
    "130680": {
        "annotation task": "natural language inference",
        "text": {
            "context": "We also have found that leading organizations strive to ensure that their core processes efficiently and effectively support mission-related outcomes.",
            "statement": "Leading organizations want to be sure their processes are successful."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Leading organizations strive to ensure the success of their processes.",
                "The statement is a paraphrase of the context.",
                "True, because leading organizations strive to ensure their processes.",
                "The organizations strive to ensure their process to support the outcomes. So the process needs to be successful to provide support for the outcomes"
            ]
        }
    },
    "49227": {
        "annotation task": "natural language inference",
        "text": {
            "context": "well that's uh i agree with you there i mean he didn't have the surrounding cast that Montana had there's no doubt about that",
            "statement": "I agree that he didn't have the same support as Montana, but he did well."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention whether he did well or not.",
                "It is not clear whether the speaker thinks that he did well.",
                "I agree that he didn't have the same support as Montana, but he could did well, or bad.",
                "No info about where he did well or not"
            ]
        }
    },
    "124853": {
        "annotation task": "natural language inference",
        "text": {
            "context": "H-2A agricultural workers are required to maintain a foreign residence which they have no intention of abandoning.",
            "statement": "Permanent foreign residence is required for some types of agricultural work visas."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "\"H-2A\" is a type of agricultural work visa that mentioned in the statement.",
                "The statement is a paraphrase of the context.",
                "H-2A agricultural workers need permanent foreign residence prove that some types of agricultural need permanent foreign residence.",
                "It's those agricultural workers with H-2A visas needs to maintain the residence as part of their work but not a requirement for their application for the visa"
            ]
        }
    },
    "117892": {
        "annotation task": "natural language inference",
        "text": {
            "context": "No, Dave Hanson, you were too important to us for that.",
            "statement": "No, Dave Hanson, we couldn't risk your life becaus you are too important to us."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention anything about risking life.",
                "It is not clear whether the speakre talks about risking Hanson's life.",
                "Dave's is too important to us for \"that\", but that can be anything, not definitely about living or death.",
                "It is not mentioned if Dave is going to risk his life for that"
            ]
        }
    },
    "111243": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The pope, suggesting that Gen.",
            "statement": "Gen is being suggested by the Pope."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both the context and the statement suggest that the pope suggested  the Gen.",
                "The statement is a paraphrase of the context.",
                "True, it is a switch of active voice and passive voice.",
                "paraphrases"
            ]
        }
    },
    "77654": {
        "annotation task": "natural language inference",
        "text": {
            "context": "but there's no uh inscriptions or or dates or anything else",
            "statement": "There aren't any dates on it?"
        },
        "number of annotators": 1,
        "annotators": "Ann4",
        "number of annotations": 1,
        "annotations": {
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "question is not a statement"
            ]
        }
    },
    "27335": {
        "annotation task": "natural language inference",
        "text": {
            "context": "is there still that type of music available",
            "statement": "Is that genre of music still a thing?"
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann2,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.33299999999999996,
                "1": 0.667
            },
            "neutral": {
                "0": 0.667,
                "1": 0.333
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both the context and statement are about the same question.",
                "The statement is a paraphrase of the context.",
                "\"is it a thing\" means \"is it popular\". A thing can be available but not popular"
            ]
        }
    },
    "87332": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Strange as it may seem to the typical household, capital gains on its existing assets do not contribute to saving as measured in NIPA.",
            "statement": "NIPA considers cat fur when it defines savings."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "entailment",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Cat fur is not mentioned in the context.",
                "Cat furs are not mentioned at all.",
                "Yes, because cat fur based on cats, which are existing assets, so it will not be counted by NIPA.",
                "Not clear statement"
            ]
        }
    },
    "53499": {
        "annotation task": "natural language inference",
        "text": {
            "context": "my goodness it's hard to believe i didn't think there was anybody in the country who hadn't seen that one",
            "statement": "I thought I was the only one in this country who had seen it."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann3,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "contradiction",
            "Ann3": "contradiction",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.33299999999999996,
                "1": 0.667
            },
            "entailment": {
                "0": 0.667,
                "1": 0.333
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The statement is false because the speaker thought that everyone had seen that one.",
                "I thought everybody in this country had seen it, so I am not the only one.",
                "I didn't think there was anybody seen that one. So I thought I was the only one in this country who had seen it"
            ]
        }
    },
    "65199": {
        "annotation task": "natural language inference",
        "text": {
            "context": "and i look back on that and i bought shoes i went shopping i did not need that money i did not need it i didn't need it i shouldn't have even qualified to get it i didn't need it and it would have been a little rough i might have eaten some bologna instead of roast beef out of the deli but i did not need it and as i look back now now we're paying that back i told my son if you have to live in the ghetto to go to college do it but don't take out ten thousand dollars in loans don't do it and i don't i hope don't think he'll have to do that but i just so like we might if we didn't have those loans we could have saved in the last five years the money for that and i believe we would have because God's really put it in our heart not to get in debt you know but we have friends at church that do this on a constant basis that are totally debt free and they pay cash for everything they buy",
            "statement": "I am envious of all my debt-free churchgoing friends."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann2,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention the speaker's attitude toward the debt-free friends at church.",
                "It is not clear whether the speaker is envious.",
                "It is not known if i am envious of my debt-free friends"
            ]
        }
    },
    "88050": {
        "annotation task": "natural language inference",
        "text": {
            "context": "If you have any questions about this report, please contact Henry R. Wray, Senior Associate General Counsel, at (202) 512-8581.",
            "statement": "Henry R. Wray can be reached at (555) 512-8581."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "contradiction",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.0,
                "1": 1.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The number of Henry R. Wray mentioned in the statement is wrong",
                "The phone number starts with (202) not with (555).",
                "The number is wrong, it should be (202) 512-8581 not (555) 512-8581.",
                "wrong phone numbers"
            ]
        }
    },
    "34094": {
        "annotation task": "natural language inference",
        "text": {
            "context": "No, monsieur.",
            "statement": "The speaker is answering no to a question."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "According to the context, the speaker did say no.",
                "This is correct.",
                "True, the speaker said no.",
                "monsieur states that the speaker is talking to a man and said no"
            ]
        }
    },
    "72875": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The policy succeeded, and I was fortunate to have had the opportunity to make that contribution to my people.",
            "statement": "Because the policy was a success, I was able to make a contribution to my people."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "contradiction",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both the context and the statement mention the sucessful of the policy and the speaker's contribution to the people.",
                "The statement is a paraphrase of the context.",
                "No, the contribution I made to my people is the success of policy.",
                "paraphrases"
            ]
        }
    },
    "19921": {
        "annotation task": "natural language inference",
        "text": {
            "context": "3) The gap between the productivity of women and the productivity of men.",
            "statement": "The gap of genders."
        },
        "number of annotators": 2,
        "annotators": "Ann3,Ann4",
        "number of annotations": 2,
        "annotations": {
            "Ann3": "entailment",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "True, the gap between the productivity of genders can prove there is a gap between genders.",
                "Statement does not specifiy the gap of WHAT of genders. In the context, it is the gap of productivity"
            ]
        }
    },
    "9557": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Tommy Thompson of Wisconsin and Mayor Rudolph Giuliani of New York, the conservative vanguard on the issue, show no inclination to exploit research that says, in effect, Why care about day-care quality?",
            "statement": "Thompson and Giuliani don't want to care about day cares."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann3,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "contradiction",
            "Ann3": "entailment",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.33299999999999996,
                "1": 0.667
            },
            "entailment": {
                "0": 0.667,
                "1": 0.333
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Thompson and Giuliani don't want to exploit the research that doesn't care about day cares, which means they did care about day cares.",
                "\"No inclination to exploit reserach\" can be understood as to study about day cares.",
                "Thompson and Giuliani did not care about the research, that says not to care about the day-care quality. So they might actually care about day cares"
            ]
        }
    },
    "15537": {
        "annotation task": "natural language inference",
        "text": {
            "context": "So unlike people who are fortunate enough to be able to afford attorneys and can go to another lawyer, our clients are simply lost in the legal system if they cannot get access to it from us.",
            "statement": "Our clients can barely afford our legal assistance."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both the context and statement suggest that their clients are not able to afford attorneys.",
                "It is not clear whether the clients have to pay for the services at all.",
                "The cost of our legal assistance is not given in the context, maybe it is free.",
                "It is not mentioned if our assistance is charged. It could be free."
            ]
        }
    },
    "3476": {
        "annotation task": "natural language inference",
        "text": {
            "context": "apparently apparently the appraisers likes it because our taxes sure is high  isn't it it really is",
            "statement": "We wished the taxes were lower."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention the speaker's wishes about the taxes.",
                "It's not clear whether the speaker wants the taxes to be lower.",
                "Our taxes is high, but our attitude to high taxes is not shown in the context.",
                "There is no info about our wishes"
            ]
        }
    },
    "56124": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Of how, when tea was done, and everyone had stood,He reached for my head, put his hands over it,And gently pulled me to his chest, which smelledOf dung smoke and cinnamon and mutton grease.I could hear his wheezy breathing now, like the prophet's Last whispered word repeated by the faithful.Then he prayed for what no one had time to translate--His son interrupted the old man to tell him a groupOf snake charmers sought his blessing, and a blind thief.The saint pushed me away, took one long look,Then straightened my collar and nodded me toward the door.",
            "statement": "When tea was done, he put his hands on me romantically."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "When tea was done he put his hands over the speaker's head. According to the context, his moves were romantic.",
                "It's not clear whether there was romantic intention.",
                "He was a \"saint\", so \"he put his hands on me\" could be romantically, but also could be nothing to do with romance, whereas about religion.",
                "He is a saint, so it is more a religious practice rather than a romantice move"
            ]
        }
    },
    "5193": {
        "annotation task": "natural language inference",
        "text": {
            "context": "EPA estimates that 5.6 million acres of lakes, estuaries and wetlands and 43,500 miles of streams, rivers and coasts are impaired by mercury emissions.",
            "statement": "The release of mercury has an impact on rivers, streams and lakes"
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both the context and the statement suggest that mercury emissions inpact streams, rivers and lakes.",
                "The mercury impairs the rivers, treams and lakes among others.",
                "True because mercury emissions impaired lakes, estuaries, wetlands, streams, rivers and coasts.",
                "A large land is impaired by mercury emissions, as reported by EPA. So the release of mercury has an impact on these natural bodies"
            ]
        }
    },
    "106390": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Mykonos has had a head start as far as diving is concerned because it was never banned here (after all, there are no ancient sites to protect).",
            "statement": "Protection of ancient sites is the reason for diving bans in other places."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context mentions that driving was never banned in Mykonos because there are no ancient sites to protect, which implies that protection of ancient site might be a reation for driving bans.",
                "The context states that Mykonos did not need to ban diving because there are no ancient sites to proect. This implies that other places banned diving for that reason.",
                "In other places, protection of ancients sites could be one reasonfor diving bans, but there could be other reasons like the danger for divers' life.",
                "Mykonos is not banned for diving, because there is no ancient site to protect. So if there is ancient sites, then it would be a reason for Mykonos to be banned for diving"
            ]
        }
    },
    "54327": {
        "annotation task": "natural language inference",
        "text": {
            "context": "substitute my my yeah my kid'll do uh four or five hours this week for me no problem",
            "statement": "I just can't make the time because of my job."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "contradiction",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The reason the speaker can't make the time is not mentioned in the context.",
                "It's not clear whether the speaker needs a substitute because of their job.",
                "\"No problem\" means I can handle the time.",
                "No clear context"
            ]
        }
    },
    "2870": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Most menu prices include taxes and a service charge, but it's customary to leave a tip if you were served satisfactorily.",
            "statement": "Most customers will tip in addition to the tax on the menus."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "entailment,neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The customers usually tip additionally if the service was good. I don't know if most customers were served satisfactorily.",
                "The statement is a paraphrase of the context.",
                "Leaving a tip is \"customary\", so it should be a behaviour that most people do.",
                "If people are satisfied with the service, they will usually leave a tip, but if they are not satisfied, maybe they will not leave a tip.",
                "The tip is customary, but it is not known how often people tip"
            ]
        }
    },
    "73191": {
        "annotation task": "natural language inference",
        "text": {
            "context": "To get a wonderful view of the whole stretch of river, and to stretch your legs in a beautiful parklike setting, climb up to the Ceteau de Marqueyssac and its jardins suspendus (hanging gardens).",
            "statement": "You will enjoy stretching your legs as you climb the Ceteau de Marqueyssac."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context indicate that people can only stretch their legs after climbing up to the Ceteau de Marqueyssac, not during the climb.",
                "It's not clear whether the people enjoy stretching their legs.",
                "Whether stretching your legs is pleasant or annoying is not discussed in the context.",
                "It is put as a suggestion to climb up the Ceteau de Marqueyssac. During the climbing one should get wonderful view of the river and be able to strech his legs"
            ]
        }
    },
    "62273": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The book is a parody of Bartlett's , serving up quotes from Lincoln, Jefferson, and Roger Rosenblatt with equal pomposity.",
            "statement": "Bill Reilly's book has quotes from various presidents ranging from Lincoln to Jefferson."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "entailment,neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention the name Bill Reilly.",
                "The context mentions only two presidents (Lincoln and Jefferson). This does not qualify as \"various\" for me.",
                "No information of Bill Reilly's book is given in the context.",
                "if assuming it is Bill Reilly's book.",
                "if the book is not known to be Bill's"
            ]
        }
    },
    "58016": {
        "annotation task": "natural language inference",
        "text": {
            "context": "(As the old saying goes, If you can't figure out who the fool is at the poker table, it's probably you.",
            "statement": "Dealers say everyone is smart that is playing."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "contradiction",
            "Ann3": "contradiction,neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention what the dealers say.",
                "The saying implies that there is at least one fool at the poker table.",
                "The context says, if people can not find the fool, then themselves are fools, so at least one guy who is playing is not smart.",
                "Dealers are not mentioned in the context.",
                "not relevant"
            ]
        }
    },
    "45605": {
        "annotation task": "natural language inference",
        "text": {
            "context": "They have prominent red protuberances and may have been named after the British redcoats.",
            "statement": "They were named after the redcoats because they are the same bright red color on their bodies."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context mentions the word \"may\", which suggests that it is just a possibility that they were named after the redcoats.",
                "The context says that they \"may have been named\" not that the definitely were named after the redcoats.",
                "The color of their body is not mentioned in the context.",
                "It is not known for sure that they were named after the British redcoats. In the context, they may have been named after that"
            ]
        }
    },
    "82174": {
        "annotation task": "natural language inference",
        "text": {
            "context": "NEH-supported exhibitions were distinguished by their elaborate wall panels--educational maps, photomurals, stenciled treatises--which competed with the objects themselves for space and attention.",
            "statement": "The exhibitions seem well-funded due to the elaborate detail of the gallery."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The statement makes sense, as it is true that the exhibitions does have elaborate wall panels that can cost a lot of money.",
                "Elaborate wall panels are costly, so most likely the exhibitions are well-funded.",
                "The financial support of the gallery is not mentioned in the context.",
                "Because the wall panels etc. are competing with the objects themselves, which cost money and thoughts"
            ]
        }
    },
    "123703": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Specifically, by defining mission improvement objectives, senior executives determine whether their organization needs a CIO who is a networking/marketing specialist, business change agent, operations specialist, policy/oversight manager, or any combination thereof.",
            "statement": "A CIO must be an operations specialist."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "contradiction",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.0,
                "1": 1.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "According to the context, a CIO could be any combination of the personnel mentioned in the context.",
                "A CIO can also have only a single of the other named specializations. This is indicated by \"or any combination thereof\".",
                "No, a CIO can be a networking/marketing specialist, too.",
                "it can also be networking/marketing specialist or business change agent and so on"
            ]
        }
    },
    "127410": {
        "annotation task": "natural language inference",
        "text": {
            "context": "In this case, shareholders can pay twice for the sins of others.",
            "statement": "shareholders can pay once for the sins of others."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "contradiction",
            "Ann3": "entailment,neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Shareholders can pay twice, that includes once.",
                "The context clearly states \"twice\" not \"once\".",
                "True, because paying once is included by paying twice.",
                "We only know shareholders can pay twice for the sins, but whether can pay once or three times or more is not mentioned in the context.",
                "they can pay twice"
            ]
        }
    },
    "55572": {
        "annotation task": "natural language inference",
        "text": {
            "context": "But they also don't seem to mind when the tranquillity of a Zen temple rock garden is shattered by recorded announcements blaring from loudspeakers parroting the information already contained in the leaflets provided at the ticket office; when heavy-metal pop music loudly emanates from the radio of the middle-aged owner of a corner grocery store; and when parks, gardens, and hallowed temples are ringed by garish souvenir shops whose shelves display both the tastefully understated and the hideously kitsch.",
            "statement": "A Zen temple rock garden is a a place for lots of people to gather and celebrate."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "contradiction",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention what people do in the Zen temple rock garden.",
                "If the tranquility of a Zen garden can be distirbued, this implies that the traniquility is the usual state. This probably precludes large celebrations.",
                "In the context, nothing is about gathering and celebrating.",
                "It is not known that many people come to the garden to celebrate"
            ]
        }
    },
    "93955": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The large scale production of entertainment films is a phenomenon well worth seeing several times.",
            "statement": "The production of entertainment films is elaborate and large scaled."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It is not clear if the production of entertainment films is elabortate.",
                "It is not clear whether it is elaborate.",
                "There is elaborate and large scaled production of entertainment films, but there could be small-scaled production too.",
                "The production is worth seeing several times, so it is elaborate. And the production is large scaled"
            ]
        }
    },
    "72870": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Because marginal costs are very low, a newspaper price for preprints might be as low as 5 or 6 cents per piece.",
            "statement": "Newspaper preprints can cost as much as $5."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "contradiction",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only mentions how low the price may be, not how high it may be.",
                "The context says 5 or 6 cents, not $5.",
                "The maximum cost of newspaper preprints is not given in the context.",
                "5 dollars for a pieace of newspaper"
            ]
        }
    },
    "77025": {
        "annotation task": "natural language inference",
        "text": {
            "context": "You are sure that you did not in any way disclose your identity?\" Tommy shook his head.",
            "statement": "I wish you hadn't revealed your identity, that was a mistake."
        },
        "number of annotators": 3,
        "annotators": "Ann2,Ann3,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Tommy did not reveal his identity.",
                "My preference of the answer of the question, whether Tommy disclosed his identity is not given in the context.",
                "can not infer about Tommy's wish about whether the other person should disclose his identity or not"
            ]
        }
    },
    "69975": {
        "annotation task": "natural language inference",
        "text": {
            "context": "I'm not interested in tactics, Al.",
            "statement": "Al is very interested in tactics."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction,neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only suggests that the speaker's attutude toward tactics, not AI's.",
                "It's not clear whether Al is interested in tactics.",
                "The speaker is not interested in tactics, but Al's interest is not given in the context.",
                "if the sentence in the context is said by AI, then AI is not interested in tactics",
                "irrelevant"
            ]
        }
    },
    "125238": {
        "annotation task": "natural language inference",
        "text": {
            "context": "If the collecting entity transfers the nonexchange revenue to the General Fund or another entity, the amount is accounted for as a custodial activity by the collecting entity.",
            "statement": "Nonexchange revenue to the General Mills."
        },
        "number of annotators": 2,
        "annotators": "Ann2,Ann3",
        "number of annotations": 2,
        "annotations": {
            "Ann2": "neutral",
            "Ann3": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "General Mills are not mentioned at all.",
                "The General Mills is not mentioned in the context."
            ]
        }
    },
    "27022": {
        "annotation task": "natural language inference",
        "text": {
            "context": "For fiscal year 1996, Congress determined that the Commission should recover $126,400,000 in costs, an amount 8.6 percent higher than required in fiscal year 1995.",
            "statement": "Congress determined that Commission should recover over $126 in costs."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann2,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "contradiction,entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "$126,400,000 is indeed more than $126 mentioned in the statement.",
                "Commission should recover $126,400,000 not over $126",
                "$ 126400000 is larger than $126, so technically it's over $126.",
                "$126400000 is more than $126, so it is correct to say over $126.",
                "commission should recover over 126,400,000 in costs"
            ]
        }
    },
    "66689": {
        "annotation task": "natural language inference",
        "text": {
            "context": "OMB issued the guidance in Memorandum M0010, dated April 25, 2000.",
            "statement": "Memorandum M0010 was issued by INS."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "neutral",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.25,
                "1": 0.75
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Memorandum M0010 was issued by OMB, not by INS.",
                "It's not clear who issued the memorandum.",
                "No, Memorandum M0010 was issued by OMB.",
                "It is issued by OMB"
            ]
        }
    },
    "112547": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Credibility is a vital factor, and Jim Lehrer does, indeed, have it.",
            "statement": "Everyone would believe whatever Jim Lehrer said."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Jim Lehrer has great credibility, which means evertone would believe him.",
                "That a person is credible does not mean that everyone will believe them.",
                "Jim Lehrer has credibility, but it cannot be promised that no one would disbelieve him whatever he said.",
                "overexaggeration. Jim Lehrer has credibility. But iit is exaggerated to say everyone would believe him"
            ]
        }
    },
    "76957": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Both initial and supplemental proposed rule publications invited comments on the information collection requirements imposed by the rule.",
            "statement": "There's no point in following politics or voting because your vote won't actually make a difference."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "We don't know whether votes would make a difference or not.",
                "There's no mention of politics or voting in the context.",
                "The context has nothing to do with the effect of following politics and voting.",
                "The comments are invited on the requirements. So one's opinoin might make a difference"
            ]
        }
    },
    "2262": {
        "annotation task": "natural language inference",
        "text": {
            "context": "She buried his remains to spare her mother the gruesome sight.",
            "statement": "The remains would have caused grief to her mother."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann2,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.667,
                "1": 0.333
            },
            "neutral": {
                "0": 0.33299999999999996,
                "1": 0.667
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "There is no mention in the context of how her mother might feel about the remains.",
                "It could also be that the remains would have triggered another strongly negative reaction like disgust (and not grief).",
                "She buried the remains to spare her mother the grief. So if her mother saw the remain, she would grieve"
            ]
        }
    },
    "40710": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Write, write, and write.",
            "statement": "You should keep practicing writing."
        },
        "number of annotators": 3,
        "annotators": "Ann2,Ann3,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann2": "neutral",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.33299999999999996,
                "1": 0.667
            },
            "neutral": {
                "0": 0.667,
                "1": 0.333
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It's not clear whether they should write for the sake of practicing.",
                "True, the context is a imperative sentence and repeated three times \"write\", which could be seen as a order to keep writing.",
                "the repetition of verbs implies the repetition doing that action. So it means keeps writing"
            ]
        }
    },
    "21957": {
        "annotation task": "natural language inference",
        "text": {
            "context": "But those that are manufactured for sale in in Europe and so forth are quite the other way around",
            "statement": "Products are made with differently designed machines in Europe."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann2,Ann3",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "There is no mention in the context of what machines are used to produce products made in Europe.",
                "It is not clear how the products made for Europe differ from the others.",
                "Products could be made with differently designed machines in Europe, or with the same designed machines."
            ]
        }
    },
    "14126": {
        "annotation task": "natural language inference",
        "text": {
            "context": "and so i have really enjoyed that but but there are i do have friends that watch programs like they want to see a particular program and they are either home watching it or definitely recording it they have some programs that they won't miss",
            "statement": "What programs do your friends like to watch?"
        },
        "number of annotators": 1,
        "annotators": "Ann4",
        "number of annotations": 1,
        "annotations": {
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "statement is a question"
            ]
        }
    },
    "79013": {
        "annotation task": "natural language inference",
        "text": {
            "context": "But it just might be because he's afraid he'll lose his No.",
            "statement": "He's definitely afraid of losing he's No."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "neutral",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both the context and the statement mention that he is afraid he'll lose his No.",
                "The context says \"might\" and not \"definitely\".",
                "No, it \"just might be\", but not definitely.",
                "It is a possible that he is afraid of losing he's No. but not definitely"
            ]
        }
    },
    "38156": {
        "annotation task": "natural language inference",
        "text": {
            "context": "BUDGETARY RESOURCES - The forms of authority given to an agency allowing it to incur obligations.",
            "statement": "Administrations generally feel that some agencies should have more budgetary resources than others."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only talks about what budget resources are.",
                "There is no mention of giving different resources to different agencies.",
                "There is no comparison of budgetary resources between agencies.",
                "context is a defination. Statement is the opinion of the administrations."
            ]
        }
    },
    "25304": {
        "annotation task": "natural language inference",
        "text": {
            "context": "well we bought this with credit too  well we found it with a clearance uh down in Memphis i guess and uh",
            "statement": "We bought non-sale items in Memphis on credit."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "contradiction",
            "Ann3": "contradiction",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.25,
                "1": 0.75
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The speaker found the item with a clearance, which suggests that it is on sale.",
                "It was a clearance, so the items were on sale.",
                "No, we bought it with a clearance down.",
                "It is not known if the items are non-sales or not"
            ]
        }
    },
    "53211": {
        "annotation task": "natural language inference",
        "text": {
            "context": "No, I exclaimed, astonished.",
            "statement": "I said no to him several time, utterly surprised by the change of events."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both the context and the statement suggest that the speaker said no and was surprised.",
                "It's not clear whether they said \"no\" several times or only once.",
                "The reason of my astonishment is not given in the context.",
                "it is not known how many times I said no"
            ]
        }
    },
    "123267": {
        "annotation task": "natural language inference",
        "text": {
            "context": "He's a bad lot.",
            "statement": "He's a dishonest person"
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "He may or may not be bad because he is dishonest.",
                "He might be bad in other ways.",
                "Bad people can be both honest and dihonest.",
                "He could be honest but bad in other quality"
            ]
        }
    },
    "80808": {
        "annotation task": "natural language inference",
        "text": {
            "context": "A button on the Chatterbox page will make this easy, so please do join in.",
            "statement": "They wanted to make the site very user friendly."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "neutral",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context did show that they want to make it easy, which is a factor in user-friendliness.",
                "It's not clear from one text of the webpage that they really tried to make it user friendly.",
                "Yes, they want to add a botton which will make the operation easy.",
                "only clicking a button would join the user in. So the user does not need to click a lot buttons. Therefore, it is user-friendly"
            ]
        }
    },
    "72740": {
        "annotation task": "natural language inference",
        "text": {
            "context": "So it wasn't Missenhardt's singing--marvelous though that was--that made Osmin's rantings so thrilling.",
            "statement": "Osmin was always calm and collected."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "contradiction",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.0,
                "1": 1.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context mentions Osmin's rantings, which implies that he was not always calm.",
                "Osmin ranted, so they probably were not always calm and collected.",
                "No, because Osmin rants and quite thrilling.",
                "Osmin is ranting. So he is not calm"
            ]
        }
    },
    "71251": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Deborah Pryce said Ohio Legal Services in Columbus will receive a $200,000 federal grant toward an online legal self-help center.",
            "statement": "A $200,000 federal grant will be received by Ohio Legal Services, said Deborah Pryce, who could finally say it to the public."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention whether Deborah Pryce said it to public or not.",
                "The statement is a paraphrase of the context, with the addition of \"who could finally say it to the public\" which is only a minor aspect.",
                "\"Finally\" means at the beginning, Deborah Pryce is not allowed to announce this information, but the true situation is not given in the context.",
                "No info about whether he could say it to the public or not"
            ]
        }
    },
    "117680": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Since the rules were issued as interim rules and not as general notices of proposed rulemaking, they are not subject to the Unfunded Mandates Reform Act of 1995.",
            "statement": "The rules were  not issued as interim rules but rather general notices of proposed rulemaking."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "contradiction",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.0,
                "1": 1.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context clearly states that the rules were issued as interim rules.",
                "It's clearly stated that the rules were issued as interim rules.",
                "No, the rules were issued as interim rules.",
                "The rules were issued as interim rules"
            ]
        }
    },
    "80930": {
        "annotation task": "natural language inference",
        "text": {
            "context": "so you um-hum so you think it comes down to education or or something like that",
            "statement": "IT all boils down to how much education you have."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann2,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.667,
                "1": 0.333
            },
            "neutral": {
                "0": 0.33299999999999996,
                "1": 0.667
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context talks about \"your\" thought, this thought may or may not be true.",
                "The context is a question, not a statement.",
                "Education is the the reason that explains something"
            ]
        }
    },
    "98844": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The m??tro (subway) is the fastest way to move around the city, but the buses, both in the capital and the other big towns, are best for taking in the sights.",
            "statement": "Taking the subway is a good way to experience big city life."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann2,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "contradiction,entailment",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Subway is the fastest way to move around the city, so it is somehow a good way to experience big city life.",
                "If \"experience big city life\" means sight seeing, then the metro is not a good way to do this.",
                "If \"experience big city life\" means having an authentic experience of people living in the city, then the metro is a good way to do that.",
                "Taking bus is a good way to experience big city life, taking subway could be good or not good.",
                "Taking buses would allow one to take in the sights in the city"
            ]
        }
    },
    "102857": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Expenses included in calculating net cost for education and training programs that are intended to increase or maintain national economic productive capacity shall be reported as investments in human capital as required supplementary stewardship information accompanying the financial statements of the Federal Government and its component units.",
            "statement": "Net cost for college programs can be calculated as a way to increase productivity."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Colledge programs mentioned in the statement are included in education and training programs mentioned in the context, which are intended to increase or maintain productivity.",
                "\"shall be reported as investiments in human capital\" means that they can be included in the calculation as investiments in productivity.",
                "Yes, college programs is kind of education that can increase productivity.",
                "It is those education and training programs that are intended to increase the producitvity. Not the NEt cost"
            ]
        }
    },
    "133243": {
        "annotation task": "natural language inference",
        "text": {
            "context": "He watched the river flow.",
            "statement": "The river roared by."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "contradiction",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context suggests that the river move steadily, while roaring means the opposite.",
                "The river flowed, it didn't roar.",
                "The condition of the river is not described in the context.",
                "It is not clear how the river flows"
            ]
        }
    },
    "144408": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Today it is possible to buy cheap papyrus printed with gaudy Egyptian scenes in almost every souvenir shop in the country, but some of the most authentic are sold at The Pharaonic Village in Cairo where the papyrus is grown, processed, and hand-painted on site.",
            "statement": "The Pharaonic Village in Cairo is the only place where one can buy authentic papyrus."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "contradiction",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.25,
                "1": 0.75
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The Pharaonic Village in Cairo is the place to buy some of the most authentic papyrus, not the only place to buy authentic papyrus.",
                "\"some of the most authentic\" implies that there are other places that also sell authentic papyrus.",
                "People can buy authentic papyrus in the Pharaonic Village in Cairo, but there can be other places also sell it.",
                "You can buy it everywhere in the country"
            ]
        }
    },
    "104412": {
        "annotation task": "natural language inference",
        "text": {
            "context": "After being diagnosed with cancer, Carrey's Kaufman decides to do a show at Carnegie Hall.",
            "statement": "Carrey's Kaufman was diagnosed with cancer before deciding to do a show at Carnegie Hall."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "contradiction",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both the context and the statement describe the same thing about Carrey's Kaufman.",
                "They decided to do the show after the diagnosis, not before.",
                "True, Carrey's Kaufman was diagnosed with cancer first, and then decided to do a show.",
                "Kaufmann decided to do a show at Carnegie Hall after he is diagnosed with cancer"
            ]
        }
    },
    "62238": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Clearly, GAO needs assistance to meet its looming human capital challenges.",
            "statement": "GAO will soon be suffering from a shortage of qualified personnel."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Shortage of qualified personnel may or may not included in the GAO's human capital challenges.",
                "\"human capital challenges\" most likely refers to a lack of qualified personnel.",
                "Human capital challenges can be a shortage of qualifies personnel, but also can be others, like too expensive labor price.",
                "It is not clear if it is a shortage. They might have hired too many personnel"
            ]
        }
    },
    "108027": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The door opened and Severn stepped out.",
            "statement": "They were waiting for someone to open the door for them."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention what \"they\" were doing.",
                "It's not clear whether they waited or whether they opened the door themselves.",
                "Servern stepped out when the door opened, maybe he waited, maybe not.",
                "It is not clear if someone opened the door for Severn or he opened it himself"
            ]
        }
    },
    "67836": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Who are these sons of eggs?",
            "statement": "I wish they were daughters of eggs."
        },
        "number of annotators": 2,
        "annotators": "Ann2,Ann4",
        "number of annotations": 2,
        "annotations": {
            "Ann2": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It's not clear what the speaker wishes.",
                "context is asking about the sons of eggs. Statement is a wish about the daughters of eggs"
            ]
        }
    },
    "21340": {
        "annotation task": "natural language inference",
        "text": {
            "context": "uh somewhat they're not my favorite team i am uh somewhat familiar with them",
            "statement": "They are the best team in the league, by they are not my favorite."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The best team in the league is not mentioned by the context.",
                "It's not clear whether they are the best team in the league.",
                "Maybe they are the best, maybe they are not.",
                "I am only somewhat familiar with the team. So I may not know if they are the best team in the league"
            ]
        }
    },
    "114971": {
        "annotation task": "natural language inference",
        "text": {
            "context": "They won't be killing off George Clooney's character at ER like they did to Jimmy Smits at NYPD . Instead, Dr. Doug Ross is being forced out over the next two episodes because the maverick heartthrob gives an unauthorized painkiller to a terminally ill boy (Thursday, 10 p.m.).",
            "statement": "George Clooney will not be getting fired from his TV show."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "contradiction",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "George Clooney's character will not be killed, which suggests that George Clooney will keep his job.",
                "\"Forced out\" means that he likely still will be fired.",
                "It is about George Clooney's character in the context, not the actor himself.",
                "George Clooney's character at ER will be kept. So he will not be fired"
            ]
        }
    },
    "103354": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The Varanasi Hindu University has an Art Museum with a superb collection of 16th-century Mughal miniatures, considered superior to the national collection in Delhi.",
            "statement": "The Varanasi Hindu University has an art museum on its campus which may be superior objectively to the national collection in Delhi."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "contradiction,neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The  Varanasi Hindu University's meseum is considered superior to the national collection in Delhi, which may not be objective.",
                "The location of the Varanasi Hindu University's museum is not mentioned.",
                "It's not clear what \"objectively superior\" means in this context and whether this really is the acse.",
                "The location of the art museum of the Varanasi Hindu University is not given in the context.",
                "Because the university's museum has a superb collection of 16th-century Mughai miniatures, superior to the national one. Therefore, it could be considered superior in this collection compared to the national collection"
            ]
        }
    },
    "11971": {
        "annotation task": "natural language inference",
        "text": {
            "context": "In a six-year study, scientists fed dogs and other animals irradiated chicken and found no evidence of increased cancer or other toxic effects.",
            "statement": "Scientists gave animals irradiated chicken and they all lived as long as the rest of them."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "They might lived longer than the rest of them.",
                "If there were no toxic effects, they should have lived as long as the others.",
                "No evidence of increased cancer or other toxic effects can make the test animals live as long as others, but the test animals may have other accidents which shorten their lifespan.",
                "no evidence of increased cancer or other toxic effects are found. So they should live no different as other chicken"
            ]
        }
    },
    "83722": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Whether a government postal service can engage in these kinds of negotiations deserves serious study.",
            "statement": "There is serious study needed to check."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both the context and the statement suggest that it needs serious study.",
                "The statement is loosely a paraphrase of the context.",
                "True, there is serious study needed to check whether a government postal service can engage in these kinds of negotiations.",
                "Context states that the postal service deserves serious study. So there is serious study needed to be looked at"
            ]
        }
    },
    "38527": {
        "annotation task": "natural language inference",
        "text": {
            "context": "will never be doused (Brit Hume, Fox News Sunday ; Tony Blankley, Late Edition ; Robert Novak, Capital Gang ; Tucker Carlson, The McLaughlin Group ). The middle way is best expressed by Howard Kurtz (NBC's Meet the Press )--he scolds Brill for undisclosed campaign contributions and for overstretching his legal case against Kenneth Starr but applauds him for casting light on the media.",
            "statement": "They wanted the public to know where the funds came from."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Praising him for casting light on the media shows that they want the public to know the truth.",
                "They scolded for undisclosed campaign contributions, so they want the public to know where the money came from.",
                "The funds are not mentioned in the context.",
                "Because they scolds Brill for undisclosed campain contributions. So they would want Brill to let public know where the fund comes from"
            ]
        }
    },
    "59934": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Likewise, at their production decision reviews, these programs did not capture manufacturing and product reliability knowledge consistent with best practices.",
            "statement": "Their production decision reviews located an anomaly in the data."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "entailment",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context does not mention the anomalies in the data.",
                "An anomaly in the data is not mentioned.",
                "True, because they find these programs did not capturing manufacturing and product reliability knowledge consistent with best practice.",
                "It is the inconsistence in the manufacutring and product reliability knowledge. Not an anomaly"
            ]
        }
    },
    "117089": {
        "annotation task": "natural language inference",
        "text": {
            "context": "appropriate agency representatives, help resolve",
            "statement": "the right agency workers, help fix my security system"
        },
        "number of annotators": 3,
        "annotators": "Ann2,Ann3,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It's not clear what should be resolved.",
                "the right agency workers may help fix the security system, may help resolve other problems",
                "No info about what the agency workers help resolve"
            ]
        }
    },
    "121910": {
        "annotation task": "natural language inference",
        "text": {
            "context": "If ancient writings give only a romanticized view, they do offer a more precise picture of Indo-Aryan society.",
            "statement": "Ancient writings don't show an accurate picture of Indo-Anryan society."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "neutral",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Ancient writings offer a more accurate picture of Ino-Aryan society.",
                "The writings are \"more precise\" than something else. That doesn't imply that they are really accurate.",
                "Yes, because ancient writings give only a romanticized view.",
                "An accurate picture of the society is given under the assumption that ancient writings give a romanticized view. So in reality, where this assumption does not hold, the picture is not accurate either"
            ]
        }
    },
    "113668": {
        "annotation task": "natural language inference",
        "text": {
            "context": "If necessary to meeting the restrictions imposed in the preceding sentence, the Administrator shall reduce, pro rata, the basic Phase II allowance allocations for each unit subject to the requirements of section 414.",
            "statement": "Section 414 helps balance allowance allocations for units."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann2,Ann2,Ann3",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "entailment,neutral",
            "Ann3": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.667,
                "1": 0.333
            },
            "entailment": {
                "0": 0.667,
                "1": 0.333
            },
            "neutral": {
                "0": 0.33299999999999996,
                "1": 0.667
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The requirements of section 414relate to the reduction of the basic Phase II allowance allocations, not the balance of allowance allocations.",
                "If \"subject to the requirements of section 414\" refers to \"reduce\", then Section 414 is involved in helping to balance the allowance.",
                "If \"subject to the requirements of section 414\" refers to \"unit\", then it is not clear whether Section 414 is involved in helping to balance the allowance.",
                "Section 414 require to reduce the allowance pro rata, so it can be balanced, or not balanced."
            ]
        }
    },
    "128176": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The chart to which Reich refers was actually presented during Saxton's opening statement, hours before Reich testified, and did not look as Reich claims it did.",
            "statement": "Reich refers to a chart that he misunderstood."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The fact that Reich was wrong about what he said about the charts he referred to shows that he misunderstood.",
                "It is not clear whether he misrepresented the chart because he misunderstood it. Maybe he did that on purpose.",
                "Reich refers to a chart wrongly, but maybe it is because he misunderstood it, maybe because he remembered incorrectly.",
                "It might be misunderstanding or Reich could also just remember it wrong"
            ]
        }
    },
    "122322": {
        "annotation task": "natural language inference",
        "text": {
            "context": "well uh normally i like to to go out fishing in a boat and uh rather than like bank fishing and just like you try and catch anything that's swimming because i've had such problems of trying to catch any type of fish that uh i just really enjoy doing the boat type fishing",
            "statement": "I fish in the boat and try catching any fish because I have trouble catching certain types."
        },
        "number of annotators": 2,
        "annotators": "Ann2,Ann4",
        "number of annotations": 2,
        "annotations": {
            "Ann2": "entailment",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The statement is a paraphrase of the context.",
                "I have problems of cathing any type of fish, not just certain types"
            ]
        }
    },
    "53619": {
        "annotation task": "natural language inference",
        "text": {
            "context": "True devotees talk shop at even more specialized groups, such as one on Northeastern weather (ne.weather), whose recent conversation topics included the great blizzard of 1978 and the freak snowstorm of May 1977.",
            "statement": "Ne.weather is a general discussion group, not only about weather."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "entailment",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.25,
                "1": 0.75
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "According to the context, ne.weather is a specialized group, not a general discussion group.",
                "ne.weather is a specialized discussion group focussing on weather",
                "No, ne.weather is a specialized group.",
                "It is specialized in weather topics"
            ]
        }
    },
    "27287": {
        "annotation task": "natural language inference",
        "text": {
            "context": "we were talking . Try to behave",
            "statement": "We are having an argument, come at me if you dare!"
        },
        "number of annotators": 3,
        "annotators": "Ann2,Ann3,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "These are different statements.",
                "Talking can be peaceful or a arguement.",
                "context means rather that the other person should behave and be quiet. It is not aggressive like in the statement"
            ]
        }
    },
    "125700": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Don't forget to take a change of clothing and a towel.",
            "statement": "Remember to replace your towel and clothing."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Taking a change of clothing and a towel means taking an extra set of them, which implies the need to replace them with the extra set of clothes and towel.",
                "The statement is a paraphrase of the context.",
                "True, a change of clothing and a towel is for replacement.",
                "paraphrases"
            ]
        }
    },
    "107252": {
        "annotation task": "natural language inference",
        "text": {
            "context": "On the northwestern Alpine frontier, a new state had appeared on the scene, destined to lead the movement to a united Italy.",
            "statement": "The alpine frontier was separated from Italy by glaciers."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only mentions that Italy is on the northweatern Alpine frontier, Glaciers are not mentioned.",
                "There are no glaciers mentioned.",
                "Glaciers are not mentioned in the context.",
                "No info about what separating them"
            ]
        }
    },
    "113967": {
        "annotation task": "natural language inference",
        "text": {
            "context": "'I don't know what happened, exactly.' I said.",
            "statement": "You aren't making sense."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention what \"you\" said.",
                "These are different statements.",
                "I don't know what happened could because your words are nonsense, but also could because of others, like that things are too complicated.",
                "irrelevant"
            ]
        }
    },
    "129464": {
        "annotation task": "natural language inference",
        "text": {
            "context": "It can entail prospective and retrospective designs and it permits synthesis of many individual case studies undertaken at different times and in different sites.",
            "statement": "It can entail prospective and retrospective designs for system redesigns."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention system redesigns.",
                "It's not clear whether the designs are for system redesigns.",
                "It permits synthesis of many individual case studies, maybe also contributes to system redesigns, but maybe not.",
                "No info about what the design is for"
            ]
        }
    },
    "82528": {
        "annotation task": "natural language inference",
        "text": {
            "context": "you know maybe it just wasn't possible at all in the first place you know like the no new taxes thing you know that's uh with the economy going the way it is and everything that was nearly ridiculous thing to",
            "statement": "it's possible to have no new taxes with the way the economy is right now."
        },
        "number of annotators": 2,
        "annotators": "Ann2,Ann3",
        "number of annotations": 2,
        "annotations": {
            "Ann2": "entailment",
            "Ann3": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The speaker says that the \"new taxes thing\" was not possible with \"the economy going the way it is\"",
                "The context is talking about the economy in the past, not right now."
            ]
        }
    },
    "134356": {
        "annotation task": "natural language inference",
        "text": {
            "context": "You will remember my saying that it was wise to beware of people who were not telling you the truth.\"",
            "statement": "There might be dishonest people around here."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context mentions to beware of dishonest people, which implies that there might be dishonest people here.",
                "The speaker warns someone of dishonest people, so likely there are some around.",
                "Dishonest people maybe are around here, maybe are somewhere else.",
                "one should be aware of people who aren't telling the truth. So there are dishonest people around"
            ]
        }
    },
    "102075": {
        "annotation task": "natural language inference",
        "text": {
            "context": "um-hum with the ice yeah",
            "statement": "With the sunshine and heat wave yes."
        },
        "number of annotators": 2,
        "annotators": "Ann2,Ann4",
        "number of annotations": 2,
        "annotations": {
            "Ann2": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "These are different statements.",
                "irrelevant"
            ]
        }
    },
    "96956": {
        "annotation task": "natural language inference",
        "text": {
            "context": "You wonder whether he could win a general election coming out of the right lane of the Democratic Party.",
            "statement": "He will not run in a general election while he is a conservative Democrat."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "neutral",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.25,
                "1": 0.75
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "He is runing in a general election since \"you\" already wonder whether he could win it.",
                "Someone only asks themselves if he could win the general election. That does not say anything about its truth.",
                "No, he run in a general election while he is under the right lane of the Democratic Party.",
                "It is wondered if he could win a general election. So it is possible that he will run in a general election"
            ]
        }
    },
    "24163": {
        "annotation task": "natural language inference",
        "text": {
            "context": "We have done that spectacularly.",
            "statement": "Spectacular results was the only way to describe the impact of our past work."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention otherr describtion than spectacular, so we don't if it is the only way.",
                "It is not clear what they have done spectacularly.",
                "Spectacular results was the one way to describe the impact of our past work, but there could be other ways, like historical.",
                "Our work has been spectacular. So the result of the work must be spectacular"
            ]
        }
    },
    "46576": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Perhaps a further password would be required, or, at any rate, some proof of identity.",
            "statement": "Identity should be a minimum requirement."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Identity is at any rate required, so it is a minimum requirement.",
                "The statement is a paraphrase of the context.",
                "True, at any rate the proof of identity is needed.",
                "Whatever is needed, some proof of identity is needed. So it makes the identity a minimum requirement"
            ]
        }
    },
    "83247": {
        "annotation task": "natural language inference",
        "text": {
            "context": "It's come back? cried Julius excitedly.",
            "statement": "They were excited to hear it will come back."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only mentions Julius, it is not clear who \"they\" are.",
                "It's not clear whether \"they\" or only Julius was excited.",
                "True, Julius cried excitedly when heard it back.",
                "Julius cried excitedly. So he is excited about it coming back"
            ]
        }
    },
    "80109": {
        "annotation task": "natural language inference",
        "text": {
            "context": "if it had rained any more in the last two weeks instead of planting Saint Augustine grass in the front yard i think i would have plowed everything under and had a rice field",
            "statement": "It has rained enough to flood everything here and make rice pattys."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "neutral",
            "Ann3": "contradiction",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context indicates that it hasn't reained enough.",
                "It's not clear what \"make rice patty\"s means, but it shouldn't be entailed by having a rice field.",
                "No, it has not rained enough in the last two weeks.",
                "it hasn't rained enough. If it had rained enough, I would have had a rice field"
            ]
        }
    },
    "52854": {
        "annotation task": "natural language inference",
        "text": {
            "context": "This was used for ceremonial purposes, allowing statues of the gods to be carried to the river for journeys to the west bank, or to the Luxor sanctuary.",
            "statement": "Statues were moved to Luxor for funerals and other ceremonies."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Statues were moved to Luxor or to the west bank.",
                "It's not clear whether the statues were moved for funerals or only for other ceremonies.",
                "Maybe statues were moved to Luxor, or to the west bank.",
                "ceremonial purposes were mentioned. Funeral is also a kind of ceremonial purpose. So the statues could also be used for that"
            ]
        }
    },
    "133274": {
        "annotation task": "natural language inference",
        "text": {
            "context": "(Imagine the difference between smoking a cigarette and injecting pure nicotine directly into a vein.)",
            "statement": "Smoking a cigarette is a lot like injecting pure nicotine."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "contradiction,neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't suggest the difference between smoking a cigarette and injecting pure nicotine.",
                "It's not clear whether the difference is large or small according to the context.",
                "No, the context emphasizes the difference between smoking a cigarette and injecting pure nocotine.",
                "Maybe there are lots of similarities between smoking a cigarette and injecting pure nocotine, but maybe they are very different.",
                "There should be a difference"
            ]
        }
    },
    "120070": {
        "annotation task": "natural language inference",
        "text": {
            "context": "well do you know you have a ten limit a ten minute time limit well that's okay and then they come on and tell you and they tell you got five seconds to say good-bye",
            "statement": "You get a ten minute time limit, but sometimes you'll be told to end early."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "entailment",
            "Ann3": "contradiction",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context suggests clearly that \"you\" have a ten minute time limit. The requirement for an early end is not valid unless explicitly stated in the context.",
                "The statement is a paraphrase of the context.",
                "No, you will not be told to end early, but will be asked to end it quickly.",
                "It is not clear if then come early or on time to tell one to end"
            ]
        }
    },
    "129185": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Lincoln glared.",
            "statement": "The man was angry."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Lincoln glared might out of anger or surprise.",
                "If he glared, then he probably was angry.",
                "The man glared maybe because of anger, maybe because of others like terror.",
                "Glaring is a state out of anger"
            ]
        }
    },
    "49172": {
        "annotation task": "natural language inference",
        "text": {
            "context": "These alone could have valuable uses.",
            "statement": "They may be valuable."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "According to the context, they could be valuable.",
                "The statement is a paraphrase of the context.",
                "True, \"could have valuable uses\" implies the probability of being valuable.",
                "Valuable uses infer being valuable"
            ]
        }
    },
    "22938": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Despite a recent renovation, the Meadows Mall is the least appealing of the three suburban malls.",
            "statement": "The Meadows Mall is not appealing."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "entailment,neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The Meadows Mall is the least appealing of the three malls, which doesn't mean it is not appealing at all, it is just not more appealing than other two.",
                "It is only less appealing than the other two malls. It's not clear whether they all are appealing.",
                "True, the Meadows Mall is the least appealing of the three suburban malls.",
                "The Meadows Mall is the least appealing of the three suburban malls, but maybe compared to other competitors, it is still appealing.",
                "THe Meadows Mall is the least appealing out of three malls. But it could be appealing, just not as appealling as the other two"
            ]
        }
    },
    "67571": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Everybody has this quote from NBA commissioner David  You cannot strike your boss and still hold your job--unless you play in the NBA.",
            "statement": "NBA commissioner said he hates NBA players."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention whether NBA commissioner said he hates NBA players.",
                "The context does not seem to be about whether the commissioner likes NBA players.",
                "NBA commissioner David said NBA players could strike their boss and still hold their job, as \"the boss\", he maybe hates NBA players, maybe not.",
                "His quote ony shows that the boss does not have total power over his players in NBA. It does not convey his personal feelings over the players"
            ]
        }
    },
    "5087": {
        "annotation task": "natural language inference",
        "text": {
            "context": "approaches to achieving missions vary considerably between agencies.",
            "statement": "Approaches to achieving missions might change a lot."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both the context and the statement show that there are different approaches to achieving missions.",
                "If they vary between agencies, they might change a lot, e.g. if you move from one agencie to another.",
                "True, \"vary considerably\" implies \"change a lot.\"",
                "A considerable change could be big"
            ]
        }
    },
    "23769": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Kom Ombo is an unusual temple in that it is dedicated to two gods.",
            "statement": "Rarely visited, Kom Ombo is a strange temple devoted to two gods."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It is not clear if Kom Ombo is rarely visited.",
                "It's not clear whether Kom Ombo is rarely visited.",
                "We don't know how often the Kom Ombo temple is visited.",
                "No info about how many people visiting the Kom Ombo"
            ]
        }
    },
    "13760": {
        "annotation task": "natural language inference",
        "text": {
            "context": "If they have overestimated how far the CPI is off, Boskin and his commission may institutionalize an underestimated CPI--guaranteeing a yearly, stealth tax increase.",
            "statement": "If they've overestimated how far the CPI is off, it will have horrific consequences."
        },
        "number of annotators": 2,
        "annotators": "Ann2,Ann4",
        "number of annotations": 2,
        "annotations": {
            "Ann2": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The tax increase will come if CPI is underestimated and it's not clear whether this is horrific.",
                "Not sure if a yearly, stealth tax increase counts as a horrific consequence"
            ]
        }
    },
    "2133": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The tomb guardian will unlock the gate to the tunnel and give you a candle to explore the small circular catacomb, but for what little you can see, it is hardly worth the effort.",
            "statement": "The tomb garden can give you a thorough tour of the catacombs."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The tomb guardian will open the gate for you and give you a candle, which implies that he will not give you a tour of the catacombs.",
                "The context speaks about the \"guardian\" not the \"garden\".",
                "A thorough tour is not mentioned in the context.",
                "No, he only gives a candle to explore the catacomb. And you can only see a little, which is not worth the effort"
            ]
        }
    },
    "117093": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Hong Kong has long been China's handiest window on the West, and the city is unrivaled in its commercial know-how and managerial expertise.",
            "statement": "Hong Kong is a great place to find commercial know-how if you are hiring someone new."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context mentions that Hong Kong is a great place in its commercial know-how.",
                "The statement is a paraphrase of the context.",
                "Hiring someone new is not mentioned in the context.",
                "the city is experience in commercial know-how. So it is a good idea to find people in that area in Hong Kong"
            ]
        }
    },
    "112402": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Although the accounting and reporting model needs to be updated, in my view, the current attest and assurance model is also out of date.",
            "statement": "The accounting model needs to be updated in addition to the acquisition model."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention the acquisition model.",
                "It is not clear whether the acquisition model has to be updated.",
                "The acquisition model is not mentioned in the context.",
                "It is the attest and assurance model needs to be updated"
            ]
        }
    },
    "25437": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Well, we will come in and interview the brave Dorcas.\" Dorcas was standing in the boudoir, her hands folded in front of her, and her grey hair rose in stiff waves under her white cap.",
            "statement": "Dorcas is well known for her bravery."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "entailment",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It is not clear what is Dorcas well known for.",
                "It is not clear whether Dorcas was known for her bravery or whether only the speaker thought she was brave.",
                "True, \"brave Dorcas\" reveals her bravery.",
                "No info about whether Dorcas is well known for her bravery or not"
            ]
        }
    },
    "56895": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The entire economy received a massive jump-start with the outbreak of the Korean War, with Japan ironically becoming the chief local supplier for an army it had battled so furiously just a few years earlier.",
            "statement": "Korea and Japan were not at war."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann3,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "contradiction",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.33299999999999996,
                "1": 0.667
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.667,
                "1": 0.333
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Japan had battled with Korea furiously.",
                "Japan was the chief local supplier for an army in the Korean war, but we don't know whether Japanese army also involved in the war.",
                "They were a war between the two countries. And Japan even was the chief local supplier for Korean after the war"
            ]
        }
    },
    "43094": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Time 's cover package considers what makes a good school.",
            "statement": "Time's cover package is about how most college students have to deal with insane student loans."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "contradiction",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The topic of the Time's cover package mentioned in the statement is completely different from the one mentioned in the context.",
                "The cover package is about good schools, not about student loans.",
                "Insane student loans are not discussed in the context.",
                "irrelevant"
            ]
        }
    },
    "21834": {
        "annotation task": "natural language inference",
        "text": {
            "context": "But the world is not run for the edification of tourists.",
            "statement": "The world does not try and morally subject to tourists."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann3,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "entailment",
            "Ann3": "entailment",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.33299999999999996,
                "1": 0.667
            },
            "neutral": {
                "0": 0.667,
                "1": 0.333
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Runing for the edification of tourists includes morally subjecting to tourists.",
                "True, the world exists not for the tourists.",
                "irrelevant"
            ]
        }
    },
    "139836": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The centralization dear to Richelieu and Louis XIV was becoming a reality.",
            "statement": "Louis XIV cared a lot about centralization of his country and people."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both the context and the statement mention that Louis XIV valued the centralization.",
                "The statement is a paraphrase of the context.",
                "True, the centraliza dear to Louis XIV implies he cares a lot about centralization.",
                "It was not mentioned whether Louis XIV cared about his people or not"
            ]
        }
    },
    "34573": {
        "annotation task": "natural language inference",
        "text": {
            "context": "But the door was locked?\" These exclamations burst from us disjointedly.",
            "statement": "We chaotically exclaimed as we all jumped up in a frenzy, \"But the door wasn't unlocked?\""
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann2,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.667,
                "1": 0.333
            },
            "entailment": {
                "0": 0.667,
                "1": 0.333
            },
            "neutral": {
                "0": 0.667,
                "1": 0.333
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn' mention that they jumped up.",
                "\"Was not unlocked?\" entails \"Was locked?\"",
                "The door was locked"
            ]
        }
    },
    "52278": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Tuppence rose.",
            "statement": "Tuppence stood up."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both the context and the statement suggest that Tuppence stood up.",
                "\"Rose\" entails \"stood up\".",
                "A human \"rose\" means \"stood up\".",
                "the word\"rise\" means gets up/ stands up"
            ]
        }
    },
    "54383": {
        "annotation task": "natural language inference",
        "text": {
            "context": "He knew how the Simulacra was supposed to develop.",
            "statement": "He didn't know about Sims."
        },
        "number of annotators": 3,
        "annotators": "Ann2,Ann3,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.667,
                "1": 0.333
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.33299999999999996,
                "1": 0.667
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Sims are not mentioned in the context.",
                "Sims is not mentioned in the context.",
                "He knew about Sims/Simulacra and how they were supposed to develop"
            ]
        }
    },
    "123751": {
        "annotation task": "natural language inference",
        "text": {
            "context": "i think that the people that are um have um a lower income which you automatically equate with lower education",
            "statement": "I think because you have lower income you are less educated."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "neutral",
            "Ann3": "entailment",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context and the statement discuss just the opposite of the causal relationship between income and educational level.",
                "It's not clear whether the speaker talks about other people associating lower income with lower education or whether they talk about themselves.",
                "True, I think that lower income is equal to lower education.",
                "There is no clear causal relation between the poorness and the education level. Either one could lead to the other"
            ]
        }
    },
    "43891": {
        "annotation task": "natural language inference",
        "text": {
            "context": "GAO recommends that the Secretary of Defense revise policy and guidance",
            "statement": "GAO recommends that you eat 5 fruit/veg per day"
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The recommendation of GAO in the statement is not mentioned in the context.",
                "Fruit/veg are not mentioned in the context.",
                "Diet is not mentioned in the context.",
                "irrelevant"
            ]
        }
    },
    "113039": {
        "annotation task": "natural language inference",
        "text": {
            "context": "In this respect, bringing Steve Jobs back to save Apple is like bringing Gen.",
            "statement": "Steve Jobs came back to Apple."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't suggest if Steve Jobs came back to Apple.",
                "The speaker talks only about what it would be like if Steve Jobs returned to Apple. He doesn't assert that he really came back.",
                "True, because it describe Steve Jobs' back as \"bringing Gen\", so he came back to Apple and saved it like a General.",
                "Steve Jobs was brought back to save Apple. So he came back to Apple"
            ]
        }
    },
    "83900": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Ca'daan closed the door behind them and retied the not.",
            "statement": "Ca'daan closed the door as they entered, and bound it shut with rope."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "contradiction",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention if Ca'daan bound the door shut with rope",
                "The \"[k]not\" was probably made with rope.",
                "No, Ca'daan closed the door after they entered, not as they entered.",
                "paraphrases"
            ]
        }
    },
    "10119": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Then he is very sure.",
            "statement": "He is very sure of himself."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention what he's very sure of.",
                "It's unclear whether he is sure of himself or of something else.",
                "He could be very sure of himself, or be sure of any other things.",
                "paraphrases"
            ]
        }
    },
    "21810": {
        "annotation task": "natural language inference",
        "text": {
            "context": "you can get a hard copy of it and that's about it",
            "statement": "An email won't cut it."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only mentions the hard copy.",
                "E-mail is not mentioned in the context.",
                "Email is not mentioned in the context.",
                "Only one hard copy of it is allowed. So an E-mail does not meet the requirement and won't do the trick"
            ]
        }
    },
    "35700": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The Honorable Bill Archer, Chairman The Honorable Charles B. Rangel Ranking Minority Member Committee on Ways and Means House of Representatives",
            "statement": "Bill Archer has never held government office in his entire life."
        },
        "number of annotators": 2,
        "annotators": "Ann2,Ann3",
        "number of annotations": 2,
        "annotations": {
            "Ann2": "contradiction",
            "Ann3": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.0,
                "1": 1.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Bill Archer was chairman of the house of representatives.",
                "No, Bill Archer at lease held government office as Chairman."
            ]
        }
    },
    "13387": {
        "annotation task": "natural language inference",
        "text": {
            "context": "yeah i can believe that",
            "statement": "I agree with what you said."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both the context and the statement suggest that the speaker believe what was said.",
                "Belief does not entail agreement.",
                "I believe something, maybe it is what you said, maybe it is anything else.",
                "To Believe what has happened does not mean to agree with what has happened"
            ]
        }
    },
    "58557": {
        "annotation task": "natural language inference",
        "text": {
            "context": "In the first instance, IRS would have no record of time before the person could get through to an agent and of discouraged callers.",
            "statement": "There is no recording of the time for callers."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "There is recording of the time after the person get through to an agent.",
                "The \"would\" implies that this is about a hypothetical situation, not about a factual one.",
                "There is no recording of the time for discouraged callers, but other callers could be recorded, or not.",
                "There will be a record after the person get through to an agent"
            ]
        }
    },
    "36811": {
        "annotation task": "natural language inference",
        "text": {
            "context": "This having come to his stepmother's ears, she taxed him with it on the afternoon before her death, and a quarrel ensued, part of which was overheard.",
            "statement": "A love affair sparked just moments before her death."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It is not clear if the context talks about A love affair.",
                "There's no mention of a love affair.",
                "A love affair is not mentioned in the context.",
                "It's not known that it's about a love affair"
            ]
        }
    },
    "73840": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Hersheimmer \"WELL,\" said Tuppence, recovering herself, \"it really seems as though it were meant to be.\" Carter nodded.",
            "statement": "See, luck is real!"
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann2,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.667,
                "1": 0.333
            },
            "neutral": {
                "0": 0.33299999999999996,
                "1": 0.667
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The speaker seems to believe in luck, as she said that it seems as it were meant to be.",
                "Luck is not mentioned.",
                "Irrelevant"
            ]
        }
    },
    "31249": {
        "annotation task": "natural language inference",
        "text": {
            "context": "(And yes, he has said a few things that can, with some effort, be construed as support for supply-side economics.)",
            "statement": "It would take some work to construe the things as support for supply-side economics."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "\"With some effort\" has the same meaning as \"take some work\".",
                "\"with some effort\" implies \"it would take some work\".",
                "True, to construe the things as support for supply-side economics need some effort.",
                "What he said would need some effort, meaning work, to be construed as support"
            ]
        }
    },
    "91913": {
        "annotation task": "natural language inference",
        "text": {
            "context": "This is one of the reasons we're growing too weak to fight the Satheri.  \"What's wrong with a ceremony of worship, if you must worship your eggshell?\" Dave asked.",
            "statement": "Eggshell worship is the reason we're growing too weak to fight the Satheri, yet Dave asked about it."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't specify the reasons why they are growing too weak to fight the Satheri.",
                "It's not clear whether \"Eggshell worship\" is really put as a reason.",
                "The reasons that we're growing too weak to fight the Satheri is not given in the context.",
                "Dave meant if\"you\" can worship a eggshell then people can also have a ceremony of worship"
            ]
        }
    },
    "100895": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Is there adequate information for judging generalizability?",
            "statement": "Every output has some kind of resource."
        },
        "number of annotators": 2,
        "annotators": "Ann2,Ann4",
        "number of annotations": 2,
        "annotations": {
            "Ann2": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Output or resources are not mentioned in the context.",
                "Irrelevant"
            ]
        }
    },
    "837": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The central features of the Results Act-strategic planning, performance measurement, and public reporting and accountability-can serve as powerful tools to help change the basic culture of government.",
            "statement": "The Results Act has strategic planning as a central feature for public organizations."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "entailment",
            "Ann3": "contradiction",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The central features of the Results Act are tools to help government, not public organizations.",
                "\"strategic planning\" is mentioned as one of the central features.",
                "The Results Act has strategic planning as a central feature, but the purpose is to help change the basic culture of government.",
                "Strategic planning is one of its central features"
            ]
        }
    },
    "141293": {
        "annotation task": "natural language inference",
        "text": {
            "context": "oh wow no i just started about well five years ago i think",
            "statement": "It had started five years ago."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "neutral",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both the context and the statement suggest that the speaker started five years ago.",
                "\"I started\" not \"it started\"",
                "True, I think I started five years ago.",
                "I started something five years ago, so this thing or it was started five years ago"
            ]
        }
    },
    "30139": {
        "annotation task": "natural language inference",
        "text": {
            "context": "oh that's not really important the the other stuff is just you know window dressing because we we've never ordered anything fact the the van that we've got we bought uh from an estate it was an estate trade uh it was almost brand new the the gentlemen who owned it had died",
            "statement": "We were very lucky to get the van given how new it was."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It is true that it's a lucky thing to get a new van.",
                "Whether they were lucky would depend on the price which is not mentioned.",
                "The van was almost brand new, but it could be lucky to have or not at all.",
                "The van was almost brand new because the gentleman who owned it died. So it's almost not used"
            ]
        }
    },
    "30380": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The NYT , in its front-page coverage, says the plane was flying far lower than the rules for training missions allow.",
            "statement": "The NYT reported that training missions did allow for planes to fly that low."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "contradiction",
            "Ann3": "contradiction",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.25,
                "1": 0.75
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It was flying lower than the rules allow, which suggests that it was not allowed by rules for training missions.",
                "The \"plane was flying far lower than the rules [...] allow\" implies that it was allowed to fly this low.",
                "No, NYT reported that training missions did not allow for planes to fly that low.",
                "The plane flew lower than the rules allowed, so the rules do not allow to fly that low"
            ]
        }
    },
    "22436": {
        "annotation task": "natural language inference",
        "text": {
            "context": "1 Now that each unit is fully staffed, the LSC Office of Program Performance and its state planning team contain over 260 years of experience in LSC-funded programs.",
            "statement": "The LSC has over 260 years of experience with their lawyers."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The experience of LSC with their lawyers is not mentioned.",
                "The experience is for the \"state planning team\" and not for \"lawyers\".",
                "The LSC has over over 260 years of experience in LSC-funded programs, but it could be with their lawyers, or with other staffs, like interns.",
                "No info about the info but about LSC-funded program, which we do not know info about"
            ]
        }
    },
    "76037": {
        "annotation task": "natural language inference",
        "text": {
            "context": "You did, didn't you?\"",
            "statement": "You didn't mean to do that, did you?"
        },
        "number of annotators": 3,
        "annotators": "Ann2,Ann3,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.667,
                "1": 0.333
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.33299999999999996,
                "1": 0.667
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "\"mean to do that\" is not mentioned in the context.",
                "The deliberation is not given in the context.",
                "Context is about whether an action has been done by someone, but the statement is about the intention to do it"
            ]
        }
    },
    "28456": {
        "annotation task": "natural language inference",
        "text": {
            "context": "A clean, wholesome-looking woman opened it.",
            "statement": "The woman was trying to be desecrate."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "contradiction,neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention anything about desecration.",
                "It's not clear what the woman was trying to be.",
                "No, the woman is clean and wholesome-looking, not desecrate.",
                "The attempt of the woman is not given in the context.",
                "Context is a compliment, statement is a negative comment"
            ]
        }
    },
    "46198": {
        "annotation task": "natural language inference",
        "text": {
            "context": "How effectively DOD manages these funds will determine whether it receives a good return on its investment.",
            "statement": "The DOD is certain to have a bad return on these funds."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "contradiction",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The return is not certainly bad. The return is determined by how DOD manages these funds.",
                "\"How effectively [...] will determine\" implies that there is at least a chance to have a good return.",
                "The return on its investment can be bad or good.",
                "It is not known yet about the result, it depends on DOD's management"
            ]
        }
    },
    "100136": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Challenges to Restore Public Confidence in",
            "statement": "Public confidence can be difficult to reestablish."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann2,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The statement suggest that it is possible that Public confidence is difficult to restore. This is true since the context only mentions that there are challenges, not how large the challenges are.",
                "If there are \"challenges to restore public confidence\" then it can be \"difficult to reestablish\".",
                "Challenge means to be a difficulty"
            ]
        }
    },
    "96516": {
        "annotation task": "natural language inference",
        "text": {
            "context": "As Ben Yagoda writes in the New York Times Book Review , somewhere along the way, Kidder must have decided not to write a book about Tommy O'Connor.",
            "statement": "A book was not written about Tommy O'Connor."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "entailment,neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Kidder didn't write a book about Tommy O'Connor doesn't mean others haven't.",
                "Other people might have written a book about Tommy O'Connor.",
                "True, Kidder decided not to write a book about Tommy O'Connor.",
                "Maybe Kidder later wrote a book not about Tommy O'Connor, maybe he even did not write a book, so no book was written at all.",
                "Kidder dicided not to write a book about O'Conner. So this book about him is not written"
            ]
        }
    },
    "52761": {
        "annotation task": "natural language inference",
        "text": {
            "context": "My unborn children will never appear on the Today show.",
            "statement": "No direct descendent of mine will ever be a guest of the Today show."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The grandchild is also a direct descendent, we don't know if the speaker's grandchildren will appear on the Today show.",
                "There might be children that have already been born. These would not be \"unborn children\" but also \"direct descendent\".",
                "Direct descendent include children and grandchildren. In the context, it is only confirmed that my children will not be a guest of the Today show, but my grandchildren could be on it.",
                "my unborn children mean the children that are born by me. So they are direct descendent of mine. So my unborn children not being on the show means my direct descendent not being on the show"
            ]
        }
    },
    "21297": {
        "annotation task": "natural language inference",
        "text": {
            "context": "He was crying like his mother had just walloped him.",
            "statement": "He was crying like his mother hit him with a spoon."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both the context and the statement describe how much he was crying.",
                "Regarding the intensity of crying \"wallop\" is probably similar to hitting \"with a spoon\".",
                "His mother could hit him with a spoon, could with other things like stike or slippers.",
                "It is not known with what his mother hit him"
            ]
        }
    },
    "98487": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Julius nodded gravely.",
            "statement": "Julius loves to ask questions."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It is not clear from the context if Julius loves to ask questions.",
                "Questions are not mentioned.",
                "Maybe Julius loves to ask questions, maybe not.",
                "irrelevant"
            ]
        }
    },
    "17179": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Lie back, and DON'T THINK.",
            "statement": "Lie back, and do not use your crazy mind."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context and the statement talk about stopping thinking.",
                "It's not clear whether the mind is crazy.",
                "Using crazy mind could be counted as thinking or not thinking, but dreaming.",
                "\"DON'T THINK\" implies not to overthink and relax in this context. So it is similar to \" not use your crazy mind\", which also means not to overthink"
            ]
        }
    },
    "132539": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Boca da Corrida Encumeada (moderate; 5 hours): views of Curral das Freiras and the valley of Ribeiro do Poco.",
            "statement": "Boca da Corrida Encumeada is a moderate text that takes 5 hours to complete."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "contradiction",
            "Ann3": "contradiction",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "\"Moderate\" and \"5 hours: are two descriptions about Boca da Corrida Encumeada made in the statement which are mentioned in the context as well.",
                "\"Boca da Corrida Encumeada\" sounds more like a hike than a text.",
                "No, Boca da Corrida Encumeada should be a route, not a text.",
                "It could be assumed that (moderate; 5 hours) is a short form of moderate text with 5 hours reading time"
            ]
        }
    },
    "23642": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The second half of the book dealt with the use of the true name.",
            "statement": "The first part dealt with the use of false names."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The first part is not mentioned by the context.",
                "It's not clear what the first part of the book is about.",
                "The first part is not given in the context.",
                "No info about the first part of the book"
            ]
        }
    },
    "84781": {
        "annotation task": "natural language inference",
        "text": {
            "context": "By coordinating policy development and awareness activities in this manner, she helps ensure that new risks and policies are communicated promptly and that employees are periodically reminded of existing policies through means such as monthly bulletins, an intranet web site, and presentations to new employees.",
            "statement": "She can find new risks with the awareness campaign."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "She can communicate new risks with the awareness campaign insteading of finding new risks.",
                "The awareness campaign is about communicating risks, not about finding new ones",
                "Finding new risks could be a effect of awareness campaign, but it is not given in the context.",
                "She does not find new risks but to ensure that the new risks are dealt with correctly"
            ]
        }
    },
    "125021": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Other functional components of the Postal Service are presumed here not to exhibit significant scale economies, although this has not been demonstrated.",
            "statement": "The Postal Service only operates very large scale economies."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The word \"only\" is not mentioned in the context.",
                "It is not clear whether the Postal Service operates economies at all, or what that should mean.",
                "Other functional components of the Postal Service could operate not very large scale economies, but this has not been demonstrated, so it can also be very large.",
                "They are presumed not to operate significant/large scale economies"
            ]
        }
    },
    "133005": {
        "annotation task": "natural language inference",
        "text": {
            "context": "In May 1967, Gallup found that the number of people who said they intensely disliked RFK--who was also probably more intensely liked than any other practicing politician--was twice as high as the number who intensely disliked Johnson, the architect of the increasingly unpopular war in Vietnam.",
            "statement": "Due to his attitudes on cheesecake, RFK was more disliked than Johnson."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "contradiction,neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The reason why RFK was more disliked than Johnson is not mentioned in the context.",
                "It is not clear whether RFK had an attitude towards cheescake or how that impacted his popularity.",
                "No, RFK was more iked than Johnson.",
                "RFK's attitudes on cheesecake is not given in the context.",
                "It is not known that the dislikes on RFK is due to his attitudes on cheesecakes"
            ]
        }
    },
    "65650": {
        "annotation task": "natural language inference",
        "text": {
            "context": "She didn't listen.",
            "statement": "She did not listen to the noise."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't suggest what she didn't listen to.",
                "It's not clear whether she didn't listen to the noise or to something else.",
                "Maybe she did not listen to the noise, maybe she did not listen to vert important messages.",
                "It is unknown what she did not listen to"
            ]
        }
    },
    "3545": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Several of the organizations had professional and administrative staffs that provided analytical capabilities and facilitated their members' participation in the organization's activities.",
            "statement": "Organizations had mandatory bonding exercises for their members."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It is not clear if the activities are mandatory.",
                "\"mandatory boding exercises\" are not mentioned.",
                "Organizations facilitated their members' participation in the organization's activities, but they maybe had mandatory bonding exercises, maybe not.",
                "The members were facilitated to participate. So it is not mandatory but encouraged"
            ]
        }
    },
    "105196": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Indeed, said San'doro.",
            "statement": "They were certain."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only mentions San'doro, it is unclear who \"they\" are.",
                "\"Indeed\" implies a level of certainty.",
                "In the context, there is only one person. \"They\" can refer to anyone.",
                "Indeed implies a positive acknowledging attitude."
            ]
        }
    },
    "114458": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Mortifyingly enough, it is all  the difficulty, the laziness, the pathetic formlessness in youth, the round peg in the square hole, the whatever do you want?",
            "statement": "Many youth are lazy."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann2,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.33299999999999996,
                "1": 0.667
            },
            "neutral": {
                "0": 0.667,
                "1": 0.333
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context does not mention whether many young people are lazy.",
                "\"the laziness [...] in youth\" implies that \"many youth are lazy\".",
                "The laziness in youth means youth being lazy"
            ]
        }
    },
    "137319": {
        "annotation task": "natural language inference",
        "text": {
            "context": "And she came to you?",
            "statement": "The person asked if the woman came to him."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The statement is the description of the question in the context.",
                "The statement is a paraphrase of the context.",
                "\"You\" can refer to a male or a female.",
                "she' implies it was a woman."
            ]
        }
    },
    "63013": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Although claims data provide the most accurate information about health care use, ensuring adequate follow-up for purposes of obtaining information from patient self-report is important because many people do not report alcohol-related events to insurance compa-nies.",
            "statement": "The insurance companies want to reduce medical payments by following-up to ensure patient was sober at the time of incident and intoxication may lead to a claim denial on reimbursement for medical expenses."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann2,Ann3",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The statement seems to be a reason of why the insurance companies want to follow-up, that may just be one possibility.",
                "It is not clear whether it is the insurance companies that should do the follow-up or why.",
                "Intoxication may lead to a claim denial on reimbursement for medical expense, but it may not."
            ]
        }
    },
    "79507": {
        "annotation task": "natural language inference",
        "text": {
            "context": "An organization's activities, core processes, and resources must be aligned to support its mission and help it achieve its goals.",
            "statement": "An organization is successful if its activities, resources, and goals align."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "What is mentioned in the statement may be a factor in an organization's success, but there might be others.",
                "The context only says that these are required, not that they are sufficient.",
                "An organization's activities, rescources and goals align can help it achieve success, but the alignment can not promise the success.",
                "Don't know if this is the only standard to measure if an organization is successful"
            ]
        }
    },
    "120323": {
        "annotation task": "natural language inference",
        "text": {
            "context": "In the original, Reich is set up by his host and then ambushed by a hostile questioner named John, and when he tries to answer with an eloquent Mr. Smith speech (My fist is clenched.",
            "statement": "Reich's host is out to get him."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "contradiction,entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "If the the word \"get\" refers to physical, for example \"catch\", then the statement is false.",
                "Reich's host is out to set him up. The word \"get\" in the statement could mean set up.",
                "His host set him up, so he is out to get him.",
                "Yes, because Reich is set by his host.",
                "He is set up by his host. So his host designed a trap for him, meanig his host is out to get him"
            ]
        }
    },
    "50480": {
        "annotation task": "natural language inference",
        "text": {
            "context": "But you will find it all right.\"",
            "statement": "You, I'm sure, will find it more than adequate."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both the context and the statement suggest that it is fine.",
                "\"find it all right\" implies \"more than adequate\"",
                "It might be more than adequate, but also can be just adequate.",
                "possible extraggeration."
            ]
        }
    },
    "123027": {
        "annotation task": "natural language inference",
        "text": {
            "context": "uh high humidity",
            "statement": "Warm, sweaty temperatures."
        },
        "number of annotators": 3,
        "annotators": "Ann2,Ann3,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.33299999999999996,
                "1": 0.667
            },
            "neutral": {
                "0": 0.667,
                "1": 0.333
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "\"high humidity\" implies \"warm sweaty temperatures\"",
                "It can be warm or cold.",
                "paraphrases to high humidity"
            ]
        }
    },
    "85279": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The almost midtown Massabielle quarter (faubourg de Massabielle), is sometimes described as the most picturesque in the city.",
            "statement": "The Massabielle quarter is a very touristy place."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention whether the Massabielle quarter is very touristy or not",
                "\"picturesque\" does not necessarily imply \"touristy\".",
                "It could be a very touristy place because of its great beauty, but it also can be not touristy because of poor transportation, or it is not so famous.",
                "picturesque is not directly related to touristy"
            ]
        }
    },
    "46650": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The draft treaty was Tommy's bait.",
            "statement": "Tommy took the bait of the treaty."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "contradiction",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It is not clear if Tommy took the bait of the treaty.",
                "The treaty acts as bait for Tommy, but it is not clear whether he really took it.",
                "No, Tommy is the person who set the bait, but not the one took the bait.",
                "It is not known if Tommy took the bait."
            ]
        }
    },
    "81579": {
        "annotation task": "natural language inference",
        "text": {
            "context": "All were prominent nationally known organizations.",
            "statement": "The only identified organizations were well-known."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention identified organizations.",
                "The statement is a paraphrase of the context.",
                "The only identified organizations could be well-known or not well-known.",
                "All the organizations were well-known"
            ]
        }
    },
    "23414": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Why bother to sacrifice your lives for dirt farmers and slavers?",
            "statement": "No one cares about the dirt farmers and slaves."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann2,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.667,
                "1": 0.333
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.33299999999999996,
                "1": 0.667
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context indicates the speaker's attitude toward the dirt farmers and slaves, not everyone's attitude.",
                "Judging by the context, the speaker probably does not care for \"dirt farmers\" and \"slavers\". It does not follow that they think that no one cares for them. Also the statement talks about \"slaves\" which are not mentioned at all.",
                "The person at whom this question was directed at, cared about the farmers."
            ]
        }
    },
    "36715": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Jon twisted the man's wrist.",
            "statement": "Jon grabbed the man."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It is clear that Jon grabbed the man by twisting his wrist.",
                "In order to twist the wrist, Jon has to first grab the man.",
                "The man could be grabbed by Jon, but also could slip from him.",
                "To twist, one would need to grab"
            ]
        }
    },
    "114492": {
        "annotation task": "natural language inference",
        "text": {
            "context": "and the same is true of the drug hangover you know if you",
            "statement": "It's just like a drug hangover but worse."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't contain any information about if it is worse.",
                "The context says that it's similar to \"drug hangover\" not \"worse\".",
                "It is similar to the drug hangover, but the extent can be worse or better.",
                "No info about where a drug hangover is worse"
            ]
        }
    },
    "136097": {
        "annotation task": "natural language inference",
        "text": {
            "context": "and going to school is also always very prohibitive now unless your parents are wealthy",
            "statement": "Wealthy parents are necessary for school."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both the context and the statement talk about the necessity of wealthy parents.",
                "If \"going to school is [...] prohibitive [...] unless your parents are wealthy\" then \"wealthy parents are necessary for school\".",
                "Wealthy parents can make going to school easier,  but maybe without wealthy parents, it is still possible.",
                "The school are too expensive to go to, making wealthy parents necessary to pay for the tuition"
            ]
        }
    },
    "101253": {
        "annotation task": "natural language inference",
        "text": {
            "context": "In his effort to build nationalism across Turkey in the 1920s, Ataterk instituted a campaign to suppress Kurdish identity that continues today.",
            "statement": "In 1942, Ataterk tried to build nationalism in Turkey."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Ataterk tried to build nationalism in Turkey in the 1920s, not in 1942.",
                "The context talks about the 1920s. It is not clear whether Ataterk still was politically active in the 1940s and whether he continued his campaign.",
                "We only can sure that in 1920s, Ataterk tried to build nationalism in Turkey, but in 1942, maybe he tried, maybe not.",
                "It should be in 1920s"
            ]
        }
    },
    "97520": {
        "annotation task": "natural language inference",
        "text": {
            "context": "AC Green's pretty good",
            "statement": "AC Green is also a solid player."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention AC Green's occupation.",
                "\"pretty good\" implies \"solid\".",
                "AC Green could be a player or do other jobs,",
                "a solid player is a good player"
            ]
        }
    },
    "103169": {
        "annotation task": "natural language inference",
        "text": {
            "context": "'Dave Hanson, to whom nothing was impossible.' Well, we have a nearly impossible task: a task of engineering and building.",
            "statement": "This building job is almost impossible, even for an experienced engineer."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "contradiction",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "We can infer that Dave Hanson is an experienced enginneer since nothing was impossible to him. The task is impossible since it is impossible to Dave Hanson, an experienced engineer.",
                "If the task is \"nearly impossible\" then it is also \"almost impossible\" for an experienced engineer.",
                "No, because nothing was impossible to Dava Hanson.",
                "It should be the job of engineering and building together"
            ]
        }
    },
    "134217": {
        "annotation task": "natural language inference",
        "text": {
            "context": "uh-huh and is it true i mean is it um",
            "statement": "It's true."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann2,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.33299999999999996,
                "1": 0.667
            },
            "neutral": {
                "0": 0.667,
                "1": 0.333
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both the context and the statement sugguest that it is true.",
                "The statement is a paraphrase of the context.",
                "context is a question"
            ]
        }
    },
    "11601": {
        "annotation task": "natural language inference",
        "text": {
            "context": "36 AC usage nationally for mercury control from power plants should be roughly proportional to the total MWe of coal-fired facilities that are equipped with the technology (this assumes an average capacity factor of 85 percent and other assumptions of Tables 4-4 and 4-5).",
            "statement": "Power plants' mercury control AC usage is higher than total MWe from coal facilities."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only mentions that Power plants' mercury control AC usage is proportional to total MWe from coal facilities. It is not clear if the usage should be lower or higher than total MWe.",
                "It is \"proportional\". This could be higher or lower.",
                "Power plants' mercury control AC usage is proportinal to the total MWe from coal facilities, so it could be more and could be less.",
                "It is not mentioned if the AC usage is higher than total MWe from coal facilities, but only in roughly proportional."
            ]
        }
    },
    "46059": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The results of even the most well designed epidemiological studies are characterized by this type of uncertainty, though well-designed studies typically report narrower uncertainty bounds around the best estimate than do studies of lesser quality.",
            "statement": "All studies have the same amount of uncertainty to them."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "contradiction",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.25,
                "1": 0.75
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Well-designed studies and studies of lesser quality have different amount of uncertainty to them.",
                "\"well-designed studies typically report narrower uncertainty bounds\" means that they have less uncertainty than other types of studies.",
                "No, well-designed studies typically report narrower uncertainty bounds around the best estimate than do studies of lesser quality.",
                "well-designed studies has less amount of uncertainty"
            ]
        }
    },
    "82156": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The great breathtaking Italian adventure remains the road.",
            "statement": "The road remains the Italy people want to see."
        },
        "number of annotators": 2,
        "annotators": "Ann1,Ann3",
        "number of annotations": 2,
        "annotations": {
            "Ann1": "neutral",
            "Ann3": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention what Italy people want to see.",
                "The road remains the great breathtaking Italian adventure, but Italy people could like adventure, could not."
            ]
        }
    },
    "122645": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Then you're ready for the fray, either in the bustling great bazaars such as Delhi's Chandni Chowk or Mumbai's Bhuleshwar, or the more sedate ambience of grander shops and showrooms.",
            "statement": "All of the great bazaars are bustling at all times."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It is not clear if all of the great bazaars are bustling at all times. The context only mentions two.",
                "It is not clear whether the bazaars are really bustling at all times or only at some times.",
                "Maybe great bazaars are bustling at all times, maybe only at day time or at night.",
                "In the context, only some bustling great bazaars were named, but it does not mean all of the great bazaars are bustling"
            ]
        }
    },
    "26142": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The importer pays duties that are required by law",
            "statement": "Imported goods have duties"
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both the context and the statement show that imported products have duties.",
                "If the importer has to pay duties then imported goods have duties on them.",
                "True, Imported goods have duties that are required by law.",
                "The importers have duties not the imported goods"
            ]
        }
    },
    "117177": {
        "annotation task": "natural language inference",
        "text": {
            "context": "I guess history repeats itself, Jane.",
            "statement": "I truly think the prior situation shows history repeats itself."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't specify what shows that history repeats itself.",
                "It is not clear whether the speaker truly believes that history repeats itself. They could also use it figuratively.",
                "I truly think history repeats itself.",
                "paraphrases"
            ]
        }
    },
    "78105": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Their supplies scarce, their harvest meager, and their spirit broken, they abandoned the fort in 1858.",
            "statement": "Their supplies remained very low and hard to maintain."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The statement is true because the context suggests that their supplies scarce and they abandonedd the fort.",
                "The supplies probably remained low because otherwise they might not have abandoned the fort.",
                "True, their supplies are scarce and they abandoned the fort.",
                "scarce means insufficient, so their supplies were low"
            ]
        }
    },
    "9393": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Next, you enter the vast and splendid Imperial Hall, with three handsome marble fountains, and a canopied throne from which the sultan would enjoy the music and dancing of his concubines.",
            "statement": "The sultan enjoyed drinking from the marble fountains in the Imperial Hall."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't talk about the sultan drinking.",
                "It is not clear whether the Sultan drank from the fountains.",
                "Maybe the sultan enjoyed drinking from the marble fountains, maybe he didn't like it.",
                "He enjoyed music and dancing there"
            ]
        }
    },
    "14459": {
        "annotation task": "natural language inference",
        "text": {
            "context": "After their savage battles, the warriors recuperated through meditation in the peace of a Zen monastery rock garden.",
            "statement": "The warriors recuperated through mediation learned from monks."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention from whom the warriors learned to meditate.",
                "They only meditated in a monastery. It is not clear from whom they learned the meditation.",
                "Maybe there are monks in the Zen monastery rock garden, maybe there are not.",
                "It is not known if they learned it from monks"
            ]
        }
    },
    "70711": {
        "annotation task": "natural language inference",
        "text": {
            "context": "because otherwise it's too it gets if you start them when it's cooler in the spring then it gets too hot in the summer",
            "statement": "You should start them during Spring if you want them to be cool during the summer."
        },
        "number of annotators": 3,
        "annotators": "Ann2,Ann3,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann2": "neutral",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.33299999999999996,
                "1": 0.667
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.667,
                "1": 0.333
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It is not clear whether \"they\" get too hot or whether it gets too hot for them.",
                "No, it gets too hot in the summer.",
                "One should start in the spring, cause it's hot summer and it's cool in spring."
            ]
        }
    },
    "77893": {
        "annotation task": "natural language inference",
        "text": {
            "context": "As he stepped across the threshold, Tommy brought the picture down with terrific force on his head.",
            "statement": "Tommy hurt his head bringing the picture down."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment,neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The statement is true since the picture that Tommy brought down hit him in the head.",
                "Tommy probably hurt his head because a picture hit his head with \"terrific force\".",
                "True, Tommy hurt his head with the picture.",
                "Tommy should hit another guy not himself, but in the statement, it could be understood as Tommy hurt himself, or another guy.",
                "He is not hurt but rather bad strong emotion"
            ]
        }
    },
    "109679": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The Palace of Jahangir is built around a square court with arches.",
            "statement": "The Palace of Jahangir houses a wonderful square court, complete with arches."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It is not clear from the context whether the square court is wonderful or not.",
                "\"is built around a square court with arches\" implies \"houses a wonderful square court [...] with arches",
                "True, the Palace of Jahangir is built around a square court with arches.",
                "The place is built around the square, so the place houses it"
            ]
        }
    },
    "123891": {
        "annotation task": "natural language inference",
        "text": {
            "context": "His proud reserve--a product of 40 years in the spotlight--is refreshing but does not bode well for his capacity to shepherd big ideas through Congress.",
            "statement": "He is way too loud."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "entailment,neutral",
            "Ann2": "neutral",
            "Ann3": "entailment",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The word \"loud\" in the statement may have two kinds of meanings. If it refers to flashy, then it fits the description of him in the context.",
                "It \"loud\" refers to noisy, the statement may be true or false since the context doesn't mention anything about his voice.",
                "It is not clear whether he is too loud or whether his proudness shows in other ways.",
                "True, his proud reserve is a product of 40 years in the spotlights, but does not bode well.",
                "Irrelevant"
            ]
        }
    },
    "16086": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Unless the report is restricted by law or regulation, auditors should ensure that copies be made available for public inspection.",
            "statement": "This report is most likely restricted by law or regulation and should not be ensured."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "entailment",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't suggest whether this report is restricted or not.",
                "The context only states what should happen if the report is not restricted. It does not say anything about whether it is restricted.",
                "True, being restricted by law or regulation is an exception that this report should not make the copies available for public inspection.",
                "No info about a specific report"
            ]
        }
    },
    "91650": {
        "annotation task": "natural language inference",
        "text": {
            "context": "yep because it's when it's self propelled it's heavy yeah",
            "statement": "it's heavy when it's self propelled, in case you were wondering"
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann2,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both the context and the statement suggest that it is heavy when it is self propelled.",
                "The statement is a paraphrase of the context.",
                "Same sentence: it's heavy when it's self propelled"
            ]
        }
    },
    "85428": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Christ on a crutch, what does he have to do to lose your support, stab David Geffen with a kitchen knife?",
            "statement": "Your support is unwavering."
        },
        "number of annotators": 3,
        "annotators": "Ann2,Ann3,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "If the only way to lose one person's support is to stab someone then the support is unwavering.",
                "True, because it used exaggeration to prove that you will always support him.",
                "\"Your\" support is so unwavering that he has to do something very extreme to lose your support"
            ]
        }
    },
    "118403": {
        "annotation task": "natural language inference",
        "text": {
            "context": "oh really it wouldn't matter if we plant them when it was starting to get warmer",
            "statement": "It is better to plant when it is colder."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context clearly suggests that it is fine to plant when it is warmer.",
                "If it doesn't matter when they plant them when it gets warmer, then it is not better to plant when it is colder.",
                "1) The question mark of context is missing, the end of the sentence could be a question mark. 2) Maybe it is better to plant when it is complete warm or even hot.",
                "It doesn't matter if it's warmer or colder"
            ]
        }
    },
    "105561": {
        "annotation task": "natural language inference",
        "text": {
            "context": "and they just put instructors out there and you you sign up for instruction and they just give you an arm band and if you see an instructor who's not doing anything you just tap him on the shoulder and ask him questions and they'll show you things",
            "statement": "The instructors are marked with armbands, and anytime you want to know anything, you just find one of them."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The contexte only mentions that they will give the speaker an arm band, it is not clear if the instructors are marked with armbands.",
                "The statement is a paraphrase of the context.",
                "No, I am marked with armbands and I need instructoion.",
                "Not anytime, the instructor has to be free first"
            ]
        }
    },
    "11303": {
        "annotation task": "natural language inference",
        "text": {
            "context": "'I see.'",
            "statement": "It was clear"
        },
        "number of annotators": 3,
        "annotators": "Ann2,Ann3,Ann4,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "entailment,neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.33299999999999996,
                "1": 0.667
            },
            "neutral": {
                "0": 0.33299999999999996,
                "1": 0.667
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The statement is a paraphrase of the context.",
                "Maybe it is not clear but I understand it anyway.",
                "If I see means literally to see",
                "Irrelevant if I see means I understand"
            ]
        }
    },
    "125013": {
        "annotation task": "natural language inference",
        "text": {
            "context": "yeah okay yeah those games are fun to watch you you you watch those games",
            "statement": "Those games are a lot of fun."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It is clear from the context and statement that those games are fun.",
                "If the games are fun to watch then they are fun (at least in that way).",
                "True, because those games are fun to watch.",
                "The games are fun to watch so it's a lot of fun"
            ]
        }
    },
    "44747": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Total volume grew 13.",
            "statement": "The expected increase was 10."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Accordding to the context, the increase was 13, not 10.",
                "It's not clear what the expected increase was.",
                "The expected increase maybe more or less than 10.",
                "It was 13"
            ]
        }
    },
    "19768": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Wear a nicely ventilated hat and keep to the shade in the street.",
            "statement": "The buildings are so low that there is no shade in the streets."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "contradiction",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context does not mention if the buildings are low.",
                "If the recommendation is to stay in the shade of the buildings then there are probably buildings casting shades. Otherwise the recommendation would not make sense.",
                "Maybe the buildings are not low, but the street is too wide.",
                "Irrelevant"
            ]
        }
    },
    "53468": {
        "annotation task": "natural language inference",
        "text": {
            "context": "But is the Internet so miraculous an advertising vehicle that Gross will be able to siphon off $400 per person from total ad spending of $1,000 per family--or persuade advertisers to spend an additional $400 to reach each of his customers?",
            "statement": "The internet is so great at advertising that is saved Gross money."
        },
        "number of annotators": 3,
        "annotators": "Ann2,Ann3,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann2": "neutral",
            "Ann3": "contradiction",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.667,
                "1": 0.333
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.33299999999999996,
                "1": 0.667
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context asks the question of whether the Internet is so great at advertising, whereas the statement asserts it.",
                "No, Gross saved no money, but siphoned money from other people.",
                "Context is a question. It can not come to a conclusion as in the statement"
            ]
        }
    },
    "10724": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Traditionally, certain designs were reserved for royalty, but today elegant geometric or exuberant, stylized floral patterns are available to all.",
            "statement": "Designs once reserved for royalty cost more to buy."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "There is no mention of designs' costs in the context.",
                "It doesn't say whether these designs are more expensive.",
                "The price of the designs once reserved for royalty is not mentioned in the context.",
                "No info about the cost"
            ]
        }
    },
    "17753": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The street ends at Taksim Square (Taksim Meydane), the heart of modern Istanbul, lined with luxurious five-star hotels and the glass-fronted Ataturk Cultural Centre (Ataturk Keleter Sarayy), also called the Opera House.",
            "statement": "The street is quite a luxurious one."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The description of the street in the context shows that it is indeed a luxurious one.",
                "Taksim Square is luxurious. It is not clear whether the street leading to it is also.",
                "The detail of the street itself is not mentioned in the context, we only know the end of the street.",
                "On the street there are fancy centers and a Luxurious hotel"
            ]
        }
    },
    "130928": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Still, I guess that can be got over.",
            "statement": "There are some things that you need to ignore."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't say anything about whether there's something to ignore.",
                "Getting over something does not necsessarily mean that you should ignore the issues. You could also work through them.",
                "There are some things that you can ignore, but it is not necessary.",
                "Getting over means to move on and stop caring about this. Thus to ignore"
            ]
        }
    },
    "92774": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The party's broad aims were to support capitalist policies and to continue close ties with Britain and the rest of the Commonwealth.",
            "statement": "The party sought to establish ties with the United States."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "contradiction",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention anything about establishing ties with the United States.",
                "The United States are not mentioned in the context.",
                "No, the party had close ties with the United States already.",
                "They aim to maintain ties with Britain and its commonwealth"
            ]
        }
    },
    "142604": {
        "annotation task": "natural language inference",
        "text": {
            "context": "As the budgets, functions, and points of service of many government programs devolve to state and local government, private entities and nonprofit organizations, and other third parties, it may become harder for GAO to obtain the records it needs to complete audits and evaluations.",
            "statement": "Audits and evaluations are harder because it is more difficult for GAO to get the records."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "entailment,neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context only mentions that it may be more difficult to obtain records, which is a possibility rather than an absolute thing.",
                "The context says that it \"may become harder\" not that \"it is more difficult\". These are not the same things because the context describes a future state.",
                "True, because it may become harder for GAO to obtain the records it needs to complete audits and evaluations.",
                "It can be more difficult for GAO to get the records, but maybe it actually doesn't become harder for GAO ro obtain the records.",
                "It was because the budget functions etc. of government programs further devolves that makes it harder"
            ]
        }
    },
    "42860": {
        "annotation task": "natural language inference",
        "text": {
            "context": "That's why we tried to kill you.",
            "statement": "That's one of the reasons we wanted to kill you."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "contradiction",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context implies that there is one reason, while the statement suggests that therea are multiple reasons.",
                "The statement is a paraphrase of the context.",
                "Maybe it is the only reason that we wanted to kill you, maybe that is just one of the reasons.",
                "From the context we know it might be the only reason"
            ]
        }
    },
    "77299": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The inquiry expanded very quickly, however, from asking what technology failed to an examination of contextual influences, such as",
            "statement": "They moved they inquiries over from technology failing because they thought it may be something else."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann2,Ann3",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.667,
                "1": 0.333
            },
            "neutral": {
                "0": 0.33299999999999996,
                "1": 0.667
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't mention the reason of moving inquiries over from technology failing.",
                "\"expanded [...] from asking what technology failed to [...] contextual influences\" means that they \"moved inquiries from technology failing because they thought it may be something else\".",
                "Maybe they thought it may be something else, maybe they just can not sure, what is the true reason."
            ]
        }
    },
    "13765": {
        "annotation task": "natural language inference",
        "text": {
            "context": "it's just it's the morals of the people which i mean i guess we everybody's responsible for the society but if i had a child that that did things so bad it's not they don't care about anybody these people they're stealing from they're just the big bad rich guy",
            "statement": "I have no issue with people stealing from others."
        },
        "number of annotators": 2,
        "annotators": "Ann2,Ann4",
        "number of annotations": 2,
        "annotations": {
            "Ann2": "entailment",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.5,
                "1": 0.5
            },
            "entailment": {
                "0": 0.5,
                "1": 0.5
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "\"they don't care about anybody these people they're stealing from\" shows that the speaker has issues with people stealing.",
                "I have no problem people stealing from bag rich people"
            ]
        }
    },
    "139362": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Endorphins were flowing.",
            "statement": "My endorphins were flowing."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't specify whose endorphins were flowing.",
                "The statement is a paraphrase of the context.",
                "It could be my endorphins, but also could be yours or anyone's.",
                "No info about whose endorphins it is"
            ]
        }
    },
    "116176": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Students of human misery can savor its underlying sadness and futility.",
            "statement": "Students of human misery will be delighted to see how sad it truly is."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.5,
                "1": 0.5
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It is not clear from the context if the students will be delighted.",
                "\"can savor\" implies \"will be delighted\".",
                "Students of human misery can \"savored\" that sadness, so maybe they are delighted to see that, maybe they are tortured by the disasters.",
                "Savor means to understand. Not to enjoy"
            ]
        }
    },
    "110234": {
        "annotation task": "natural language inference",
        "text": {
            "context": "really oh i thought it was great yeah",
            "statement": "that was a nice experience"
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann3,Ann4",
        "number of annotations": 5,
        "annotations": {
            "Ann1": "entailment",
            "Ann2": "entailment",
            "Ann3": "entailment,neutral",
            "Ann4": "contradiction"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.75,
                "1": 0.25
            },
            "entailment": {
                "0": 0.25,
                "1": 0.75
            },
            "neutral": {
                "0": 0.75,
                "1": 0.25
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both the context and the statement say that it was good.",
                "\"it was great\" means that it was \"a nice experience\".",
                "True, because I thought it was great",
                "Maybe it was a great experience, maybe it was a great present or something else.",
                "He thought it was a great experience. But in the context there was an element of surprise. So it was not a great experience after all"
            ]
        }
    },
    "128160": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Suddenly she started, and her face blanched.",
            "statement": "She moved swiftly, her face pale."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.75,
                "1": 0.25
            },
            "neutral": {
                "0": 0.25,
                "1": 0.75
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context doesn't specify what she started doing,  it could be moving, thinking or talking.",
                "The statement is a paraphrase of the context.",
                "She \"started\" can mean she moved swiftly, but also can do other actions, like crying, singing, etc.",
                "No info about her moving swiftly"
            ]
        }
    },
    "123038": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Reports on attestation engagements should state that the engagement was made in accordance with generally accepted government auditing standards.",
            "statement": "Details regarding validation engagements ought to express that the engagement was made as per by and large acknowledged government evaluating guidelines."
        },
        "number of annotators": 2,
        "annotators": "Ann1,Ann4",
        "number of annotations": 2,
        "annotations": {
            "Ann1": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Both the context and statement suggest the same requirement for reports.",
                "Generally accepted is synonym to large acknowledged"
            ]
        }
    },
    "98621": {
        "annotation task": "natural language inference",
        "text": {
            "context": "In other cases, we must rely on survey approaches to estimate WTP, usually through a variant of the contingent valuation approach, which generally involves directly questioning respondents for their WTP in hypothetical market situations.",
            "statement": "Hypothetical market situations are uniform across all respondents."
        },
        "number of annotators": 3,
        "annotators": "Ann2,Ann3,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann2": "neutral",
            "Ann3": "contradiction",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 0.667,
                "1": 0.333
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.33299999999999996,
                "1": 0.667
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It is not clear whether they use the same market situtations for all people they ask.",
                "No, if market situations are uniform, then \"a variant of the contingent valuation approach\" is not necessary.",
                "No info about uniformity across the respondents"
            ]
        }
    },
    "98561": {
        "annotation task": "natural language inference",
        "text": {
            "context": "was it bad",
            "statement": "Was it not good?"
        },
        "number of annotators": 2,
        "annotators": "Ann2,Ann4",
        "number of annotations": 2,
        "annotations": {
            "Ann2": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.0,
                "1": 1.0
            },
            "neutral": {
                "0": 1.0,
                "1": 0.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The statement is a paraphrase of the context.",
                "Bad and not good are synonyms"
            ]
        }
    },
    "86429": {
        "annotation task": "natural language inference",
        "text": {
            "context": "Agencies may perform the analyses required by sections 603 and 604 in conjunction with or as part of any other agenda or analysis required by other law if such other analysis satisfies the provisions of these sections.",
            "statement": "There are many times when the agencies have gotten in trouble."
        },
        "number of annotators": 4,
        "annotators": "Ann1,Ann2,Ann3,Ann4",
        "number of annotations": 4,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "neutral",
            "Ann3": "neutral",
            "Ann4": "neutral"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 1.0,
                "1": 0.0
            },
            "neutral": {
                "0": 0.0,
                "1": 1.0
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "It is not clear from the context if th eagencies have gotten in trouble.",
                "The context does not say anything about trouble.",
                "The conditions of trouble is not offered in the context.",
                "No info about agencies in trouble"
            ]
        }
    },
    "105911": {
        "annotation task": "natural language inference",
        "text": {
            "context": "and to have children and just get a day care or someone to take care of it and not really have the bonding process that takes place with babies and stuff you know",
            "statement": "The children should not go to day car."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann2,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.33299999999999996,
                "1": 0.667
            },
            "neutral": {
                "0": 0.667,
                "1": 0.333
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "The context says that it is not good if sending children to day care is the only way to take care of them, but it doesn't mention if children shouldn't go to day care at all.",
                "\"just get a day care [...] and not really have the bonding process\" sound like the speaker is opposed to day care.",
                "Because the bonding process will be missed"
            ]
        }
    },
    "126486": {
        "annotation task": "natural language inference",
        "text": {
            "context": "The entire setup has an anti-competitive, anti-entrepreneurial flavor that rewards political lobbying rather than good business practices.",
            "statement": "The setup has lead to increases in political lobbying."
        },
        "number of annotators": 3,
        "annotators": "Ann1,Ann2,Ann4",
        "number of annotations": 3,
        "annotations": {
            "Ann1": "neutral",
            "Ann2": "entailment",
            "Ann4": "entailment"
        },
        "soft_label": {
            "contradiction": {
                "0": 1.0,
                "1": 0.0
            },
            "entailment": {
                "0": 0.33299999999999996,
                "1": 0.667
            },
            "neutral": {
                "0": 0.667,
                "1": 0.333
            }
        },
        "split": "train",
        "lang": "en",
        "other_info": {
            "explanations": [
                "Rewarding political lobbying does not necessarily mean an actual increase in political lobbying.",
                "If the setup rewards political lobbying then it will likely lead to increases in lobbying.",
                "Because political lobbying is rewarded"
            ]
        }
    }
}